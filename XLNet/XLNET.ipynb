{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.parsing.preprocessing import (preprocess_string, remove_stopwords,\n",
    "                                          strip_punctuation, strip_tags)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams  # function for making ngrams\n",
    "from numpy import asarray, save, savez_compressed\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         imdb_id  ... synopsis_source\n0      tt0057603  ...            imdb\n1      tt1733125  ...            imdb\n2      tt0033045  ...            imdb\n3      tt0113862  ...            imdb\n4      tt0086250  ...            imdb\n...          ...  ...             ...\n14823  tt0219952  ...       wikipedia\n14824  tt1371159  ...       wikipedia\n14825  tt0063443  ...       wikipedia\n14826  tt0039464  ...       wikipedia\n14827  tt0235166  ...       wikipedia\n\n[14828 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imdb_id</th>\n      <th>title</th>\n      <th>plot_synopsis</th>\n      <th>tags</th>\n      <th>split</th>\n      <th>synopsis_source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>tt0057603</td>\n      <td>I tre volti della paura</td>\n      <td>Note: this synopsis is for the orginal Italian...</td>\n      <td>cult, horror, gothic, murder, atmospheric</td>\n      <td>train</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tt1733125</td>\n      <td>Dungeons &amp; Dragons: The Book of Vile Darkness</td>\n      <td>Two thousand years ago, Nhagruul the Foul, a s...</td>\n      <td>violence</td>\n      <td>train</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tt0033045</td>\n      <td>The Shop Around the Corner</td>\n      <td>Matuschek's, a gift store in Budapest, is the ...</td>\n      <td>romantic</td>\n      <td>test</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tt0113862</td>\n      <td>Mr. Holland's Opus</td>\n      <td>Glenn Holland, not a morning person by anyone'...</td>\n      <td>inspiring, romantic, stupid, feel-good</td>\n      <td>train</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>tt0086250</td>\n      <td>Scarface</td>\n      <td>In May 1980, a Cuban man named Tony Montana (A...</td>\n      <td>cruelty, murder, dramatic, cult, violence, atm...</td>\n      <td>val</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14823</th>\n      <td>tt0219952</td>\n      <td>Lucky Numbers</td>\n      <td>In 1988 Russ Richards (John Travolta), the wea...</td>\n      <td>comedy, murder</td>\n      <td>test</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14824</th>\n      <td>tt1371159</td>\n      <td>Iron Man 2</td>\n      <td>In Russia, the media covers Tony Stark's discl...</td>\n      <td>good versus evil, violence</td>\n      <td>train</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14825</th>\n      <td>tt0063443</td>\n      <td>Play Dirty</td>\n      <td>During the North African Campaign in World War...</td>\n      <td>anti war</td>\n      <td>train</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14826</th>\n      <td>tt0039464</td>\n      <td>High Wall</td>\n      <td>Steven Kenet catches his unfaithful wife in th...</td>\n      <td>murder</td>\n      <td>test</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14827</th>\n      <td>tt0235166</td>\n      <td>Against All Hope</td>\n      <td>Sometime in the 1950s in Chicago a man, Cecil ...</td>\n      <td>christian film</td>\n      <td>test</td>\n      <td>wikipedia</td>\n    </tr>\n  </tbody>\n</table>\n<p>14828 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "mpstDF= pd.read_csv(\"mpst.csv\")\n",
    "mpstDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can't\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"couldn't\", \"could not\", phrase)\n",
    "    phrase = re.sub(r\"wouldn't\", \"would not\", phrase)\n",
    "    phrase = re.sub(r\"shouldn't\", \"should not\", phrase)\n",
    "    phrase = re.sub(r\"don't\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"doesn't\", \"does not\", phrase)\n",
    "    phrase = re.sub(r\"haven't\", \"have not\", phrase)\n",
    "    phrase = re.sub(r\"hasn't\", \"has not\", phrase)\n",
    "    phrase = re.sub(r\"ain't\", \"not\", phrase)\n",
    "    phrase = re.sub(r\"hadn't\", \"had not\", phrase)\n",
    "    phrase = re.sub(r\"didn't\", \"did not\", phrase)\n",
    "    phrase = re.sub(r\"wasn't\", \"was not\", phrase)\n",
    "    phrase = re.sub(r\"aren't\", \"are not\", phrase)\n",
    "    phrase = re.sub(r\"isn't\", \"is not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpstDF_processsed=mpstDF.copy()\n",
    "# Type 1: Decontracted Text, The puncutation and stop words are still there\n",
    "mpstDF_processsed[\"processed_synopsis_t1\"]=mpstDF_processsed[\"plot_synopsis\"].apply(lambda x: decontracted(\" \".join(preprocess_string(x, [lambda x: x.lower(), strip_tags]))))\n",
    "# Type 2 Decontracted Text Stop Words Removed\n",
    "mpstDF_processsed[\"processed_synopsis_t2\"]=mpstDF_processsed[\"plot_synopsis\"].apply(lambda x: decontracted(\" \".join(preprocess_string(x, [lambda x: x.lower(), strip_tags,remove_stopwords]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import XLNetConfig, XLNetModel, XLNetTokenizer\n",
    "from transformers import AutoModel,AutoConfig,AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xlnConfig= XLNetConfig()\n",
    "# xlnModel = XLNetModel(xlnConfig)\n",
    "from transformers import AutoModel,AutoConfig,AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased')\n",
    "# model = AutoModel.from_config(config)\n",
    "model = AutoModel.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Encodings(text,tokenizer=tokenizer,model=model, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"Text:\")\n",
    "        print(text[:20] + \"....\")\n",
    "    text_encoding_tensor=torch.tensor([tokenizer.encode(text)])\n",
    "    if verbose:\n",
    "        print(\"text_encoding_tensor:\")\n",
    "        print(text_encoding_tensor)\n",
    "        print(\"shape:\")\n",
    "        print(text_encoding_tensor.shape)\n",
    "    attention_mask_tensor= torch.tensor([[1]*text_encoding_tensor.shape[1]])\n",
    "    if verbose:\n",
    "        print(\"attention_mask_tensor:\")\n",
    "        print(attention_mask_tensor)\n",
    "        print(\"shape:\")\n",
    "        print(attention_mask_tensor.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(text_encoding_tensor, attention_mask=attention_mask_tensor)\n",
    "        if verbose:\n",
    "            print(\"outputs:\")\n",
    "            print(outputs)\n",
    "            print(\"Lenght of outputs\",len(outputs))\n",
    "            print(\"outputs[0]:\")\n",
    "            print(outputs[0])\n",
    "            print(\"outputs[0].shape:\")\n",
    "            print(outputs[0].shape)\n",
    "            print(\"outputs[1]:\")\n",
    "            print(outputs[1])\n",
    "            print(\"Length Ooutputstput[1]:\")\n",
    "            print(len(outputs[1]))\n",
    "            print(\"Sample from Output[1], first hidden layer:\")\n",
    "            print(outputs[1][0])\n",
    "            print(\"Sample shape, first hidden layer\")\n",
    "            \n",
    "    if verbose:\n",
    "        print(\"getting the last tensor for XLNet\")\n",
    "        print(outputs[0].squeeze()[-1])\n",
    "    return outputs[0].squeeze()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXLNetEmbeddings(df,column,verbose=False):\n",
    "    embeddings = np.array(torch.tensor([np.array(get_Encodings(x,verbose=verbose)) for x in df[column]]))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS NORMAL TEST FUNCTION CREATED AND LATER MNERGED WITH THE getXLNetEmbeddings METHOD\n",
    "# def getXLNetEmbeddings_testMode(df,column,verbose=False):\n",
    "#     embeddings = np.array(torch.tensor([np.array(get_Encodings(x,verbose=verbose)) for x in df[column]]))\n",
    "#     return embeddings"
   ]
  },
  {
   "source": [
    "**Testing verbose mode for one Input**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Text:\nIn May 1980, a Cuban....\ntext_encoding_tensor:\ntensor([[   67,   428,  1910,    19,    24,  5907,   326,   812,  3342,  9662,\n            17,    10,  3869, 12012,  2844,    11,  1897, 11003,    19,    25,\n          1442,    19,  3224,    19,    21,    27,    25,   987,    20,    18,\n            17,    12,  2730,  7684,    12,    99, 21077,  4310,    25,    18,\n          9052,   368,  2694,  7358,    20,  1910,     9,   311,  6242,    37,\n           139,  2434,    13, 14877,    56,    35,  6522,   361,    19,    63,\n          2588,    24, 10784,    31,  3342,    26,    23,   263,  1860,    20,\n            24,   710,   758,    33,    24,  4811,  1383,   267,   135,    36,\n            19,    59, 17142,   103,    34,    24,   645,   249,    19,    21,\n         24417,   103,    25,    24,  1900,   271,    17,    26,  8060,  5173,\n          2391,    26,    33,    86,  5907,    23,    19,   208,  3342,    26,\n            23,   252,  1233,    21,   317,  5907,  1422, 20555,  1341,  8292,\n            17,    12,  5916,  2159,  3724,    12, 20824,  2151,   101,    17,\n            10, 28766,   180, 23719,    11,    19,   168,    18,   303,    35,\n            13,  2270, 29432,   171,    18,   146,  8548,    23,    58,  8611,\n          6307,    23,     9,  9098,   544,   307,    20, 16881,  4425,  5073,\n            56,    21,  1900, 11533,    19, 25353,  9118,    48,   670,    40,\n            18,  5907, 24128,    59,    43,  1068,  8721,    23,    22,  3342,\n             9,   183,    63,  2201, 30084,   741,  7639,  1463,    17,    10,\n         23912,   155,  1387,    46,    88,  8304,    11,    24,   317,  9260,\n            22, 16600,  8925,    61,    27,   145,  5435,    25,  8282,  2391,\n            19,    63,    53,  1217,  1540,  2535,     9,  3342, 10893,    19,\n            21, 13465,   741,  7639,  1463,   181,    24, 10666,    38,  8282,\n          2391,     9,     4,     3]])\nshape:\ntorch.Size([1, 234])\nattention_mask_tensor:\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\nshape:\ntorch.Size([1, 234])\noutputs:\n(tensor([[[ 0.6994, -0.7551, -1.4868,  ..., -1.0411, -2.6801, -0.8442],\n         [-0.0194,  0.4038, -1.0099,  ..., -1.8781,  0.0992, -0.6813],\n         [ 1.7263, -1.0081,  0.8448,  ..., -0.0263, -0.7917,  0.5056],\n         ...,\n         [-1.1882, -0.7369, -1.6018,  ..., -0.4104, -1.2504, -1.6098],\n         [-0.1397,  0.7827, -0.8009,  ...,  0.1338, -0.9038, -1.5689],\n         [-0.2145,  0.9864, -1.3569,  ..., -0.0941, -1.1556, -1.5487]]]), (tensor([[[ 0.0095, -0.0154, -0.0060,  ..., -0.0189, -0.0411, -0.0025]],\n\n        [[-0.0082,  0.0970, -0.0516,  ..., -0.0475, -0.0350, -0.0446]],\n\n        [[ 0.0028,  0.0029, -0.0050,  ...,  0.0228, -0.0612, -0.0213]],\n\n        ...,\n\n        [[-0.0238, -0.0061, -0.0163,  ..., -0.0160,  0.0431, -0.0309]],\n\n        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n\n        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]]), tensor([[[ 0.0690, -0.5341, -0.0530,  ..., -0.9793, -0.4927,  0.3185]],\n\n        [[-0.3387,  2.7231, -1.0147,  ..., -2.8134, -0.8657, -0.7620]],\n\n        [[ 0.7158,  0.6257, -0.6323,  ..., -0.1507, -1.3484, -0.0525]],\n\n        ...,\n\n        [[ 0.2417,  0.2710, -0.0241,  ..., -0.5146,  0.0908, -0.2431]],\n\n        [[ 1.6406, -0.5561, -1.8112,  ...,  0.7264,  0.6781,  0.0349]],\n\n        [[ 0.7250, -0.2270, -2.5181,  ...,  0.3814, -0.0679,  0.4538]]]), tensor([[[ 0.0404, -0.5268, -0.0286,  ..., -0.7399, -0.7015,  0.9109]],\n\n        [[ 0.0527,  1.3367, -1.1325,  ..., -2.4993, -0.3660, -0.0743]],\n\n        [[ 1.0475,  0.3670, -0.4815,  ..., -0.0142, -0.7057,  0.4183]],\n\n        ...,\n\n        [[ 0.2802,  0.2141,  0.2821,  ..., -0.5068,  0.1401, -0.1850]],\n\n        [[ 0.8736, -0.5288, -1.6994,  ...,  0.3465,  0.8730, -0.2368]],\n\n        [[ 0.5242, -0.4325, -2.7047,  ...,  0.6055,  0.1722, -0.0948]]]), tensor([[[ 0.0242, -0.2637,  0.2331,  ..., -0.6617, -0.5469,  1.1571]],\n\n        [[-0.2078,  1.7432, -0.7233,  ..., -2.6746, -0.3184, -0.0413]],\n\n        [[ 0.7137,  0.4446, -0.6881,  ..., -0.1234, -1.0547, -0.0601]],\n\n        ...,\n\n        [[ 0.2183,  0.2967, -0.1525,  ..., -0.7183,  0.6977, -0.7726]],\n\n        [[ 0.7620, -0.4230, -1.9231,  ...,  1.3091,  1.3400, -1.5286]],\n\n        [[ 0.9665, -0.0176, -2.4879,  ...,  0.8153,  0.3322, -0.7670]]]), tensor([[[ 0.1296, -0.0828, -0.2622,  ...,  0.1939, -0.4472,  0.9703]],\n\n        [[ 0.0927,  1.3519, -0.7332,  ..., -1.9907, -0.5160,  0.3583]],\n\n        [[ 1.5803,  0.1451, -0.3670,  ...,  0.7971, -1.2420,  0.4941]],\n\n        ...,\n\n        [[ 0.3848, -0.1304, -0.1386,  ..., -0.8818, -0.2353, -0.4644]],\n\n        [[ 1.1480, -0.5250, -1.9834,  ...,  1.7248,  1.1639, -0.6752]],\n\n        [[ 1.7331, -0.3146, -2.3592,  ...,  1.0520, -0.0247, -0.0265]]]), tensor([[[ 0.5954, -0.2561, -0.2318,  ...,  0.2631, -0.2029,  0.4560]],\n\n        [[ 0.9146,  1.1603, -0.3282,  ..., -1.7810, -0.1079, -0.0405]],\n\n        [[ 1.4999,  0.3422, -0.4625,  ...,  0.6409, -0.9832,  0.0320]],\n\n        ...,\n\n        [[ 0.4214, -0.3180, -0.1872,  ..., -0.7913, -0.3253, -0.3300]],\n\n        [[ 0.5864, -0.3648, -1.7058,  ...,  1.2228,  0.4706, -0.8943]],\n\n        [[ 1.3517, -0.3131, -1.9205,  ...,  0.8023, -0.0611, -0.1870]]]), tensor([[[ 0.8927, -0.4630, -0.3729,  ...,  0.2690, -0.5382,  0.5780]],\n\n        [[ 1.2890,  1.0300, -0.1611,  ..., -1.6201,  0.2968,  0.2825]],\n\n        [[ 1.9668,  0.4809, -0.9813,  ...,  1.2703, -1.8089,  0.3426]],\n\n        ...,\n\n        [[ 0.4924, -0.5834, -0.2054,  ..., -0.4755, -0.4102, -0.2930]],\n\n        [[ 0.7159, -0.2497, -1.5915,  ...,  1.0259,  0.3615, -0.3789]],\n\n        [[ 1.3053, -0.0036, -1.8318,  ...,  0.8736, -0.2593,  0.3711]]]), tensor([[[ 0.9462, -0.8524, -0.6207,  ...,  0.0321, -0.7025,  0.4042]],\n\n        [[ 0.8001,  0.9491,  0.0828,  ..., -1.7104,  0.5739, -0.2652]],\n\n        [[ 1.4355,  0.1511, -0.8139,  ...,  1.0896, -1.3393,  0.2125]],\n\n        ...,\n\n        [[ 0.6637, -0.4129,  0.0728,  ..., -0.8062, -0.1118,  0.1651]],\n\n        [[ 0.7758, -0.2890, -0.8239,  ..., -0.0768,  0.2755,  0.3563]],\n\n        [[ 0.9280, -0.0489, -0.9207,  ...,  0.2202, -0.2134,  0.8065]]]), tensor([[[ 1.1675, -1.0751, -0.7863,  ..., -0.3570, -0.8146,  0.1820]],\n\n        [[ 0.7365,  0.8743,  0.0154,  ..., -2.2731,  0.4960, -0.7167]],\n\n        [[ 1.7333,  0.0895, -1.2739,  ...,  1.1084, -1.0799,  0.2718]],\n\n        ...,\n\n        [[ 0.6134, -0.2187, -0.0843,  ..., -0.4409, -0.2080,  0.1626]],\n\n        [[ 0.8345, -0.5287, -0.9191,  ..., -0.0094,  0.3518,  0.5914]],\n\n        [[ 0.8165, -0.2298, -0.9250,  ...,  0.2106, -0.1752,  0.8533]]]), tensor([[[ 7.6031e-01, -7.4567e-01, -9.5526e-01,  ..., -5.1511e-01,\n          -5.0290e-01, -1.0698e-01]],\n\n        [[ 2.7711e-01,  6.7505e-01, -2.9533e-01,  ..., -1.6090e+00,\n           3.6866e-01, -8.4836e-01]],\n\n        [[ 1.0272e+00,  1.2048e-01, -1.0141e+00,  ...,  1.0111e+00,\n          -8.6741e-01, -1.8023e-01]],\n\n        ...,\n\n        [[ 1.8301e-01, -4.7403e-02, -2.2853e-01,  ..., -1.3079e-01,\n           5.9575e-04,  6.3242e-02]],\n\n        [[ 5.0155e-01, -2.8504e-01, -1.0635e+00,  ..., -3.5485e-02,\n           4.1572e-01,  2.6728e-01]],\n\n        [[ 5.4873e-01, -1.7803e-01, -1.0803e+00,  ...,  7.8188e-02,\n           6.3941e-02,  5.1633e-01]]]), tensor([[[ 6.7087e-01, -3.7367e-01, -7.0798e-01,  ..., -8.0827e-01,\n          -6.4806e-01, -3.0193e-01]],\n\n        [[-1.4400e-03,  4.6013e-01,  2.9902e-01,  ..., -1.7545e+00,\n           3.4593e-01, -1.0504e+00]],\n\n        [[ 9.6069e-01, -1.3984e-01, -9.5270e-01,  ...,  1.1877e+00,\n          -8.1687e-01, -3.7305e-01]],\n\n        ...,\n\n        [[ 5.5025e-02, -2.8632e-02, -1.1166e-01,  ...,  6.0107e-02,\n           3.5829e-02,  5.7531e-02]],\n\n        [[ 7.3460e-01,  2.2389e-01, -6.3710e-01,  ..., -2.5127e-02,\n          -1.0897e-01,  9.3741e-02]],\n\n        [[ 6.6431e-01,  2.3361e-01, -6.6281e-01,  ..., -4.8470e-02,\n          -4.8516e-01,  1.8516e-01]]]), tensor([[[ 0.1677, -0.1927, -0.4004,  ..., -0.4064, -0.4957,  0.2079]],\n\n        [[-0.1893,  0.3441,  0.3934,  ..., -1.1659,  0.1232, -0.7636]],\n\n        [[ 0.8200, -0.3971, -0.2547,  ...,  0.6752, -0.6345, -0.2071]],\n\n        ...,\n\n        [[-0.1047,  0.0238, -0.1989,  ...,  0.0022,  0.0259,  0.3637]],\n\n        [[ 0.0309,  0.2163, -0.4064,  ...,  0.0229, -0.3999, -0.2156]],\n\n        [[ 0.0394,  0.1884, -0.4690,  ..., -0.0498, -0.5239, -0.0483]]])))\nLenght of outputs 2\noutputs[0]:\ntensor([[[ 0.6994, -0.7551, -1.4868,  ..., -1.0411, -2.6801, -0.8442],\n         [-0.0194,  0.4038, -1.0099,  ..., -1.8781,  0.0992, -0.6813],\n         [ 1.7263, -1.0081,  0.8448,  ..., -0.0263, -0.7917,  0.5056],\n         ...,\n         [-1.1882, -0.7369, -1.6018,  ..., -0.4104, -1.2504, -1.6098],\n         [-0.1397,  0.7827, -0.8009,  ...,  0.1338, -0.9038, -1.5689],\n         [-0.2145,  0.9864, -1.3569,  ..., -0.0941, -1.1556, -1.5487]]])\noutputs[0].shape:\ntorch.Size([1, 234, 768])\noutputs[1]:\n(tensor([[[ 0.0095, -0.0154, -0.0060,  ..., -0.0189, -0.0411, -0.0025]],\n\n        [[-0.0082,  0.0970, -0.0516,  ..., -0.0475, -0.0350, -0.0446]],\n\n        [[ 0.0028,  0.0029, -0.0050,  ...,  0.0228, -0.0612, -0.0213]],\n\n        ...,\n\n        [[-0.0238, -0.0061, -0.0163,  ..., -0.0160,  0.0431, -0.0309]],\n\n        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n\n        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]]), tensor([[[ 0.0690, -0.5341, -0.0530,  ..., -0.9793, -0.4927,  0.3185]],\n\n        [[-0.3387,  2.7231, -1.0147,  ..., -2.8134, -0.8657, -0.7620]],\n\n        [[ 0.7158,  0.6257, -0.6323,  ..., -0.1507, -1.3484, -0.0525]],\n\n        ...,\n\n        [[ 0.2417,  0.2710, -0.0241,  ..., -0.5146,  0.0908, -0.2431]],\n\n        [[ 1.6406, -0.5561, -1.8112,  ...,  0.7264,  0.6781,  0.0349]],\n\n        [[ 0.7250, -0.2270, -2.5181,  ...,  0.3814, -0.0679,  0.4538]]]), tensor([[[ 0.0404, -0.5268, -0.0286,  ..., -0.7399, -0.7015,  0.9109]],\n\n        [[ 0.0527,  1.3367, -1.1325,  ..., -2.4993, -0.3660, -0.0743]],\n\n        [[ 1.0475,  0.3670, -0.4815,  ..., -0.0142, -0.7057,  0.4183]],\n\n        ...,\n\n        [[ 0.2802,  0.2141,  0.2821,  ..., -0.5068,  0.1401, -0.1850]],\n\n        [[ 0.8736, -0.5288, -1.6994,  ...,  0.3465,  0.8730, -0.2368]],\n\n        [[ 0.5242, -0.4325, -2.7047,  ...,  0.6055,  0.1722, -0.0948]]]), tensor([[[ 0.0242, -0.2637,  0.2331,  ..., -0.6617, -0.5469,  1.1571]],\n\n        [[-0.2078,  1.7432, -0.7233,  ..., -2.6746, -0.3184, -0.0413]],\n\n        [[ 0.7137,  0.4446, -0.6881,  ..., -0.1234, -1.0547, -0.0601]],\n\n        ...,\n\n        [[ 0.2183,  0.2967, -0.1525,  ..., -0.7183,  0.6977, -0.7726]],\n\n        [[ 0.7620, -0.4230, -1.9231,  ...,  1.3091,  1.3400, -1.5286]],\n\n        [[ 0.9665, -0.0176, -2.4879,  ...,  0.8153,  0.3322, -0.7670]]]), tensor([[[ 0.1296, -0.0828, -0.2622,  ...,  0.1939, -0.4472,  0.9703]],\n\n        [[ 0.0927,  1.3519, -0.7332,  ..., -1.9907, -0.5160,  0.3583]],\n\n        [[ 1.5803,  0.1451, -0.3670,  ...,  0.7971, -1.2420,  0.4941]],\n\n        ...,\n\n        [[ 0.3848, -0.1304, -0.1386,  ..., -0.8818, -0.2353, -0.4644]],\n\n        [[ 1.1480, -0.5250, -1.9834,  ...,  1.7248,  1.1639, -0.6752]],\n\n        [[ 1.7331, -0.3146, -2.3592,  ...,  1.0520, -0.0247, -0.0265]]]), tensor([[[ 0.5954, -0.2561, -0.2318,  ...,  0.2631, -0.2029,  0.4560]],\n\n        [[ 0.9146,  1.1603, -0.3282,  ..., -1.7810, -0.1079, -0.0405]],\n\n        [[ 1.4999,  0.3422, -0.4625,  ...,  0.6409, -0.9832,  0.0320]],\n\n        ...,\n\n        [[ 0.4214, -0.3180, -0.1872,  ..., -0.7913, -0.3253, -0.3300]],\n\n        [[ 0.5864, -0.3648, -1.7058,  ...,  1.2228,  0.4706, -0.8943]],\n\n        [[ 1.3517, -0.3131, -1.9205,  ...,  0.8023, -0.0611, -0.1870]]]), tensor([[[ 0.8927, -0.4630, -0.3729,  ...,  0.2690, -0.5382,  0.5780]],\n\n        [[ 1.2890,  1.0300, -0.1611,  ..., -1.6201,  0.2968,  0.2825]],\n\n        [[ 1.9668,  0.4809, -0.9813,  ...,  1.2703, -1.8089,  0.3426]],\n\n        ...,\n\n        [[ 0.4924, -0.5834, -0.2054,  ..., -0.4755, -0.4102, -0.2930]],\n\n        [[ 0.7159, -0.2497, -1.5915,  ...,  1.0259,  0.3615, -0.3789]],\n\n        [[ 1.3053, -0.0036, -1.8318,  ...,  0.8736, -0.2593,  0.3711]]]), tensor([[[ 0.9462, -0.8524, -0.6207,  ...,  0.0321, -0.7025,  0.4042]],\n\n        [[ 0.8001,  0.9491,  0.0828,  ..., -1.7104,  0.5739, -0.2652]],\n\n        [[ 1.4355,  0.1511, -0.8139,  ...,  1.0896, -1.3393,  0.2125]],\n\n        ...,\n\n        [[ 0.6637, -0.4129,  0.0728,  ..., -0.8062, -0.1118,  0.1651]],\n\n        [[ 0.7758, -0.2890, -0.8239,  ..., -0.0768,  0.2755,  0.3563]],\n\n        [[ 0.9280, -0.0489, -0.9207,  ...,  0.2202, -0.2134,  0.8065]]]), tensor([[[ 1.1675, -1.0751, -0.7863,  ..., -0.3570, -0.8146,  0.1820]],\n\n        [[ 0.7365,  0.8743,  0.0154,  ..., -2.2731,  0.4960, -0.7167]],\n\n        [[ 1.7333,  0.0895, -1.2739,  ...,  1.1084, -1.0799,  0.2718]],\n\n        ...,\n\n        [[ 0.6134, -0.2187, -0.0843,  ..., -0.4409, -0.2080,  0.1626]],\n\n        [[ 0.8345, -0.5287, -0.9191,  ..., -0.0094,  0.3518,  0.5914]],\n\n        [[ 0.8165, -0.2298, -0.9250,  ...,  0.2106, -0.1752,  0.8533]]]), tensor([[[ 7.6031e-01, -7.4567e-01, -9.5526e-01,  ..., -5.1511e-01,\n          -5.0290e-01, -1.0698e-01]],\n\n        [[ 2.7711e-01,  6.7505e-01, -2.9533e-01,  ..., -1.6090e+00,\n           3.6866e-01, -8.4836e-01]],\n\n        [[ 1.0272e+00,  1.2048e-01, -1.0141e+00,  ...,  1.0111e+00,\n          -8.6741e-01, -1.8023e-01]],\n\n        ...,\n\n        [[ 1.8301e-01, -4.7403e-02, -2.2853e-01,  ..., -1.3079e-01,\n           5.9575e-04,  6.3242e-02]],\n\n        [[ 5.0155e-01, -2.8504e-01, -1.0635e+00,  ..., -3.5485e-02,\n           4.1572e-01,  2.6728e-01]],\n\n        [[ 5.4873e-01, -1.7803e-01, -1.0803e+00,  ...,  7.8188e-02,\n           6.3941e-02,  5.1633e-01]]]), tensor([[[ 6.7087e-01, -3.7367e-01, -7.0798e-01,  ..., -8.0827e-01,\n          -6.4806e-01, -3.0193e-01]],\n\n        [[-1.4400e-03,  4.6013e-01,  2.9902e-01,  ..., -1.7545e+00,\n           3.4593e-01, -1.0504e+00]],\n\n        [[ 9.6069e-01, -1.3984e-01, -9.5270e-01,  ...,  1.1877e+00,\n          -8.1687e-01, -3.7305e-01]],\n\n        ...,\n\n        [[ 5.5025e-02, -2.8632e-02, -1.1166e-01,  ...,  6.0107e-02,\n           3.5829e-02,  5.7531e-02]],\n\n        [[ 7.3460e-01,  2.2389e-01, -6.3710e-01,  ..., -2.5127e-02,\n          -1.0897e-01,  9.3741e-02]],\n\n        [[ 6.6431e-01,  2.3361e-01, -6.6281e-01,  ..., -4.8470e-02,\n          -4.8516e-01,  1.8516e-01]]]), tensor([[[ 0.1677, -0.1927, -0.4004,  ..., -0.4064, -0.4957,  0.2079]],\n\n        [[-0.1893,  0.3441,  0.3934,  ..., -1.1659,  0.1232, -0.7636]],\n\n        [[ 0.8200, -0.3971, -0.2547,  ...,  0.6752, -0.6345, -0.2071]],\n\n        ...,\n\n        [[-0.1047,  0.0238, -0.1989,  ...,  0.0022,  0.0259,  0.3637]],\n\n        [[ 0.0309,  0.2163, -0.4064,  ...,  0.0229, -0.3999, -0.2156]],\n\n        [[ 0.0394,  0.1884, -0.4690,  ..., -0.0498, -0.5239, -0.0483]]]))\nLength Ooutputstput[1]:\n12\nSample from Output[1], first hidden layer:\ntensor([[[ 0.0095, -0.0154, -0.0060,  ..., -0.0189, -0.0411, -0.0025]],\n\n        [[-0.0082,  0.0970, -0.0516,  ..., -0.0475, -0.0350, -0.0446]],\n\n        [[ 0.0028,  0.0029, -0.0050,  ...,  0.0228, -0.0612, -0.0213]],\n\n        ...,\n\n        [[-0.0238, -0.0061, -0.0163,  ..., -0.0160,  0.0431, -0.0309]],\n\n        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n\n        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]])\nSample shape, first hidden layer\ngetting the last tensor for XLNet\ntensor([-2.1453e-01,  9.8638e-01, -1.3569e+00,  4.9744e-02, -2.0580e-01,\n         2.0028e-01, -1.1084e+00, -2.4160e-01,  5.4918e-01,  6.1399e-01,\n         2.2983e-01, -2.1084e+00,  4.5196e-01, -4.5227e-01,  1.9634e+00,\n        -7.0501e-01,  7.6946e-01,  1.3087e+00,  4.6494e-01, -1.6070e+00,\n         8.9788e-01, -1.1989e+00, -4.9769e-02, -7.3024e-01, -4.6916e-01,\n         6.4692e-01, -1.2097e+00,  9.8949e-01, -1.8701e-01, -1.4202e+00,\n         3.2042e-01, -2.4059e-02,  1.2414e+00,  7.3341e-02, -8.1281e-01,\n        -9.1461e-01, -1.1381e+00,  3.1421e-01,  8.6016e-01,  1.2998e-02,\n         5.2342e-01,  5.1991e-01, -1.0325e+00, -3.0562e-01, -2.7786e-01,\n        -5.8735e-01,  1.1165e+00,  1.5297e+00, -6.8228e-01,  3.2196e-01,\n        -3.0287e-01, -6.1153e-01, -1.8400e+00, -6.7133e-01,  6.4504e-01,\n         7.3167e-01,  2.7735e-01, -1.7589e+00,  4.9479e-01,  1.1035e+00,\n         2.3803e-03,  1.0375e+00, -2.1944e-01, -1.5443e+00,  4.3127e-01,\n         1.3444e+00, -1.4885e+00,  4.8591e-01,  6.0964e-01, -5.2496e-01,\n        -1.6359e-01,  1.6733e+00,  1.0346e-01, -1.0006e+00, -2.9769e+00,\n         3.2755e-01,  2.3209e+00,  3.0375e-01, -1.5163e+00,  1.2310e+00,\n         1.2857e+00,  1.0182e+00, -2.1612e-02,  4.0715e-01, -4.6098e-01,\n        -7.3214e-01,  2.0897e+00, -2.0317e+00,  4.8229e-01, -2.0518e+00,\n         3.3633e-01,  2.9072e-01,  2.1798e+00, -9.9940e-01,  2.2056e-02,\n        -6.1339e-01,  2.9633e+00, -1.4316e+00, -7.5785e-01,  5.0299e-01,\n        -1.0213e-01, -9.9785e-01, -3.4671e-01,  4.2762e-01,  3.0119e+00,\n         1.1853e+00, -1.1546e+00,  1.0388e+00,  1.4966e-01,  1.4104e-01,\n         2.3719e+00, -4.4957e-01, -1.0572e-01, -7.8094e-01, -5.7102e-01,\n         1.8318e-01, -1.8381e+00,  1.5358e+00,  1.5481e-01, -4.9581e-01,\n        -2.2278e+00,  1.2948e+00, -1.0413e+00,  2.8898e-01, -1.2084e-01,\n         1.7138e-01,  2.7609e+00,  1.9643e+00, -3.0080e-01,  1.4541e-01,\n         4.0629e-01,  1.3333e+00,  1.1593e+00, -7.5110e-01,  3.0733e-01,\n        -7.4196e-01,  2.7876e-01,  8.9721e-01,  2.3159e-01, -5.5106e-01,\n        -2.4755e+00,  6.2858e-02, -1.1050e-01,  6.1831e-01,  3.3976e-01,\n        -5.9876e-01, -2.8440e-01, -1.7728e+00,  8.7163e-01, -8.0275e-01,\n         1.0852e+00,  1.9542e-01,  1.8726e+00,  7.4482e-01,  1.5311e-01,\n         2.6769e+00,  6.6298e-01, -1.7538e+00, -5.7625e-02, -4.5067e-01,\n         1.4741e-01,  1.4166e+00,  7.5462e-01,  5.7376e-01,  1.2701e-01,\n         1.1588e+00,  9.0293e-01,  1.3042e-01,  1.2332e+00, -1.9588e+00,\n         3.9885e-02,  2.1610e-01, -1.1540e-01, -2.0532e+00,  1.7119e-01,\n         2.5524e+00,  1.0150e-01, -1.6426e+00,  1.2831e+00,  1.3772e+00,\n        -9.3901e-01,  6.9770e-02, -2.1853e+00,  8.7528e-02, -6.9599e-01,\n         8.6451e-01, -4.3844e-01, -1.2232e-01,  2.2238e-01,  1.4665e+00,\n         3.8622e-01, -1.5519e-01,  1.1260e+00, -4.7662e-01, -1.6015e+00,\n         3.1916e-01,  1.1152e+00,  1.2062e+00,  5.3036e-02,  1.0497e+00,\n        -6.1548e-01,  1.5715e+00, -2.2236e+00,  8.6250e-02, -8.2571e-01,\n        -3.3818e+00, -3.9739e-01,  5.9320e-01, -1.0722e+00, -2.1490e+00,\n        -3.6851e-01, -4.9082e-01, -3.5355e-02, -4.8317e-01,  2.0705e+00,\n        -1.1507e+00, -6.5444e-01, -1.6963e+00,  1.4567e+00,  8.6049e-01,\n        -6.5158e-01,  2.0209e+00,  1.1277e+00, -6.2164e-01, -1.7967e-01,\n         1.7883e+00,  1.1099e+00, -1.0780e-01,  1.8339e+00, -1.1829e+00,\n         9.1396e-01, -1.0373e+00,  1.1238e-01, -1.0181e+00, -1.5731e+00,\n         6.5384e-01, -9.0480e-01,  1.1720e+00, -7.5896e-01,  6.6610e-01,\n        -1.3671e-01,  1.2426e+00, -7.4945e-01, -2.3609e-01, -8.3539e-01,\n         1.7154e+00, -1.2646e+00, -1.4290e+00,  1.1565e-01,  8.0008e-01,\n        -8.0761e-01, -5.9724e-01, -1.2979e+00,  2.0907e+00, -9.5528e-01,\n         5.2270e-01, -1.1853e+00,  2.0436e-01, -1.1253e+00, -2.0493e+00,\n        -9.9545e-01,  1.4973e+00,  2.6603e-02, -1.6016e-01,  5.5180e-03,\n         6.4405e-01,  8.9588e-02, -4.8602e-01,  7.1840e-01,  1.3879e-01,\n        -1.9723e+00, -1.0912e+00, -2.1722e+00,  1.3910e+00,  9.0147e-01,\n         1.2536e+00, -1.6592e+00,  2.1257e+00, -1.9984e+00,  1.4908e+00,\n         3.2881e-01, -1.4587e+00, -5.2943e-01,  1.8709e-01, -3.1590e+00,\n        -4.9768e-01, -1.4314e-01,  9.3489e-01,  1.0437e+00, -4.3188e-01,\n        -5.2128e-01, -4.6545e-01, -8.2936e-01, -1.3454e+00,  5.1912e-02,\n         3.9470e-02,  1.1484e+00, -5.1803e-01,  1.3648e+00,  1.3714e-01,\n        -6.4435e-01, -8.4210e-01, -1.7412e+00,  2.0659e+00,  1.0211e+00,\n        -1.2989e+00,  1.2540e-01,  1.9543e-01,  5.7408e-01,  4.1764e-02,\n        -4.9603e-02, -1.6279e+00,  9.4856e-01,  2.0670e+00,  5.3050e-01,\n         5.8467e-01,  1.0871e+00, -4.0797e-01,  1.1409e+00, -1.0692e+00,\n         2.9354e-01, -8.9125e-01, -2.4840e-01, -1.6881e+00,  7.4998e-01,\n         1.1048e+00, -7.1387e-01,  1.0101e+00, -1.1354e+00, -1.4488e-01,\n        -3.8969e+00, -3.5645e-01, -1.4997e+00,  4.3670e-01,  5.8357e-01,\n        -2.1449e-01,  6.6824e-01, -3.5385e+00,  8.6140e-01,  2.2280e+00,\n        -8.6619e-01, -1.1507e-01, -4.4029e-01, -3.5025e-01,  7.8375e-01,\n         1.2814e+00,  1.5552e-01, -2.1280e+00,  2.1976e-01, -2.7340e+00,\n        -1.2524e+00, -1.9484e+00,  1.6327e+00, -1.7756e+00, -1.3328e+00,\n         2.6205e-01,  1.3682e+00, -8.5758e-01, -9.7497e-02, -1.7539e+00,\n        -1.1427e+00,  4.4934e-02, -1.2070e+00, -1.8945e-01, -3.6902e-02,\n        -1.1454e+00, -4.9156e-01,  1.6384e+00, -3.8166e-01,  3.7253e-01,\n        -2.2684e-01,  3.1305e-01,  1.4779e+00,  3.3009e-01, -5.2985e-01,\n        -1.0515e+00, -1.5627e+00,  9.3483e-01, -1.2519e+00,  2.2258e-01,\n         2.9920e-01,  4.6717e-03,  9.7037e-01,  9.3146e-02,  1.3370e+00,\n        -6.8196e-01,  4.5106e-01,  1.6951e+00, -7.6028e-01,  1.1101e+00,\n        -2.6384e+00, -1.3144e+00, -1.9607e-02, -1.4172e+00,  7.8883e-01,\n        -3.6962e-01,  1.4095e+00,  1.1067e+00,  1.2178e+00,  5.8441e-01,\n        -2.1983e+00, -1.4277e+00,  9.8459e-02,  6.9648e-01, -5.0838e-01,\n         2.4521e-01, -1.0173e+00, -4.2939e-01,  3.9808e-01, -2.2220e+00,\n         1.3788e+00, -1.9310e+00, -1.2716e+00,  1.8036e+00, -1.7245e+00,\n        -7.4479e-01,  1.5149e-01,  8.9425e-01,  1.5406e+00, -4.0202e-02,\n         1.9446e+00,  1.8900e-01, -3.1360e-01, -1.7611e-01, -1.1113e+00,\n         1.1721e-01, -1.1700e+00, -5.4249e-02,  2.4942e+00,  9.3798e-01,\n         4.8418e-01,  6.4945e-01, -1.3807e+00,  1.2400e+00,  1.8008e-01,\n        -2.4113e+00, -8.9756e-02, -7.2267e-02, -1.0999e+00, -1.4695e-01,\n        -2.6345e+00, -1.4727e+00, -1.6954e+00, -7.7998e-01, -6.2965e-01,\n        -1.7881e-01,  5.1728e-01, -3.9755e-01, -1.6394e+00, -1.0089e+00,\n         1.9862e+00,  1.4115e-01,  5.2719e-01, -1.2833e+00, -5.0331e-01,\n         1.3289e+00, -7.0200e-01, -1.3461e+00, -1.6944e+00,  1.4036e+00,\n         9.8193e-01, -6.4413e-01,  6.2290e-01, -2.2626e-01,  6.6347e-01,\n         1.5525e+00, -2.1926e+00, -1.3891e+00,  2.8898e-01,  1.2426e+00,\n         2.3103e-01, -1.1832e-01, -1.1906e+00,  2.7114e-01,  5.1607e-01,\n        -2.2052e+00,  9.2495e-01, -1.2769e-01,  1.5139e+00,  7.8701e-01,\n         7.4431e-01, -1.6492e-01,  1.3957e-01,  1.2427e+00, -3.9936e-01,\n         1.0268e+00, -8.1749e-01, -1.6536e+00,  1.1583e+00, -6.0149e-01,\n         1.3037e+00, -1.5865e+00, -1.2954e+00, -1.7343e+00,  1.8969e-01,\n         1.2042e+00,  1.3569e-01,  3.0305e-01, -6.6328e-01,  4.2401e-01,\n        -1.8526e-01,  1.7876e-01,  1.1274e+00,  7.1105e-01,  1.0016e+00,\n        -1.0542e+00,  2.2776e-01,  1.1125e+00, -1.3756e-01, -5.9948e-01,\n         1.2532e-01,  4.4875e-01, -3.2859e+00,  7.0325e-01, -1.4238e+00,\n        -5.1156e-01,  1.2198e+00, -9.7238e-02, -1.4447e-01, -1.6210e+00,\n         1.5796e+00, -1.3337e+00, -9.3004e-01,  2.2620e-01, -8.0319e-01,\n        -5.7009e-01,  6.2643e-01,  8.3831e-01,  3.4209e-01,  5.5753e-01,\n         2.1434e+00,  5.1033e-01,  1.3235e-02, -1.3793e+00, -1.1478e+00,\n         1.8771e+00,  1.7251e+00,  1.5407e-01,  9.1308e-01,  1.9434e+00,\n         8.8198e-01, -3.5997e-01,  2.9065e+00,  9.1285e-01,  1.9355e+00,\n         1.6056e+00,  5.6866e-01,  2.0768e+00,  8.6787e-01, -2.2931e-02,\n         8.3905e-01,  7.2683e-01, -6.0721e-02, -1.8671e+00,  9.9623e-01,\n        -6.5899e-01, -1.2392e+00, -2.0062e-01, -2.1718e-01, -1.7522e+00,\n         3.4222e-01,  9.3519e-01,  2.6198e-01,  4.0103e-01,  3.3233e-01,\n        -3.4144e-01,  8.7069e-01, -6.8507e-01, -9.2126e-01, -8.6281e-02,\n        -1.5173e-01, -7.7543e-01, -3.1340e-01,  9.4671e-01, -6.6685e-01,\n         1.5142e+00,  5.8379e-01, -1.7654e-01, -8.8808e-01,  3.5779e-01,\n         2.7198e-01,  3.7428e-02, -6.6635e-01,  1.2272e+00, -8.8221e-01,\n         2.7930e-01,  1.3620e+00, -1.0215e-01,  1.6067e+00,  8.8354e-01,\n        -9.1101e-01, -1.1794e-01,  3.5961e-01,  8.0464e-01, -1.1855e+00,\n        -1.8708e+00, -1.7048e-02,  4.2338e-01,  1.1876e+00,  3.7264e-01,\n        -1.6972e+00,  6.2226e-01, -2.8600e+00, -9.8395e-01, -1.0913e+00,\n         3.8669e+00,  1.5290e-01, -8.7765e-01, -7.4308e-01, -2.4191e-01,\n        -3.0232e-01, -5.3909e-01, -1.0366e+00, -3.0453e-01,  1.0702e+00,\n         1.5789e+00, -7.1529e-01, -1.7520e+00,  3.2104e-01,  1.2714e+00,\n         8.2794e-01, -1.7445e-01, -8.6687e-01, -1.5617e+00,  1.4571e-01,\n        -2.8659e+00,  4.4525e-01,  2.6224e+00,  8.5578e-01, -9.3843e-01,\n         3.5320e+00, -1.0526e+00, -3.3882e-01,  1.4104e+00,  1.0956e+00,\n        -1.5400e+00, -3.7736e-01, -1.0202e+00, -2.0895e-01,  3.1079e-03,\n         4.9658e-01,  4.6117e-01, -1.1214e+00,  2.6658e-02, -3.0991e+00,\n         4.4106e-01, -1.2591e+00, -1.0375e+00,  1.6153e+00,  3.2657e-01,\n        -4.0481e-02, -5.7306e-01, -3.0543e-02,  2.2385e+00, -7.5224e-01,\n         8.0265e-01, -7.6170e-01,  1.1493e+00, -1.5330e-01,  2.1856e+00,\n        -2.0625e+00,  1.4827e-01, -8.6528e-01,  4.0643e-01,  4.9671e-01,\n        -1.2934e+00,  1.3553e+00,  8.9471e+00,  1.2010e-01,  6.1475e-01,\n         1.7927e-01, -1.6315e+00,  6.1841e-01, -6.8354e-01,  1.1180e+00,\n        -6.6650e-01,  8.7611e-01, -1.4513e+00,  5.6974e-01,  4.2913e-01,\n        -9.8622e-01, -3.8110e-01, -4.5555e-01,  2.0714e+00, -1.5918e+00,\n         4.5925e-01,  1.1442e+00, -5.1364e-01, -1.8913e+00,  3.0349e-01,\n        -1.6888e+00,  1.9609e+00,  7.7843e-01,  2.0561e+00,  2.7419e-01,\n        -1.1826e-01, -2.0508e+00, -2.0858e-01, -1.2502e+00,  9.7472e-01,\n         1.1850e+00,  3.6681e+00, -2.0549e+00, -9.8252e-01,  1.0629e+00,\n         8.4684e-02,  6.8038e-01, -6.4833e-01, -1.8062e+00,  3.7127e-01,\n        -2.0946e+00, -6.5025e-02, -3.8743e+00, -3.3673e+00, -1.5098e+00,\n        -1.5450e-01,  4.5021e-01, -5.0693e-01, -2.5925e+00, -9.5800e-01,\n         4.3311e-01,  8.3092e-01,  1.3436e+00, -1.1999e+00, -2.8139e-01,\n        -8.3131e-01, -3.0034e-01,  9.4809e-01,  2.0950e+00,  2.9593e+00,\n         6.2950e-01, -8.3132e-01,  3.6281e+00,  3.5306e-01, -2.1721e+00,\n        -1.3455e+00, -1.7943e-01, -7.0785e-01,  8.8919e-01,  1.9405e-01,\n         3.8667e-01, -3.9391e-01, -6.9740e-01,  8.2895e-01, -2.6867e-01,\n         9.6548e-01,  1.5483e+00,  8.2763e-01,  4.6222e-02,  8.3067e-01,\n        -1.4615e-01,  6.9635e-01, -2.8679e-02,  2.6626e-01,  8.7689e-02,\n        -2.6214e-01,  1.8634e+00, -1.7545e+00, -1.1901e+00,  1.1427e+00,\n         4.2843e-01,  2.5285e+00, -9.5746e-01,  6.3902e-01,  1.9181e+00,\n        -9.4142e-02, -1.1556e+00, -1.5487e+00])\ntensor([-2.1453e-01,  9.8638e-01, -1.3569e+00,  4.9744e-02, -2.0580e-01,\n         2.0028e-01, -1.1084e+00, -2.4160e-01,  5.4918e-01,  6.1399e-01,\n         2.2983e-01, -2.1084e+00,  4.5196e-01, -4.5227e-01,  1.9634e+00,\n        -7.0501e-01,  7.6946e-01,  1.3087e+00,  4.6494e-01, -1.6070e+00,\n         8.9788e-01, -1.1989e+00, -4.9769e-02, -7.3024e-01, -4.6916e-01,\n         6.4692e-01, -1.2097e+00,  9.8949e-01, -1.8701e-01, -1.4202e+00,\n         3.2042e-01, -2.4059e-02,  1.2414e+00,  7.3341e-02, -8.1281e-01,\n        -9.1461e-01, -1.1381e+00,  3.1421e-01,  8.6016e-01,  1.2998e-02,\n         5.2342e-01,  5.1991e-01, -1.0325e+00, -3.0562e-01, -2.7786e-01,\n        -5.8735e-01,  1.1165e+00,  1.5297e+00, -6.8228e-01,  3.2196e-01,\n        -3.0287e-01, -6.1153e-01, -1.8400e+00, -6.7133e-01,  6.4504e-01,\n         7.3167e-01,  2.7735e-01, -1.7589e+00,  4.9479e-01,  1.1035e+00,\n         2.3803e-03,  1.0375e+00, -2.1944e-01, -1.5443e+00,  4.3127e-01,\n         1.3444e+00, -1.4885e+00,  4.8591e-01,  6.0964e-01, -5.2496e-01,\n        -1.6359e-01,  1.6733e+00,  1.0346e-01, -1.0006e+00, -2.9769e+00,\n         3.2755e-01,  2.3209e+00,  3.0375e-01, -1.5163e+00,  1.2310e+00,\n         1.2857e+00,  1.0182e+00, -2.1612e-02,  4.0715e-01, -4.6098e-01,\n        -7.3214e-01,  2.0897e+00, -2.0317e+00,  4.8229e-01, -2.0518e+00,\n         3.3633e-01,  2.9072e-01,  2.1798e+00, -9.9940e-01,  2.2056e-02,\n        -6.1339e-01,  2.9633e+00, -1.4316e+00, -7.5785e-01,  5.0299e-01,\n        -1.0213e-01, -9.9785e-01, -3.4671e-01,  4.2762e-01,  3.0119e+00,\n         1.1853e+00, -1.1546e+00,  1.0388e+00,  1.4966e-01,  1.4104e-01,\n         2.3719e+00, -4.4957e-01, -1.0572e-01, -7.8094e-01, -5.7102e-01,\n         1.8318e-01, -1.8381e+00,  1.5358e+00,  1.5481e-01, -4.9581e-01,\n        -2.2278e+00,  1.2948e+00, -1.0413e+00,  2.8898e-01, -1.2084e-01,\n         1.7138e-01,  2.7609e+00,  1.9643e+00, -3.0080e-01,  1.4541e-01,\n         4.0629e-01,  1.3333e+00,  1.1593e+00, -7.5110e-01,  3.0733e-01,\n        -7.4196e-01,  2.7876e-01,  8.9721e-01,  2.3159e-01, -5.5106e-01,\n        -2.4755e+00,  6.2858e-02, -1.1050e-01,  6.1831e-01,  3.3976e-01,\n        -5.9876e-01, -2.8440e-01, -1.7728e+00,  8.7163e-01, -8.0275e-01,\n         1.0852e+00,  1.9542e-01,  1.8726e+00,  7.4482e-01,  1.5311e-01,\n         2.6769e+00,  6.6298e-01, -1.7538e+00, -5.7625e-02, -4.5067e-01,\n         1.4741e-01,  1.4166e+00,  7.5462e-01,  5.7376e-01,  1.2701e-01,\n         1.1588e+00,  9.0293e-01,  1.3042e-01,  1.2332e+00, -1.9588e+00,\n         3.9885e-02,  2.1610e-01, -1.1540e-01, -2.0532e+00,  1.7119e-01,\n         2.5524e+00,  1.0150e-01, -1.6426e+00,  1.2831e+00,  1.3772e+00,\n        -9.3901e-01,  6.9770e-02, -2.1853e+00,  8.7528e-02, -6.9599e-01,\n         8.6451e-01, -4.3844e-01, -1.2232e-01,  2.2238e-01,  1.4665e+00,\n         3.8622e-01, -1.5519e-01,  1.1260e+00, -4.7662e-01, -1.6015e+00,\n         3.1916e-01,  1.1152e+00,  1.2062e+00,  5.3036e-02,  1.0497e+00,\n        -6.1548e-01,  1.5715e+00, -2.2236e+00,  8.6250e-02, -8.2571e-01,\n        -3.3818e+00, -3.9739e-01,  5.9320e-01, -1.0722e+00, -2.1490e+00,\n        -3.6851e-01, -4.9082e-01, -3.5355e-02, -4.8317e-01,  2.0705e+00,\n        -1.1507e+00, -6.5444e-01, -1.6963e+00,  1.4567e+00,  8.6049e-01,\n        -6.5158e-01,  2.0209e+00,  1.1277e+00, -6.2164e-01, -1.7967e-01,\n         1.7883e+00,  1.1099e+00, -1.0780e-01,  1.8339e+00, -1.1829e+00,\n         9.1396e-01, -1.0373e+00,  1.1238e-01, -1.0181e+00, -1.5731e+00,\n         6.5384e-01, -9.0480e-01,  1.1720e+00, -7.5896e-01,  6.6610e-01,\n        -1.3671e-01,  1.2426e+00, -7.4945e-01, -2.3609e-01, -8.3539e-01,\n         1.7154e+00, -1.2646e+00, -1.4290e+00,  1.1565e-01,  8.0008e-01,\n        -8.0761e-01, -5.9724e-01, -1.2979e+00,  2.0907e+00, -9.5528e-01,\n         5.2270e-01, -1.1853e+00,  2.0436e-01, -1.1253e+00, -2.0493e+00,\n        -9.9545e-01,  1.4973e+00,  2.6603e-02, -1.6016e-01,  5.5180e-03,\n         6.4405e-01,  8.9588e-02, -4.8602e-01,  7.1840e-01,  1.3879e-01,\n        -1.9723e+00, -1.0912e+00, -2.1722e+00,  1.3910e+00,  9.0147e-01,\n         1.2536e+00, -1.6592e+00,  2.1257e+00, -1.9984e+00,  1.4908e+00,\n         3.2881e-01, -1.4587e+00, -5.2943e-01,  1.8709e-01, -3.1590e+00,\n        -4.9768e-01, -1.4314e-01,  9.3489e-01,  1.0437e+00, -4.3188e-01,\n        -5.2128e-01, -4.6545e-01, -8.2936e-01, -1.3454e+00,  5.1912e-02,\n         3.9470e-02,  1.1484e+00, -5.1803e-01,  1.3648e+00,  1.3714e-01,\n        -6.4435e-01, -8.4210e-01, -1.7412e+00,  2.0659e+00,  1.0211e+00,\n        -1.2989e+00,  1.2540e-01,  1.9543e-01,  5.7408e-01,  4.1764e-02,\n        -4.9603e-02, -1.6279e+00,  9.4856e-01,  2.0670e+00,  5.3050e-01,\n         5.8467e-01,  1.0871e+00, -4.0797e-01,  1.1409e+00, -1.0692e+00,\n         2.9354e-01, -8.9125e-01, -2.4840e-01, -1.6881e+00,  7.4998e-01,\n         1.1048e+00, -7.1387e-01,  1.0101e+00, -1.1354e+00, -1.4488e-01,\n        -3.8969e+00, -3.5645e-01, -1.4997e+00,  4.3670e-01,  5.8357e-01,\n        -2.1449e-01,  6.6824e-01, -3.5385e+00,  8.6140e-01,  2.2280e+00,\n        -8.6619e-01, -1.1507e-01, -4.4029e-01, -3.5025e-01,  7.8375e-01,\n         1.2814e+00,  1.5552e-01, -2.1280e+00,  2.1976e-01, -2.7340e+00,\n        -1.2524e+00, -1.9484e+00,  1.6327e+00, -1.7756e+00, -1.3328e+00,\n         2.6205e-01,  1.3682e+00, -8.5758e-01, -9.7497e-02, -1.7539e+00,\n        -1.1427e+00,  4.4934e-02, -1.2070e+00, -1.8945e-01, -3.6902e-02,\n        -1.1454e+00, -4.9156e-01,  1.6384e+00, -3.8166e-01,  3.7253e-01,\n        -2.2684e-01,  3.1305e-01,  1.4779e+00,  3.3009e-01, -5.2985e-01,\n        -1.0515e+00, -1.5627e+00,  9.3483e-01, -1.2519e+00,  2.2258e-01,\n         2.9920e-01,  4.6717e-03,  9.7037e-01,  9.3146e-02,  1.3370e+00,\n        -6.8196e-01,  4.5106e-01,  1.6951e+00, -7.6028e-01,  1.1101e+00,\n        -2.6384e+00, -1.3144e+00, -1.9607e-02, -1.4172e+00,  7.8883e-01,\n        -3.6962e-01,  1.4095e+00,  1.1067e+00,  1.2178e+00,  5.8441e-01,\n        -2.1983e+00, -1.4277e+00,  9.8459e-02,  6.9648e-01, -5.0838e-01,\n         2.4521e-01, -1.0173e+00, -4.2939e-01,  3.9808e-01, -2.2220e+00,\n         1.3788e+00, -1.9310e+00, -1.2716e+00,  1.8036e+00, -1.7245e+00,\n        -7.4479e-01,  1.5149e-01,  8.9425e-01,  1.5406e+00, -4.0202e-02,\n         1.9446e+00,  1.8900e-01, -3.1360e-01, -1.7611e-01, -1.1113e+00,\n         1.1721e-01, -1.1700e+00, -5.4249e-02,  2.4942e+00,  9.3798e-01,\n         4.8418e-01,  6.4945e-01, -1.3807e+00,  1.2400e+00,  1.8008e-01,\n        -2.4113e+00, -8.9756e-02, -7.2267e-02, -1.0999e+00, -1.4695e-01,\n        -2.6345e+00, -1.4727e+00, -1.6954e+00, -7.7998e-01, -6.2965e-01,\n        -1.7881e-01,  5.1728e-01, -3.9755e-01, -1.6394e+00, -1.0089e+00,\n         1.9862e+00,  1.4115e-01,  5.2719e-01, -1.2833e+00, -5.0331e-01,\n         1.3289e+00, -7.0200e-01, -1.3461e+00, -1.6944e+00,  1.4036e+00,\n         9.8193e-01, -6.4413e-01,  6.2290e-01, -2.2626e-01,  6.6347e-01,\n         1.5525e+00, -2.1926e+00, -1.3891e+00,  2.8898e-01,  1.2426e+00,\n         2.3103e-01, -1.1832e-01, -1.1906e+00,  2.7114e-01,  5.1607e-01,\n        -2.2052e+00,  9.2495e-01, -1.2769e-01,  1.5139e+00,  7.8701e-01,\n         7.4431e-01, -1.6492e-01,  1.3957e-01,  1.2427e+00, -3.9936e-01,\n         1.0268e+00, -8.1749e-01, -1.6536e+00,  1.1583e+00, -6.0149e-01,\n         1.3037e+00, -1.5865e+00, -1.2954e+00, -1.7343e+00,  1.8969e-01,\n         1.2042e+00,  1.3569e-01,  3.0305e-01, -6.6328e-01,  4.2401e-01,\n        -1.8526e-01,  1.7876e-01,  1.1274e+00,  7.1105e-01,  1.0016e+00,\n        -1.0542e+00,  2.2776e-01,  1.1125e+00, -1.3756e-01, -5.9948e-01,\n         1.2532e-01,  4.4875e-01, -3.2859e+00,  7.0325e-01, -1.4238e+00,\n        -5.1156e-01,  1.2198e+00, -9.7238e-02, -1.4447e-01, -1.6210e+00,\n         1.5796e+00, -1.3337e+00, -9.3004e-01,  2.2620e-01, -8.0319e-01,\n        -5.7009e-01,  6.2643e-01,  8.3831e-01,  3.4209e-01,  5.5753e-01,\n         2.1434e+00,  5.1033e-01,  1.3235e-02, -1.3793e+00, -1.1478e+00,\n         1.8771e+00,  1.7251e+00,  1.5407e-01,  9.1308e-01,  1.9434e+00,\n         8.8198e-01, -3.5997e-01,  2.9065e+00,  9.1285e-01,  1.9355e+00,\n         1.6056e+00,  5.6866e-01,  2.0768e+00,  8.6787e-01, -2.2931e-02,\n         8.3905e-01,  7.2683e-01, -6.0721e-02, -1.8671e+00,  9.9623e-01,\n        -6.5899e-01, -1.2392e+00, -2.0062e-01, -2.1718e-01, -1.7522e+00,\n         3.4222e-01,  9.3519e-01,  2.6198e-01,  4.0103e-01,  3.3233e-01,\n        -3.4144e-01,  8.7069e-01, -6.8507e-01, -9.2126e-01, -8.6281e-02,\n        -1.5173e-01, -7.7543e-01, -3.1340e-01,  9.4671e-01, -6.6685e-01,\n         1.5142e+00,  5.8379e-01, -1.7654e-01, -8.8808e-01,  3.5779e-01,\n         2.7198e-01,  3.7428e-02, -6.6635e-01,  1.2272e+00, -8.8221e-01,\n         2.7930e-01,  1.3620e+00, -1.0215e-01,  1.6067e+00,  8.8354e-01,\n        -9.1101e-01, -1.1794e-01,  3.5961e-01,  8.0464e-01, -1.1855e+00,\n        -1.8708e+00, -1.7048e-02,  4.2338e-01,  1.1876e+00,  3.7264e-01,\n        -1.6972e+00,  6.2226e-01, -2.8600e+00, -9.8395e-01, -1.0913e+00,\n         3.8669e+00,  1.5290e-01, -8.7765e-01, -7.4308e-01, -2.4191e-01,\n        -3.0232e-01, -5.3909e-01, -1.0366e+00, -3.0453e-01,  1.0702e+00,\n         1.5789e+00, -7.1529e-01, -1.7520e+00,  3.2104e-01,  1.2714e+00,\n         8.2794e-01, -1.7445e-01, -8.6687e-01, -1.5617e+00,  1.4571e-01,\n        -2.8659e+00,  4.4525e-01,  2.6224e+00,  8.5578e-01, -9.3843e-01,\n         3.5320e+00, -1.0526e+00, -3.3882e-01,  1.4104e+00,  1.0956e+00,\n        -1.5400e+00, -3.7736e-01, -1.0202e+00, -2.0895e-01,  3.1079e-03,\n         4.9658e-01,  4.6117e-01, -1.1214e+00,  2.6658e-02, -3.0991e+00,\n         4.4106e-01, -1.2591e+00, -1.0375e+00,  1.6153e+00,  3.2657e-01,\n        -4.0481e-02, -5.7306e-01, -3.0543e-02,  2.2385e+00, -7.5224e-01,\n         8.0265e-01, -7.6170e-01,  1.1493e+00, -1.5330e-01,  2.1856e+00,\n        -2.0625e+00,  1.4827e-01, -8.6528e-01,  4.0643e-01,  4.9671e-01,\n        -1.2934e+00,  1.3553e+00,  8.9471e+00,  1.2010e-01,  6.1475e-01,\n         1.7927e-01, -1.6315e+00,  6.1841e-01, -6.8354e-01,  1.1180e+00,\n        -6.6650e-01,  8.7611e-01, -1.4513e+00,  5.6974e-01,  4.2913e-01,\n        -9.8622e-01, -3.8110e-01, -4.5555e-01,  2.0714e+00, -1.5918e+00,\n         4.5925e-01,  1.1442e+00, -5.1364e-01, -1.8913e+00,  3.0349e-01,\n        -1.6888e+00,  1.9609e+00,  7.7843e-01,  2.0561e+00,  2.7419e-01,\n        -1.1826e-01, -2.0508e+00, -2.0858e-01, -1.2502e+00,  9.7472e-01,\n         1.1850e+00,  3.6681e+00, -2.0549e+00, -9.8252e-01,  1.0629e+00,\n         8.4684e-02,  6.8038e-01, -6.4833e-01, -1.8062e+00,  3.7127e-01,\n        -2.0946e+00, -6.5025e-02, -3.8743e+00, -3.3673e+00, -1.5098e+00,\n        -1.5450e-01,  4.5021e-01, -5.0693e-01, -2.5925e+00, -9.5800e-01,\n         4.3311e-01,  8.3092e-01,  1.3436e+00, -1.1999e+00, -2.8139e-01,\n        -8.3131e-01, -3.0034e-01,  9.4809e-01,  2.0950e+00,  2.9593e+00,\n         6.2950e-01, -8.3132e-01,  3.6281e+00,  3.5306e-01, -2.1721e+00,\n        -1.3455e+00, -1.7943e-01, -7.0785e-01,  8.8919e-01,  1.9405e-01,\n         3.8667e-01, -3.9391e-01, -6.9740e-01,  8.2895e-01, -2.6867e-01,\n         9.6548e-01,  1.5483e+00,  8.2763e-01,  4.6222e-02,  8.3067e-01,\n        -1.4615e-01,  6.9635e-01, -2.8679e-02,  2.6626e-01,  8.7689e-02,\n        -2.6214e-01,  1.8634e+00, -1.7545e+00, -1.1901e+00,  1.1427e+00,\n         4.2843e-01,  2.5285e+00, -9.5746e-01,  6.3902e-01,  1.9181e+00,\n        -9.4142e-02, -1.1556e+00, -1.5487e+00])\n"
    }
   ],
   "source": [
    "sample_text=\"In May 1980, a Cuban man named Tony Montana (Al Pacino) claims asylum, in Florida, USA, and is in search of the \\\"American Dream\\\" after departing Cuba in the Mariel boatlift of 1980. When questioned by three tough-talking INS officials, they notice a tattoo on Tony's left arm of a black heart with a pitchfork through it, which identifies him as a hitman, and detain him in a camp called 'Freedomtown' with other Cubans, including Tony's best friend and former Cuban Army buddy Manolo \\\"Manny Ray\\\" Ribiera (Steven Bauer), under the local I-95 expressway while the government evaluates their visa petitions.After 30 days of governmental dithering and camp rumors, Manny receives an offer from the Cuban Mafia which he quickly relays to Tony. If they kill Emilio Rebenga (Roberto Contreras) a former aide to Fidel Castro who is now detained in Freedomtown, they will receive green cards. Tony agrees, and kills Rebenga during a riot at Freedomtown.\"\n",
    "test_output=get_Encodings(sample_text,verbose=True)\n",
    "print(test_output)"
   ]
  },
  {
   "source": [
    "Now testing the helper functions for input from a dataframe to see if we get the correct desired output for our work"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                               processed_synopsis_t1                              processed_synopsis_t2\n0  note: this synopsis is for the orginal italian...  note: synopsis orginal italian release segment...\n1  two thousand years ago, nhagruul the foul, a s...  thousand years ago, nhagruul foul, sorcerer re...\n2  matuschek is, a gift store in budapest, is the...  matuschek is, gift store budapest, workplace a...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>processed_synopsis_t1</th>\n      <th>processed_synopsis_t2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>note: this synopsis is for the orginal italian...</td>\n      <td>note: synopsis orginal italian release segment...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>two thousand years ago, nhagruul the foul, a s...</td>\n      <td>thousand years ago, nhagruul foul, sorcerer re...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>matuschek is, a gift store in budapest, is the...</td>\n      <td>matuschek is, gift store budapest, workplace a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "sampleDF=mpstDF_processsed[[\"processed_synopsis_t1\",\"processed_synopsis_t2\"]].head(3)\n",
    "sampleDF[\"processed_synopsis_t1\"]=sampleDF[\"processed_synopsis_t1\"].apply(lambda x : x[:1000])\n",
    "sampleDF[\"processed_synopsis_t2\"]=sampleDF[\"processed_synopsis_t2\"].apply(lambda x : x[:1000])\n",
    "display(sampleDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Text:\nnote: this synopsis ....\ntext_encoding_tensor:\ntensor([[ 2039,    60,    52,    17, 12681, 24243,    27,    28,    18,    49,\n          5043,   212,    36,   212,   884,   947,    33,    18, 11860,    25,\n            52,  1028,   374,     9,  7157,   590,    17,  4597,   368,  1372,\n         17945,   139,  6941, 14753,    20,    18, 10255,  2460,    88,    21,\n            18, 19707,   318,    34,    18,   116,  9494,  3462,    20,  1819,\n            26,     9,   305,  2289,  4044,   117,    17,    10,    98, 23808,\n           529, 23445,  2151,    11,    27,    48,  5320,    19,   227,    13,\n         18736,    17, 21605,    23,   884,   547,    13, 11163,    61,  4094,\n            22,    62, 16707,    19,  9464,  3246,    99,    48,  2060,    78,\n            90,    85,  1262,  2298,    39,  4238,    37,    24,   461,    20,\n          4572,   983,  1624,     9,    18, 18380,   802,  2399,   666,    34,\n         23675,    19,    62,  2002,    13,   450,  6863,    61,    51,  1050,\n          7028,    40,  1840,     9,    17,  4044,   117,    27, 17360,    28,\n            36,    30,    62,  5674,    29,  5594,    18,   326,    25,  3679,\n             9,   589,    28,    17,  9253,  4639,    19,    17,  4044,   117,\n          6377,    62, 13026, 11139,    17,    98,  1449,    17,    10,  9027,\n          1011,  1296, 11916,  2865,    11,     9,    18,    87,   412,    47,\n            72, 28011,    28,   106,    92,    19,    57,    17,  4044,   117,\n            27,  1028,    29,    85,    27,    18,   114,    65,    61,    64,\n           222,    62,     9,    17,    98,  1449, 10893,    22,   280,    95,\n            29,   391,     9,  2465,   266,    19, 23675,  1624,   292,    19,\n          7559,    29,   116,   918,    61,    85,  1624,    28,  1754,    19,\n            43,    53,    47,    45,  9621,     9,  4027,    22,    17,  4044,\n           117,    19,    17,    98,  1449,    27,    18, 18380,  7693,  6104,\n          2076, 23675,     9,  8499,     4,     3]])\nshape:\ntorch.Size([1, 246])\nattention_mask_tensor:\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1]])\nshape:\ntorch.Size([1, 246])\noutputs:\n(tensor([[[ 0.4933, -1.1920, -1.5274,  ...,  3.0771, -0.8879, -0.9605],\n         [ 1.1407,  1.0814, -2.7806,  ...,  2.2058, -0.8002, -0.2620],\n         [-0.3778, -0.6504, -3.2482,  ...,  1.8693, -1.8730, -0.0049],\n         ...,\n         [-1.6888, -0.7179, -1.0047,  ..., -0.6184,  0.7949,  0.0195],\n         [-1.6697,  0.2477, -1.7644,  ..., -1.0568,  0.2891, -0.4864],\n         [-1.5468,  0.4124, -1.3485,  ..., -1.2316,  0.1243, -0.1708]]]), (tensor([[[-0.0083,  0.0112, -0.0417,  ..., -0.0380, -0.0314, -0.0489]],\n\n        [[ 0.0630, -0.0162,  0.0030,  ..., -0.0294, -0.0021, -0.0215]],\n\n        [[-0.0248,  0.0275, -0.0304,  ..., -0.0309, -0.0517, -0.0115]],\n\n        ...,\n\n        [[-0.0659, -0.0170, -0.0140,  ..., -0.0381, -0.0744, -0.0161]],\n\n        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n\n        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]]), tensor([[[-0.3717,  0.1767, -0.7357,  ..., -0.3212, -0.1315,  0.1602]],\n\n        [[ 1.1113,  1.0329,  0.5437,  ..., -0.3979,  0.0189,  0.5195]],\n\n        [[-0.3506, -0.0334, -0.7820,  ..., -0.2851, -2.1737,  0.1372]],\n\n        ...,\n\n        [[-1.0677, -1.3324, -1.0613,  ..., -1.1253, -0.8176,  0.1688]],\n\n        [[ 1.2730, -0.5700, -1.8173,  ...,  0.5090,  0.5348,  0.2416]],\n\n        [[ 0.6121, -0.3278, -2.4509,  ...,  0.4140, -0.0738,  0.4864]]]), tensor([[[-0.3168,  0.0135, -0.3362,  ..., -0.2852,  0.1140,  0.6741]],\n\n        [[ 0.7680,  1.2466,  0.8970,  ...,  0.1299,  0.3330,  1.3868]],\n\n        [[-0.1463, -0.4146, -0.8516,  ..., -0.0169, -2.0740,  0.4103]],\n\n        ...,\n\n        [[-0.6739, -1.2693, -1.3271,  ..., -0.9336, -0.2479, -0.1529]],\n\n        [[ 0.6208, -0.5825, -2.0870,  ...,  0.2839,  0.8639,  0.0945]],\n\n        [[ 0.2198, -0.5618, -2.7772,  ...,  0.5215,  0.2798,  0.0544]]]), tensor([[[-0.4920,  0.0856, -0.0387,  ...,  0.4822, -0.2564,  0.5527]],\n\n        [[ 0.5091,  1.3840,  0.5272,  ...,  0.3739,  0.2885,  2.1220]],\n\n        [[ 0.1336,  0.1807, -0.7280,  ...,  0.3859, -2.7167,  0.7208]],\n\n        ...,\n\n        [[-0.8899, -1.3942, -2.0794,  ..., -0.9489,  0.2636,  0.0205]],\n\n        [[ 0.4967, -0.2337, -2.0755,  ...,  1.1864,  1.0922, -1.1212]],\n\n        [[ 0.4644, -0.1541, -2.7527,  ...,  1.1436,  0.4998, -0.0995]]]), tensor([[[-8.5001e-01, -4.4033e-01, -3.8987e-01,  ...,  8.3983e-01,\n          -3.7527e-01,  8.2460e-01]],\n\n        [[ 4.1715e-01,  8.4992e-01,  2.8989e-03,  ...,  6.8369e-01,\n          -6.0978e-01,  1.8736e+00]],\n\n        [[-3.0452e-02, -4.5919e-01, -1.1213e+00,  ...,  4.3880e-01,\n          -3.7259e+00,  7.2557e-01]],\n\n        ...,\n\n        [[-9.1677e-01, -1.6709e+00, -2.1565e+00,  ...,  4.3695e-02,\n           1.5001e-01,  1.9282e-01]],\n\n        [[ 9.4310e-01, -6.6965e-01, -2.2446e+00,  ...,  1.0103e+00,\n           1.0484e+00, -1.9912e-02]],\n\n        [[ 1.1775e+00, -4.7535e-01, -2.5400e+00,  ...,  7.8640e-01,\n           2.6333e-01,  8.5746e-01]]]), tensor([[[-0.4371, -0.4599, -0.1149,  ...,  0.8101, -0.7177, -0.0726]],\n\n        [[ 0.1070,  0.6039, -0.2669,  ...,  1.0479, -0.5184,  0.7663]],\n\n        [[-0.2381, -0.6448, -1.0878,  ...,  0.9624, -2.1699, -0.0962]],\n\n        ...,\n\n        [[-0.8547, -1.1347, -1.7201,  ...,  0.0293,  0.0889,  0.0323]],\n\n        [[ 0.3757, -0.3803, -1.9211,  ...,  1.0787,  0.3782, -1.0387]],\n\n        [[ 0.7878, -0.4103, -2.0311,  ...,  0.4828, -0.1212, -0.2111]]]), tensor([[[-0.2763, -0.3729, -0.7731,  ...,  1.0683, -1.0078, -0.2297]],\n\n        [[ 0.4611,  0.6795, -1.0338,  ...,  1.1558, -1.3165,  0.8770]],\n\n        [[ 0.3116, -0.4881, -1.5998,  ...,  1.0207, -2.1212,  0.3672]],\n\n        ...,\n\n        [[-0.9228, -1.4560, -2.0227,  ...,  0.4839, -0.1825, -0.1258]],\n\n        [[ 0.5496, -0.1049, -2.3214,  ...,  0.9069, -0.2271, -0.5613]],\n\n        [[ 0.8196, -0.1208, -1.8822,  ...,  0.7014, -0.5232, -0.1607]]]), tensor([[[ 0.3257, -0.4448, -0.1957,  ...,  0.9156, -0.9259, -0.4481]],\n\n        [[ 0.7163,  0.5644, -0.5879,  ...,  0.8532, -1.0729,  0.6803]],\n\n        [[ 0.4031, -0.3662, -1.0876,  ...,  0.8002, -2.0826,  0.0574]],\n\n        ...,\n\n        [[-0.7909, -1.2103, -1.4659,  ...,  0.2548,  0.1259, -0.0914]],\n\n        [[ 0.2237, -0.2069, -1.2685,  ..., -0.3568, -0.0106, -0.2619]],\n\n        [[ 0.4330, -0.0513, -0.9063,  ..., -0.5758, -0.0398,  0.1826]]]), tensor([[[ 0.6391, -0.3426, -0.2488,  ...,  0.8456, -0.4329, -0.4440]],\n\n        [[ 0.8058,  1.1470, -0.7979,  ...,  0.8404, -0.8909,  0.6504]],\n\n        [[ 1.0640,  0.0405, -1.5676,  ...,  0.6028, -2.2115, -0.7233]],\n\n        ...,\n\n        [[-0.1569, -0.7254, -1.4531,  ...,  0.4089, -0.3253, -0.3888]],\n\n        [[ 0.4806,  0.0267, -1.4551,  ...,  0.1760, -0.2696, -0.1394]],\n\n        [[ 0.6588, -0.0171, -0.8798,  ...,  0.0267, -0.2890,  0.3852]]]), tensor([[[ 0.5067, -0.7351, -0.1417,  ...,  0.7543, -0.9919, -0.5611]],\n\n        [[ 0.4935,  0.6908, -0.8825,  ...,  0.6821, -1.0584,  0.2163]],\n\n        [[ 1.2580, -0.1225, -1.6348,  ...,  0.1836, -1.6994, -0.3226]],\n\n        ...,\n\n        [[-0.1968, -0.8288, -1.0984,  ...,  0.4785, -0.4564,  0.0359]],\n\n        [[ 0.2613,  0.0563, -1.1556,  ...,  0.0881,  0.0734, -0.2671]],\n\n        [[ 0.5799,  0.1747, -0.6004,  ..., -0.0782,  0.1118,  0.0730]]]), tensor([[[ 0.9816, -0.9419, -0.7005,  ...,  1.4425, -1.2929,  0.3707]],\n\n        [[ 0.8950,  0.6388, -1.3217,  ...,  1.0849, -1.4124,  0.7009]],\n\n        [[ 1.6064,  0.2683, -2.5727,  ...,  0.5859, -2.3453, -0.6522]],\n\n        ...,\n\n        [[-0.4674, -0.9215, -1.0562,  ...,  0.5708, -0.2011, -0.1681]],\n\n        [[ 0.1170,  0.4859, -1.0555,  ...,  0.2881, -0.1242, -0.5110]],\n\n        [[ 0.3433,  0.5380, -0.4546,  ...,  0.1495,  0.1095, -0.0423]]]), tensor([[[ 0.5189, -0.8339, -0.3631,  ...,  1.2841, -0.2331,  0.1093]],\n\n        [[ 0.4296,  0.4405, -0.6623,  ...,  0.8722, -0.5318,  0.6447]],\n\n        [[ 0.7016, -0.0182, -1.6994,  ...,  0.4845, -1.0478, -0.1570]],\n\n        ...,\n\n        [[-0.2891, -0.6899, -0.5960,  ...,  0.1836, -0.2311, -0.1458]],\n\n        [[-0.2625,  0.3744, -0.7016,  ...,  0.1573, -0.1972, -0.3098]],\n\n        [[-0.0809,  0.4677, -0.4155,  ...,  0.1188, -0.0903, -0.0056]]])))\nLenght of outputs 2\noutputs[0]:\ntensor([[[ 0.4933, -1.1920, -1.5274,  ...,  3.0771, -0.8879, -0.9605],\n         [ 1.1407,  1.0814, -2.7806,  ...,  2.2058, -0.8002, -0.2620],\n         [-0.3778, -0.6504, -3.2482,  ...,  1.8693, -1.8730, -0.0049],\n         ...,\n         [-1.6888, -0.7179, -1.0047,  ..., -0.6184,  0.7949,  0.0195],\n         [-1.6697,  0.2477, -1.7644,  ..., -1.0568,  0.2891, -0.4864],\n         [-1.5468,  0.4124, -1.3485,  ..., -1.2316,  0.1243, -0.1708]]])\noutputs[0].shape:\ntorch.Size([1, 246, 768])\noutputs[1]:\n(tensor([[[-0.0083,  0.0112, -0.0417,  ..., -0.0380, -0.0314, -0.0489]],\n\n        [[ 0.0630, -0.0162,  0.0030,  ..., -0.0294, -0.0021, -0.0215]],\n\n        [[-0.0248,  0.0275, -0.0304,  ..., -0.0309, -0.0517, -0.0115]],\n\n        ...,\n\n        [[-0.0659, -0.0170, -0.0140,  ..., -0.0381, -0.0744, -0.0161]],\n\n        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n\n        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]]), tensor([[[-0.3717,  0.1767, -0.7357,  ..., -0.3212, -0.1315,  0.1602]],\n\n        [[ 1.1113,  1.0329,  0.5437,  ..., -0.3979,  0.0189,  0.5195]],\n\n        [[-0.3506, -0.0334, -0.7820,  ..., -0.2851, -2.1737,  0.1372]],\n\n        ...,\n\n        [[-1.0677, -1.3324, -1.0613,  ..., -1.1253, -0.8176,  0.1688]],\n\n        [[ 1.2730, -0.5700, -1.8173,  ...,  0.5090,  0.5348,  0.2416]],\n\n        [[ 0.6121, -0.3278, -2.4509,  ...,  0.4140, -0.0738,  0.4864]]]), tensor([[[-0.3168,  0.0135, -0.3362,  ..., -0.2852,  0.1140,  0.6741]],\n\n        [[ 0.7680,  1.2466,  0.8970,  ...,  0.1299,  0.3330,  1.3868]],\n\n        [[-0.1463, -0.4146, -0.8516,  ..., -0.0169, -2.0740,  0.4103]],\n\n        ...,\n\n        [[-0.6739, -1.2693, -1.3271,  ..., -0.9336, -0.2479, -0.1529]],\n\n        [[ 0.6208, -0.5825, -2.0870,  ...,  0.2839,  0.8639,  0.0945]],\n\n        [[ 0.2198, -0.5618, -2.7772,  ...,  0.5215,  0.2798,  0.0544]]]), tensor([[[-0.4920,  0.0856, -0.0387,  ...,  0.4822, -0.2564,  0.5527]],\n\n        [[ 0.5091,  1.3840,  0.5272,  ...,  0.3739,  0.2885,  2.1220]],\n\n        [[ 0.1336,  0.1807, -0.7280,  ...,  0.3859, -2.7167,  0.7208]],\n\n        ...,\n\n        [[-0.8899, -1.3942, -2.0794,  ..., -0.9489,  0.2636,  0.0205]],\n\n        [[ 0.4967, -0.2337, -2.0755,  ...,  1.1864,  1.0922, -1.1212]],\n\n        [[ 0.4644, -0.1541, -2.7527,  ...,  1.1436,  0.4998, -0.0995]]]), tensor([[[-8.5001e-01, -4.4033e-01, -3.8987e-01,  ...,  8.3983e-01,\n          -3.7527e-01,  8.2460e-01]],\n\n        [[ 4.1715e-01,  8.4992e-01,  2.8989e-03,  ...,  6.8369e-01,\n          -6.0978e-01,  1.8736e+00]],\n\n        [[-3.0452e-02, -4.5919e-01, -1.1213e+00,  ...,  4.3880e-01,\n          -3.7259e+00,  7.2557e-01]],\n\n        ...,\n\n        [[-9.1677e-01, -1.6709e+00, -2.1565e+00,  ...,  4.3695e-02,\n           1.5001e-01,  1.9282e-01]],\n\n        [[ 9.4310e-01, -6.6965e-01, -2.2446e+00,  ...,  1.0103e+00,\n           1.0484e+00, -1.9912e-02]],\n\n        [[ 1.1775e+00, -4.7535e-01, -2.5400e+00,  ...,  7.8640e-01,\n           2.6333e-01,  8.5746e-01]]]), tensor([[[-0.4371, -0.4599, -0.1149,  ...,  0.8101, -0.7177, -0.0726]],\n\n        [[ 0.1070,  0.6039, -0.2669,  ...,  1.0479, -0.5184,  0.7663]],\n\n        [[-0.2381, -0.6448, -1.0878,  ...,  0.9624, -2.1699, -0.0962]],\n\n        ...,\n\n        [[-0.8547, -1.1347, -1.7201,  ...,  0.0293,  0.0889,  0.0323]],\n\n        [[ 0.3757, -0.3803, -1.9211,  ...,  1.0787,  0.3782, -1.0387]],\n\n        [[ 0.7878, -0.4103, -2.0311,  ...,  0.4828, -0.1212, -0.2111]]]), tensor([[[-0.2763, -0.3729, -0.7731,  ...,  1.0683, -1.0078, -0.2297]],\n\n        [[ 0.4611,  0.6795, -1.0338,  ...,  1.1558, -1.3165,  0.8770]],\n\n        [[ 0.3116, -0.4881, -1.5998,  ...,  1.0207, -2.1212,  0.3672]],\n\n        ...,\n\n        [[-0.9228, -1.4560, -2.0227,  ...,  0.4839, -0.1825, -0.1258]],\n\n        [[ 0.5496, -0.1049, -2.3214,  ...,  0.9069, -0.2271, -0.5613]],\n\n        [[ 0.8196, -0.1208, -1.8822,  ...,  0.7014, -0.5232, -0.1607]]]), tensor([[[ 0.3257, -0.4448, -0.1957,  ...,  0.9156, -0.9259, -0.4481]],\n\n        [[ 0.7163,  0.5644, -0.5879,  ...,  0.8532, -1.0729,  0.6803]],\n\n        [[ 0.4031, -0.3662, -1.0876,  ...,  0.8002, -2.0826,  0.0574]],\n\n        ...,\n\n        [[-0.7909, -1.2103, -1.4659,  ...,  0.2548,  0.1259, -0.0914]],\n\n        [[ 0.2237, -0.2069, -1.2685,  ..., -0.3568, -0.0106, -0.2619]],\n\n        [[ 0.4330, -0.0513, -0.9063,  ..., -0.5758, -0.0398,  0.1826]]]), tensor([[[ 0.6391, -0.3426, -0.2488,  ...,  0.8456, -0.4329, -0.4440]],\n\n        [[ 0.8058,  1.1470, -0.7979,  ...,  0.8404, -0.8909,  0.6504]],\n\n        [[ 1.0640,  0.0405, -1.5676,  ...,  0.6028, -2.2115, -0.7233]],\n\n        ...,\n\n        [[-0.1569, -0.7254, -1.4531,  ...,  0.4089, -0.3253, -0.3888]],\n\n        [[ 0.4806,  0.0267, -1.4551,  ...,  0.1760, -0.2696, -0.1394]],\n\n        [[ 0.6588, -0.0171, -0.8798,  ...,  0.0267, -0.2890,  0.3852]]]), tensor([[[ 0.5067, -0.7351, -0.1417,  ...,  0.7543, -0.9919, -0.5611]],\n\n        [[ 0.4935,  0.6908, -0.8825,  ...,  0.6821, -1.0584,  0.2163]],\n\n        [[ 1.2580, -0.1225, -1.6348,  ...,  0.1836, -1.6994, -0.3226]],\n\n        ...,\n\n        [[-0.1968, -0.8288, -1.0984,  ...,  0.4785, -0.4564,  0.0359]],\n\n        [[ 0.2613,  0.0563, -1.1556,  ...,  0.0881,  0.0734, -0.2671]],\n\n        [[ 0.5799,  0.1747, -0.6004,  ..., -0.0782,  0.1118,  0.0730]]]), tensor([[[ 0.9816, -0.9419, -0.7005,  ...,  1.4425, -1.2929,  0.3707]],\n\n        [[ 0.8950,  0.6388, -1.3217,  ...,  1.0849, -1.4124,  0.7009]],\n\n        [[ 1.6064,  0.2683, -2.5727,  ...,  0.5859, -2.3453, -0.6522]],\n\n        ...,\n\n        [[-0.4674, -0.9215, -1.0562,  ...,  0.5708, -0.2011, -0.1681]],\n\n        [[ 0.1170,  0.4859, -1.0555,  ...,  0.2881, -0.1242, -0.5110]],\n\n        [[ 0.3433,  0.5380, -0.4546,  ...,  0.1495,  0.1095, -0.0423]]]), tensor([[[ 0.5189, -0.8339, -0.3631,  ...,  1.2841, -0.2331,  0.1093]],\n\n        [[ 0.4296,  0.4405, -0.6623,  ...,  0.8722, -0.5318,  0.6447]],\n\n        [[ 0.7016, -0.0182, -1.6994,  ...,  0.4845, -1.0478, -0.1570]],\n\n        ...,\n\n        [[-0.2891, -0.6899, -0.5960,  ...,  0.1836, -0.2311, -0.1458]],\n\n        [[-0.2625,  0.3744, -0.7016,  ...,  0.1573, -0.1972, -0.3098]],\n\n        [[-0.0809,  0.4677, -0.4155,  ...,  0.1188, -0.0903, -0.0056]]]))\nLength Ooutputstput[1]:\n12\nSample from Output[1], first hidden layer:\ntensor([[[-0.0083,  0.0112, -0.0417,  ..., -0.0380, -0.0314, -0.0489]],\n\n        [[ 0.0630, -0.0162,  0.0030,  ..., -0.0294, -0.0021, -0.0215]],\n\n        [[-0.0248,  0.0275, -0.0304,  ..., -0.0309, -0.0517, -0.0115]],\n\n        ...,\n\n        [[-0.0659, -0.0170, -0.0140,  ..., -0.0381, -0.0744, -0.0161]],\n\n        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n\n        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]])\nSample shape, first hidden layer\ngetting the last tensor for XLNet\ntensor([-1.5468e+00,  4.1238e-01, -1.3485e+00,  8.1071e-01,  6.7494e-01,\n        -1.9946e+00, -2.7063e+00,  9.8970e-01,  1.4540e+00,  1.8433e+00,\n        -1.5463e+00, -1.3059e+00,  7.1876e-01, -8.2810e-02,  1.9127e+00,\n         2.5946e-01,  8.8492e-01,  4.0023e-01, -8.9860e-01, -3.5978e-01,\n         2.1720e-01, -1.5405e-01, -1.2455e-01, -1.0175e-01,  3.5794e-01,\n        -1.5675e-01, -1.2292e-01,  1.7174e+00,  6.6208e-01, -1.0243e-01,\n         2.4917e-01,  1.9936e-01,  1.2147e+00,  1.7267e+00,  2.4979e-01,\n        -1.7191e+00, -1.5752e+00,  1.3167e+00,  2.7095e+00,  2.2549e-01,\n        -8.9836e-02, -6.8747e-01, -5.2374e-01, -8.5563e-01, -1.2496e-01,\n        -1.6223e+00,  5.1655e-01,  1.9685e+00, -7.9067e-01,  1.3159e+00,\n         3.6281e-01, -5.9299e-02, -1.7030e+00,  3.0240e-01, -6.2362e-01,\n         2.9879e-01,  2.3882e+00, -7.8125e-01, -6.2950e-01, -6.8442e-03,\n         6.5377e-01,  1.6099e-01,  1.0437e+00, -7.9226e-01, -7.6838e-01,\n         3.0705e-01, -5.7610e-01,  1.8568e+00,  3.9707e-01, -5.6731e-02,\n        -1.6269e+00,  5.4913e-01,  1.8409e-01,  3.4226e-02, -6.6628e-01,\n        -1.0443e+00, -1.7209e-01, -1.7148e-01, -8.5392e-01,  6.6788e-01,\n         7.1839e-01, -5.4796e-01,  9.6325e-01,  6.3558e-01, -5.3731e-01,\n        -5.3907e-01,  1.9625e+00, -2.2191e+00,  6.9932e-01, -5.7082e-01,\n        -9.2275e-01, -1.3263e+00,  3.9156e+00, -7.0337e-01, -1.2436e-01,\n         2.5219e-01,  8.2846e-01,  6.5108e-01,  3.1672e-01, -1.0827e+00,\n        -2.5976e-01,  2.3946e+00,  1.5683e+00,  7.4879e-01,  7.8799e-01,\n         2.3907e+00, -3.5213e-01,  2.1280e+00, -4.7507e-01, -2.6553e-01,\n         8.8572e-01, -9.6262e-01,  6.4015e-01, -1.8442e+00, -1.5611e-01,\n        -1.6956e+00, -1.3662e+00,  1.3955e+00, -2.6118e-02,  4.9600e-01,\n        -5.2130e-03,  3.1280e-01,  2.3824e-01,  7.4876e-01,  1.5025e-01,\n        -2.6263e-01,  1.8873e+00,  1.6590e+00,  4.4845e-01, -2.0948e-01,\n        -3.9668e-01,  3.2085e-01,  8.9681e-01,  6.4657e-01,  5.0699e-01,\n        -6.2201e-01,  2.0909e+00, -7.3309e-01,  1.1853e+00, -1.4611e+00,\n        -8.1182e-01,  4.0973e-01,  2.9663e-01,  1.8377e+00,  1.4481e+00,\n        -5.2655e-02,  1.2399e+00, -2.4100e+00,  1.5926e+00, -1.0671e-01,\n        -3.7427e-01,  2.7297e+00,  8.0472e-01,  1.1489e+00, -5.5799e-01,\n         3.4004e+00,  9.2427e-01, -1.2153e+00,  9.5318e-01, -7.4822e-01,\n        -1.6866e+00,  1.3102e+00,  9.0677e-02,  2.3100e-01,  1.3479e+00,\n        -2.2552e-01, -5.3356e-02,  2.0700e+00,  1.6003e+00, -2.1008e+00,\n         1.3755e+00, -1.7204e-01,  6.4850e-01,  3.5431e-03, -1.6004e-01,\n         7.8703e-01,  1.6851e+00, -2.5436e-01, -1.7496e+00,  4.3289e-01,\n        -1.0293e+00, -4.7463e-02,  4.4038e-01, -2.6694e-01,  8.1071e-01,\n         8.2272e-01, -1.2758e-01,  4.7417e-02, -5.0910e-01, -6.3058e-01,\n         2.0061e-01, -1.1787e+00, -1.3899e+00, -1.1435e+00,  1.1692e+00,\n        -1.2932e+00,  1.0019e-01, -7.6216e-01, -1.3414e+00,  6.2437e-01,\n        -6.1783e-01,  7.2094e-01, -1.1629e+00, -1.2362e+00, -7.8230e-01,\n        -1.3272e+00, -1.2211e+00,  1.3331e+00, -2.6693e-01, -5.8471e-01,\n         1.7284e+00, -6.8896e-01, -9.8203e-01, -4.1630e-01,  2.0556e-02,\n        -5.2642e-01,  4.1401e-01, -4.8589e-01,  5.8830e-01, -7.7723e-01,\n         1.9665e+00,  2.5467e+00,  5.3548e-01, -8.8568e-01,  7.4367e-01,\n         1.9565e+00, -2.4573e-01,  3.6063e+00, -2.0635e+00, -4.8435e-01,\n         7.7897e-01, -4.3888e-01, -1.2678e-01, -7.7197e-01, -1.6203e+00,\n         7.8990e-01,  1.4594e+00,  1.7705e+00, -1.3629e-01,  2.6246e-01,\n         1.7456e+00,  4.2459e-01, -2.1311e+00,  4.2955e-02,  1.0718e-01,\n         2.1122e+00, -3.8901e-01, -1.3182e+00, -1.7332e+00,  6.5238e-01,\n         4.8287e-01, -9.8554e-01, -3.1922e+00,  8.7739e-01, -6.0187e-01,\n         5.6525e-01,  1.0494e+00,  2.4144e+00, -1.0389e+00, -1.0180e+00,\n        -1.6488e+00, -9.2612e-01, -8.9320e-01, -4.6657e-01, -5.8382e-01,\n        -4.6113e-01,  2.0204e+00, -1.6183e+00,  2.8565e-01,  8.3841e-01,\n        -1.6927e-01,  1.7029e+00, -1.0349e+00,  3.2535e+00,  1.2617e+00,\n        -8.7408e-02, -1.2508e+00, -1.3854e-01, -7.4781e-01,  2.8792e-01,\n         1.0615e+00,  7.1909e-01, -5.7981e-01, -2.1724e+00, -1.2137e+00,\n        -1.0066e+00,  1.3313e-01,  1.2391e-02,  1.2619e+00,  4.2889e-01,\n        -1.0350e+00,  1.3028e-01, -5.2564e-01, -9.7582e-01,  1.5454e+00,\n        -3.0973e-01, -1.9192e+00, -1.6493e+00, -2.1865e-01, -1.9326e+00,\n        -7.7945e-02, -8.2556e-01,  1.2571e-01,  5.7494e-01,  7.2058e-01,\n         5.3700e-01, -1.1602e+00,  4.7579e-01,  9.2292e-01, -4.5311e-01,\n        -1.2216e+00,  6.2207e-01, -3.8958e-02, -7.6933e-01,  8.0452e-01,\n         7.5937e-01,  1.0641e+00, -1.5334e+00,  9.8729e-01, -2.4849e-01,\n        -1.1523e+00,  9.9811e-01, -1.6102e-01, -8.9729e-01,  1.9048e+00,\n         1.0410e+00, -7.9294e-01,  1.6829e+00, -2.8166e+00,  7.7107e-02,\n        -2.5879e+00,  5.7042e-01,  1.0769e-01,  9.7793e-01, -5.4241e-01,\n        -2.6540e-01, -1.7334e+00, -1.3174e+00, -8.2412e-01,  2.2392e-01,\n        -7.8671e-01,  1.8970e+00, -5.5880e-01, -1.0432e+00,  1.0299e+00,\n        -3.6646e-01,  6.9866e-01, -1.3994e+00, -1.6609e-01, -1.3364e+00,\n        -1.3454e+00, -7.0916e-01,  5.2355e-01, -2.0228e+00,  1.2663e+00,\n        -4.8290e-02, -1.3464e-01,  1.4105e+00, -2.1588e+00,  5.5339e-01,\n         5.6257e-01,  7.4583e-01, -6.0478e-01, -1.0570e+00, -1.2123e-02,\n        -1.1971e+00,  4.9328e-01, -7.6473e-01,  1.8051e+00, -1.5367e+00,\n         9.0937e-01,  7.1756e-01, -2.5732e-01,  6.9815e-01, -1.6364e+00,\n        -4.3911e-01, -1.2570e+00, -1.1905e+00, -2.9444e+00, -1.2101e+00,\n        -3.5128e-01,  1.3636e+00,  4.3323e-01,  1.1312e-01,  7.3571e-01,\n         6.0437e-01,  6.6537e-01, -3.6402e-01,  5.0314e-01,  1.1971e+00,\n        -5.1952e-01, -1.2263e+00,  1.3027e+00, -1.0772e+00,  4.1941e-02,\n        -3.1191e+00,  4.2324e-01,  2.6843e-01, -4.7272e-01, -1.3963e+00,\n         1.2998e+00, -1.4260e+00,  1.2539e-01,  9.0149e-01,  8.9614e-01,\n         2.3680e-01,  6.3263e-01,  6.0333e-01,  9.1639e-02, -3.9356e-01,\n        -7.5629e-01, -1.3080e+00, -5.7312e-01,  1.9098e-01, -1.3336e+00,\n         9.5309e-01, -5.3851e-01,  2.1937e+00,  6.2738e-01,  1.1396e+00,\n         1.1669e-01,  7.5114e-01, -1.5911e-02, -1.8159e+00,  8.9223e-01,\n         2.9582e-01,  1.4139e+00, -5.8152e-01,  4.8687e-01,  4.7185e-02,\n        -1.8983e-01,  1.1443e+00, -1.7656e-01, -4.9086e-01, -2.0961e-01,\n        -5.4744e-01, -4.5262e-02,  1.8146e+00, -3.8143e-01,  4.3555e-01,\n        -3.0186e+00, -4.4666e-01,  1.4712e-01, -8.2687e-01, -3.1150e+00,\n        -5.0510e-01, -5.6949e-01,  7.5870e-01, -7.6843e-02, -1.1940e+00,\n         2.9484e+00, -1.2670e+00, -1.4054e+00, -8.3146e-01, -1.0834e+00,\n         1.2629e+00, -2.5806e-01, -3.2543e-01,  1.1415e+00,  7.3500e-02,\n        -3.5573e-02, -3.7692e-01, -8.1042e-01, -1.8344e-01, -1.8422e+00,\n         1.0744e-01, -7.6275e-01, -7.1804e-02,  5.0956e-01,  5.8870e-01,\n         1.3629e-01,  6.0656e-01, -1.6495e+00, -1.1707e+00,  5.0133e-01,\n        -1.2471e+00,  1.1481e+00,  4.3978e-02,  7.4062e-01,  7.7473e-01,\n        -1.5457e+00, -2.0296e-01, -2.5053e-01,  3.2507e+00, -2.5979e-02,\n         1.9202e-01,  7.5427e-02, -1.6599e+00,  1.3559e+00, -8.9478e-01,\n         1.1468e-01,  1.9398e+00, -1.3959e+00, -1.9246e+00,  9.5217e-01,\n         7.5259e-01,  4.3564e-01,  1.2239e+00,  5.6159e-01, -1.4648e+00,\n        -9.6445e-01,  1.6430e-01, -1.3701e+00, -4.6044e-01,  6.8279e-01,\n        -6.2779e-01,  8.8969e-01, -1.9083e-01, -4.3486e-01,  8.8001e-01,\n        -8.6143e-01,  9.6498e-01,  1.3471e-01, -6.8935e-01, -1.3154e+00,\n        -1.2314e+00,  1.1123e+00, -1.1407e+00,  1.3874e-01, -1.0104e+00,\n         9.6588e-01, -1.2870e+00,  6.9025e-01, -3.6807e-02,  1.4773e+00,\n         8.1568e-01,  7.4997e-01,  1.1626e+00, -2.2088e-01, -3.8834e-01,\n         9.6981e-01,  1.5865e-01, -9.5702e-01, -6.5672e-02,  2.4065e-01,\n        -8.6410e-02, -1.0530e+00,  6.4656e-01,  1.3654e+00,  7.6046e-01,\n         1.9773e-01, -1.3784e+00, -3.9955e-01, -6.3570e-01,  2.4144e-01,\n         4.5118e-01,  1.5707e+00,  1.3943e+00,  1.0834e+00, -1.0424e+00,\n         3.7233e-01, -2.7659e-01, -2.0939e-01, -1.9224e+00, -6.9858e-01,\n        -9.5700e-01, -1.3447e+00, -5.4390e-01, -2.0038e+00, -1.1455e+00,\n        -8.8843e-03,  2.9741e-01, -3.7462e-01,  2.8220e+00,  5.0875e-01,\n         4.1236e-02, -7.5666e-01, -2.2841e+00, -1.3913e+00,  1.3062e+00,\n        -2.7236e+00,  1.1195e-01, -8.3606e-01,  1.8382e+00, -6.9133e-01,\n         5.2211e-01,  1.5074e-01, -1.7995e-01,  5.6660e-01, -6.4599e-01,\n        -6.9801e-01, -8.8652e-01, -7.8868e-01,  3.6103e-01, -1.4961e+00,\n         4.8347e-01,  5.2790e-02, -1.4018e-01,  7.1999e-01,  3.5178e-01,\n         6.1484e-01,  3.7137e-01, -3.4510e-01,  8.5484e-01,  8.0501e-01,\n         2.4573e-01, -5.2775e-02,  6.7211e-01,  7.8048e-01, -1.1343e+00,\n         7.7489e-01, -8.3248e-01, -1.7643e+00, -1.4608e+00, -6.1971e-01,\n         3.2491e+00,  4.1387e-01,  6.5595e-02,  1.1778e-01, -6.8053e-01,\n         1.4533e+00,  5.4302e-01, -1.0729e+00,  1.0763e-01,  1.3283e+00,\n         2.2471e-01,  4.0492e-01, -1.5573e+00,  5.2697e-01, -4.0788e-01,\n         5.8849e-01, -1.1060e+00, -1.6912e+00,  2.0043e-01, -4.9513e-01,\n        -3.0953e-01, -1.1333e+00,  8.6849e-01, -3.2642e-01, -9.9657e-01,\n         3.5088e+00, -1.8746e+00, -2.3550e+00,  9.2398e-01,  2.1740e+00,\n        -3.1436e-01, -6.1456e-01,  1.4714e-01, -3.7582e-01,  1.6941e-01,\n         3.8042e-01, -6.5203e-01, -3.2933e-01, -4.0742e-01, -1.1593e+00,\n        -1.1926e+00,  2.0270e-01, -1.8661e+00,  1.8768e+00,  1.1769e+00,\n         9.0521e-01,  4.9193e-01, -3.0902e+00,  7.7252e-01, -5.6809e-01,\n        -5.6795e-01,  1.5843e+00, -3.0750e-01, -1.7495e+00,  1.2874e+00,\n        -1.2531e+00,  2.0353e+00, -5.9279e-02,  1.8626e+00, -2.9384e-01,\n        -6.4625e-01,  5.6523e-01, -3.1770e+01,  6.9277e-01, -7.2159e-01,\n         4.2380e-02,  1.0506e-01,  1.3204e-01, -1.6428e+00,  1.0794e+00,\n        -2.6821e-01,  2.5440e-01, -4.4804e-01, -1.2277e+00,  9.2016e-02,\n         2.0050e-02,  6.2390e-02, -1.0866e-02,  1.0380e-01, -1.2438e-01,\n        -5.9040e-01, -3.0274e-01, -1.1314e+00, -1.8714e+00, -2.9596e-01,\n        -2.1804e+00,  2.1274e+00,  1.7605e+00,  8.8668e-01, -6.2438e-01,\n         8.8561e-02,  1.2307e-01, -1.8607e+00,  1.3714e+00, -1.7888e-01,\n         2.8680e+00,  1.4985e+00, -5.7275e-01, -3.8410e-01,  5.7757e-01,\n        -1.1652e+00, -1.6352e-01, -8.0740e-01,  2.1468e-01, -7.1519e-01,\n        -1.6809e+00,  1.8410e+00, -1.4585e+00, -7.5596e-01,  1.1257e+00,\n        -4.4310e-01, -1.9747e+00, -3.2639e-02, -3.0232e+00, -5.9310e-02,\n         8.2489e-01,  1.5674e+00,  1.7832e+00, -1.8606e+00, -4.0138e-01,\n         2.5570e-01,  1.6032e-01,  2.2816e-01, -1.2105e+00,  8.6890e-01,\n         1.5128e+00,  8.3028e-01,  2.2367e+00,  7.4897e-01, -2.4442e+00,\n        -2.4877e-01, -8.2410e-01,  5.5521e-01,  4.4882e-01,  4.9589e-01,\n        -5.9597e-01, -6.7496e-01,  1.5972e-01, -1.1238e+00,  1.5888e+00,\n        -3.5432e-01,  3.8910e+00, -1.7053e-01,  1.1255e+00,  4.2584e-01,\n         1.1821e+00,  4.3361e-01, -4.0412e-01,  7.2178e-01, -9.0951e-01,\n        -7.9386e-01, -2.5185e-01,  5.4912e-01, -1.4863e-01,  2.1071e+00,\n         9.4096e-01,  5.6170e-01, -1.1491e+00, -8.2318e-01,  2.1504e+00,\n        -1.2316e+00,  1.2427e-01, -1.7078e-01])\nText:\ntwo thousand years a....\ntext_encoding_tensor:\ntensor([[   87,  4263,   123,   526,    19,    17,   180,  1664, 10862,  1797,\n            18,  8067,    19,    24,    17,    23,   218,  3720,   118,    61,\n         21651,    68,    25,  9453,    56,    18,  5901,    21,    18,  1912,\n            20, 15521,    19,   479,    68,    18,   239,    20,    45, 15912,\n           307,    21,    30, 18908,    68,     9, 12095,    37, 13184,    28,\n            18,   826,    19,    17,   180,  1664, 10862,  1797,  1245,    45,\n          3776,    22,    18,  9471, 11508,    23,    20,    18, 31460,   102,\n            29,    45,    17,    98,  6200,  3424,    74,  4338,     9,    25,\n            48,  2002, 20670,  2076, 11335,    19,    17,   180,  1664, 10862,\n           215,    23,  1563,    30,    17,  5653,   117,    68,    91,  2366,\n            19,    45,  8138, 23005,    91,    24,  1263,    19,    21,    45,\n          1487,    66,  1032,   403,    18, 17691,    22,  6995,    24,   522,\n           127,    17,  9908,    93,     9,  9737,    17,  9908,    93,    21,\n           321,  7627,  8529,  1004,    40,   300,  6062,    21,  7827,   262,\n          1808,  3357,    22,   188, 10379,    25,    18,  8415,    20,  4176,\n             9,    18,  5975,    23,    20,    17,  4597,  1507,   138,    55,\n         12095,    37,    52, 17025,    20,  4401,   259,    48,   374,    20,\n          7177, 16096, 14573,    40,    18, 21352,     9,    18, 13031,    23,\n            20,    18,   109,  2176, 23153,    48, 14557,    22, 22991,   937,\n            25,    18,   883,     9,    18, 22845,    20,    58, 10545,    30,\n           102,   312,    29,    17,  6884,   218,    19,    18,  7290,    20,\n           697,    19,   675,    18, 13031,    23,  1746,    24,  5023,  9073,\n            33,    59,    22,  3264,    45,   350,     9, 31645,    33, 11450,\n           327,    19,    18, 13031,    23,    20,    18,   109,  2176, 26358,\n            18,  6455,    29,    54, 23893,    18,   883,     4,     3]])\nshape:\ntorch.Size([1, 249])\nattention_mask_tensor:\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1]])\nshape:\ntorch.Size([1, 249])\noutputs:\n(tensor([[[-1.5256, -1.4488,  0.1495,  ...,  0.3308,  0.1241,  0.8293],\n         [-1.0124, -2.9765,  0.2557,  ...,  1.3121,  1.6442,  1.0584],\n         [-0.3693, -1.7811,  0.8048,  ...,  1.0571,  1.9698,  1.3454],\n         ...,\n         [-0.7483, -1.3709,  0.7150,  ..., -0.1992,  2.1973,  0.2071],\n         [-0.0854,  0.2224,  0.4188,  ...,  0.0802,  2.5034, -0.4041],\n         [-0.5634,  0.5462,  0.3155,  ..., -0.4278,  2.4666, -0.6191]]]), (tensor([[[ 0.0032, -0.0315,  0.0196,  ...,  0.0253, -0.0354, -0.0340]],\n\n        [[ 0.1135, -0.0894,  0.0006,  ..., -0.0089, -0.0294, -0.0396]],\n\n        [[ 0.0666, -0.0365,  0.0668,  ..., -0.0341, -0.0013, -0.0623]],\n\n        ...,\n\n        [[ 0.0128, -0.0286,  0.0141,  ...,  0.0234,  0.0169, -0.0086]],\n\n        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n\n        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]]), tensor([[[-0.5261, -1.1701, -0.3583,  ...,  0.4829, -1.0230, -0.9354]],\n\n        [[ 2.0216, -2.2310, -0.8636,  ..., -0.9406, -0.5933, -0.3295]],\n\n        [[ 1.5123, -0.7973,  0.3041,  ..., -0.9443, -0.5576, -0.9735]],\n\n        ...,\n\n        [[ 0.2693, -0.7767, -0.1016,  ...,  0.4463,  0.7056,  0.3659]],\n\n        [[ 1.2262, -0.4358, -1.5022,  ...,  0.4164,  0.4983,  0.1656]],\n\n        [[ 0.5799, -0.2595, -2.4905,  ...,  0.4727, -0.0567,  0.3934]]]), tensor([[[-0.7369, -0.6627, -1.1479,  ...,  0.4274, -1.4225, -0.8221]],\n\n        [[ 0.7590, -1.5501, -0.4110,  ..., -0.6870, -1.2824, -0.7416]],\n\n        [[ 1.2486, -0.8238,  0.8753,  ..., -1.0984, -0.3809, -0.8143]],\n\n        ...,\n\n        [[-0.1264, -0.8991,  0.2482,  ...,  0.2683,  0.7338,  0.1283]],\n\n        [[ 0.5458, -0.3588, -1.6524,  ...,  0.2807,  0.6365, -0.0170]],\n\n        [[ 0.1240, -0.3940, -2.8101,  ...,  0.5914,  0.1931,  0.0582]]]), tensor([[[-0.8036, -0.7057, -0.6769,  ...,  0.2780, -1.3504, -1.1373]],\n\n        [[ 0.4553, -2.0812, -0.3435,  ..., -0.7319, -0.7237, -1.3838]],\n\n        [[ 0.7760, -0.7381,  0.8065,  ..., -1.8279, -0.5578, -0.9260]],\n\n        ...,\n\n        [[-0.0902, -1.2476, -0.2099,  ..., -0.0310,  0.5697, -1.5466]],\n\n        [[ 0.2113, -0.2835, -1.6604,  ...,  1.0774,  0.6964, -1.3799]],\n\n        [[ 0.4627,  0.0389, -2.8822,  ...,  1.1632,  0.5634, -0.3277]]]), tensor([[[-0.5112, -0.8904, -0.5923,  ...,  0.5472, -1.4077, -0.2739]],\n\n        [[ 1.1617, -1.5938, -0.6740,  ..., -0.3580, -0.3637, -0.9155]],\n\n        [[ 1.2989, -0.7229,  0.3824,  ..., -0.8154, -0.3060, -0.2828]],\n\n        ...,\n\n        [[ 0.3510, -0.9354, -0.6970,  ..., -0.5928,  0.3514, -0.4261]],\n\n        [[ 0.5424, -0.5169, -2.1034,  ...,  0.9028,  1.0406, -0.4077]],\n\n        [[ 0.9847, -0.2464, -2.8325,  ...,  0.8564,  0.5528, -0.0879]]]), tensor([[[-0.0406, -0.4373, -0.1477,  ...,  0.1568, -0.8376, -0.2860]],\n\n        [[ 1.0536, -0.8088, -0.6082,  ..., -0.6802,  0.1741, -0.7474]],\n\n        [[ 1.4418, -0.0043, -0.0663,  ..., -0.8759, -0.1933, -0.8577]],\n\n        ...,\n\n        [[ 0.0944, -0.2383, -0.3422,  ..., -0.1536, -0.3056, -0.3251]],\n\n        [[ 0.3406, -0.1829, -1.6604,  ...,  0.8123,  0.0887, -1.3026]],\n\n        [[ 0.7220, -0.0490, -2.3900,  ...,  0.6851, -0.2784, -0.7982]]]), tensor([[[ 0.3538, -0.7011,  0.1355,  ...,  0.0912, -1.4751, -0.3032]],\n\n        [[ 1.7260, -1.2371, -0.5203,  ..., -0.4229, -0.0199, -0.7303]],\n\n        [[ 2.1150, -0.4711,  0.5520,  ..., -0.4018, -0.2267, -0.7367]],\n\n        ...,\n\n        [[-0.2094, -0.5073, -0.7549,  ...,  0.2137, -0.6454, -0.5232]],\n\n        [[ 0.5325, -0.2638, -1.9003,  ...,  0.9388, -0.0830, -0.3557]],\n\n        [[ 0.4565,  0.0699, -2.6016,  ...,  0.8593, -0.4189,  0.0271]]]), tensor([[[-9.9006e-04, -8.3685e-01,  5.9874e-01,  ...,  4.0764e-01,\n          -1.0346e+00, -4.0091e-01]],\n\n        [[ 1.4977e+00, -1.1171e+00, -7.4040e-02,  ...,  9.2290e-02,\n           8.7714e-01, -6.2549e-01]],\n\n        [[ 2.0026e+00, -6.1965e-01,  4.9363e-01,  ..., -1.8394e-01,\n           4.3372e-01, -4.5617e-01]],\n\n        ...,\n\n        [[-2.1229e-01, -7.7680e-01,  1.9733e-01,  ...,  1.2256e-01,\n           1.0972e-01, -2.0376e-01]],\n\n        [[ 8.3739e-01, -1.9654e-01, -1.0783e+00,  ...,  2.3006e-01,\n           1.5886e-01, -1.9944e-01]],\n\n        [[ 5.9456e-01,  1.1208e-01, -1.3362e+00,  ...,  4.8854e-01,\n           5.4456e-02, -3.8967e-02]]]), tensor([[[ 0.1950, -0.6356,  0.8977,  ..., -0.0475, -1.4947, -0.1426]],\n\n        [[ 1.8091, -0.7023,  0.3854,  ..., -0.4700,  0.3304, -0.3470]],\n\n        [[ 2.0336, -0.2752,  0.8669,  ..., -0.8400, -0.7042,  0.0057]],\n\n        ...,\n\n        [[-0.3159, -0.6435, -0.2026,  ..., -0.2428,  0.2846, -0.0893]],\n\n        [[ 1.1016,  0.2237, -1.2877,  ..., -0.0482,  0.0613, -0.7050]],\n\n        [[ 0.3958,  0.2539, -0.8466,  ...,  0.0174, -0.1932,  0.0343]]]), tensor([[[-0.0506, -0.6617,  0.3882,  ..., -0.0042, -1.0797, -0.0867]],\n\n        [[ 1.0111, -0.7698,  0.0408,  ..., -0.1930,  0.2902, -0.1821]],\n\n        [[ 1.3112, -0.3588,  0.7120,  ..., -0.5989, -0.4447,  0.0761]],\n\n        ...,\n\n        [[-0.3072, -0.6513, -0.6231,  ..., -0.2192,  0.8463,  0.1804]],\n\n        [[ 0.6510,  0.1915, -0.9260,  ...,  0.1047,  0.8065, -0.3097]],\n\n        [[ 0.1388,  0.0498, -0.2055,  ..., -0.0218,  0.0385,  0.0312]]]), tensor([[[-0.8866, -1.1458,  0.4816,  ..., -0.2999, -1.2026,  0.7538]],\n\n        [[ 0.3383, -1.6679, -0.0108,  ..., -0.3250,  0.6752,  0.3044]],\n\n        [[ 0.9515, -1.0855,  0.7353,  ..., -0.1509, -0.1007,  0.4642]],\n\n        ...,\n\n        [[-0.5375, -0.9538, -0.5101,  ..., -0.0157,  1.0665,  0.3272]],\n\n        [[ 0.4423, -0.0047, -0.4852,  ...,  0.1220,  1.0180, -0.6279]],\n\n        [[ 0.0357, -0.0250, -0.0915,  ...,  0.0430,  0.0432,  0.0476]]]), tensor([[[-0.3457, -0.9145,  0.0100,  ..., -0.4585, -0.9520,  1.1424]],\n\n        [[ 0.1617, -1.6695,  0.1308,  ..., -0.0765,  0.3736,  0.3521]],\n\n        [[ 0.5313, -1.2578,  0.4652,  ...,  0.0268,  0.3139,  0.6950]],\n\n        ...,\n\n        [[-0.1741, -1.1738,  0.1853,  ...,  0.0467,  0.7462,  0.8298]],\n\n        [[ 0.0781, -0.2275, -0.0092,  ...,  0.1665,  0.9218, -0.2769]],\n\n        [[-0.0587,  0.0722, -0.1692,  ...,  0.0122,  0.0301,  0.3692]]])))\nLenght of outputs 2\noutputs[0]:\ntensor([[[-1.5256, -1.4488,  0.1495,  ...,  0.3308,  0.1241,  0.8293],\n         [-1.0124, -2.9765,  0.2557,  ...,  1.3121,  1.6442,  1.0584],\n         [-0.3693, -1.7811,  0.8048,  ...,  1.0571,  1.9698,  1.3454],\n         ...,\n         [-0.7483, -1.3709,  0.7150,  ..., -0.1992,  2.1973,  0.2071],\n         [-0.0854,  0.2224,  0.4188,  ...,  0.0802,  2.5034, -0.4041],\n         [-0.5634,  0.5462,  0.3155,  ..., -0.4278,  2.4666, -0.6191]]])\noutputs[0].shape:\ntorch.Size([1, 249, 768])\noutputs[1]:\n(tensor([[[ 0.0032, -0.0315,  0.0196,  ...,  0.0253, -0.0354, -0.0340]],\n\n        [[ 0.1135, -0.0894,  0.0006,  ..., -0.0089, -0.0294, -0.0396]],\n\n        [[ 0.0666, -0.0365,  0.0668,  ..., -0.0341, -0.0013, -0.0623]],\n\n        ...,\n\n        [[ 0.0128, -0.0286,  0.0141,  ...,  0.0234,  0.0169, -0.0086]],\n\n        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n\n        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]]), tensor([[[-0.5261, -1.1701, -0.3583,  ...,  0.4829, -1.0230, -0.9354]],\n\n        [[ 2.0216, -2.2310, -0.8636,  ..., -0.9406, -0.5933, -0.3295]],\n\n        [[ 1.5123, -0.7973,  0.3041,  ..., -0.9443, -0.5576, -0.9735]],\n\n        ...,\n\n        [[ 0.2693, -0.7767, -0.1016,  ...,  0.4463,  0.7056,  0.3659]],\n\n        [[ 1.2262, -0.4358, -1.5022,  ...,  0.4164,  0.4983,  0.1656]],\n\n        [[ 0.5799, -0.2595, -2.4905,  ...,  0.4727, -0.0567,  0.3934]]]), tensor([[[-0.7369, -0.6627, -1.1479,  ...,  0.4274, -1.4225, -0.8221]],\n\n        [[ 0.7590, -1.5501, -0.4110,  ..., -0.6870, -1.2824, -0.7416]],\n\n        [[ 1.2486, -0.8238,  0.8753,  ..., -1.0984, -0.3809, -0.8143]],\n\n        ...,\n\n        [[-0.1264, -0.8991,  0.2482,  ...,  0.2683,  0.7338,  0.1283]],\n\n        [[ 0.5458, -0.3588, -1.6524,  ...,  0.2807,  0.6365, -0.0170]],\n\n        [[ 0.1240, -0.3940, -2.8101,  ...,  0.5914,  0.1931,  0.0582]]]), tensor([[[-0.8036, -0.7057, -0.6769,  ...,  0.2780, -1.3504, -1.1373]],\n\n        [[ 0.4553, -2.0812, -0.3435,  ..., -0.7319, -0.7237, -1.3838]],\n\n        [[ 0.7760, -0.7381,  0.8065,  ..., -1.8279, -0.5578, -0.9260]],\n\n        ...,\n\n        [[-0.0902, -1.2476, -0.2099,  ..., -0.0310,  0.5697, -1.5466]],\n\n        [[ 0.2113, -0.2835, -1.6604,  ...,  1.0774,  0.6964, -1.3799]],\n\n        [[ 0.4627,  0.0389, -2.8822,  ...,  1.1632,  0.5634, -0.3277]]]), tensor([[[-0.5112, -0.8904, -0.5923,  ...,  0.5472, -1.4077, -0.2739]],\n\n        [[ 1.1617, -1.5938, -0.6740,  ..., -0.3580, -0.3637, -0.9155]],\n\n        [[ 1.2989, -0.7229,  0.3824,  ..., -0.8154, -0.3060, -0.2828]],\n\n        ...,\n\n        [[ 0.3510, -0.9354, -0.6970,  ..., -0.5928,  0.3514, -0.4261]],\n\n        [[ 0.5424, -0.5169, -2.1034,  ...,  0.9028,  1.0406, -0.4077]],\n\n        [[ 0.9847, -0.2464, -2.8325,  ...,  0.8564,  0.5528, -0.0879]]]), tensor([[[-0.0406, -0.4373, -0.1477,  ...,  0.1568, -0.8376, -0.2860]],\n\n        [[ 1.0536, -0.8088, -0.6082,  ..., -0.6802,  0.1741, -0.7474]],\n\n        [[ 1.4418, -0.0043, -0.0663,  ..., -0.8759, -0.1933, -0.8577]],\n\n        ...,\n\n        [[ 0.0944, -0.2383, -0.3422,  ..., -0.1536, -0.3056, -0.3251]],\n\n        [[ 0.3406, -0.1829, -1.6604,  ...,  0.8123,  0.0887, -1.3026]],\n\n        [[ 0.7220, -0.0490, -2.3900,  ...,  0.6851, -0.2784, -0.7982]]]), tensor([[[ 0.3538, -0.7011,  0.1355,  ...,  0.0912, -1.4751, -0.3032]],\n\n        [[ 1.7260, -1.2371, -0.5203,  ..., -0.4229, -0.0199, -0.7303]],\n\n        [[ 2.1150, -0.4711,  0.5520,  ..., -0.4018, -0.2267, -0.7367]],\n\n        ...,\n\n        [[-0.2094, -0.5073, -0.7549,  ...,  0.2137, -0.6454, -0.5232]],\n\n        [[ 0.5325, -0.2638, -1.9003,  ...,  0.9388, -0.0830, -0.3557]],\n\n        [[ 0.4565,  0.0699, -2.6016,  ...,  0.8593, -0.4189,  0.0271]]]), tensor([[[-9.9006e-04, -8.3685e-01,  5.9874e-01,  ...,  4.0764e-01,\n          -1.0346e+00, -4.0091e-01]],\n\n        [[ 1.4977e+00, -1.1171e+00, -7.4040e-02,  ...,  9.2290e-02,\n           8.7714e-01, -6.2549e-01]],\n\n        [[ 2.0026e+00, -6.1965e-01,  4.9363e-01,  ..., -1.8394e-01,\n           4.3372e-01, -4.5617e-01]],\n\n        ...,\n\n        [[-2.1229e-01, -7.7680e-01,  1.9733e-01,  ...,  1.2256e-01,\n           1.0972e-01, -2.0376e-01]],\n\n        [[ 8.3739e-01, -1.9654e-01, -1.0783e+00,  ...,  2.3006e-01,\n           1.5886e-01, -1.9944e-01]],\n\n        [[ 5.9456e-01,  1.1208e-01, -1.3362e+00,  ...,  4.8854e-01,\n           5.4456e-02, -3.8967e-02]]]), tensor([[[ 0.1950, -0.6356,  0.8977,  ..., -0.0475, -1.4947, -0.1426]],\n\n        [[ 1.8091, -0.7023,  0.3854,  ..., -0.4700,  0.3304, -0.3470]],\n\n        [[ 2.0336, -0.2752,  0.8669,  ..., -0.8400, -0.7042,  0.0057]],\n\n        ...,\n\n        [[-0.3159, -0.6435, -0.2026,  ..., -0.2428,  0.2846, -0.0893]],\n\n        [[ 1.1016,  0.2237, -1.2877,  ..., -0.0482,  0.0613, -0.7050]],\n\n        [[ 0.3958,  0.2539, -0.8466,  ...,  0.0174, -0.1932,  0.0343]]]), tensor([[[-0.0506, -0.6617,  0.3882,  ..., -0.0042, -1.0797, -0.0867]],\n\n        [[ 1.0111, -0.7698,  0.0408,  ..., -0.1930,  0.2902, -0.1821]],\n\n        [[ 1.3112, -0.3588,  0.7120,  ..., -0.5989, -0.4447,  0.0761]],\n\n        ...,\n\n        [[-0.3072, -0.6513, -0.6231,  ..., -0.2192,  0.8463,  0.1804]],\n\n        [[ 0.6510,  0.1915, -0.9260,  ...,  0.1047,  0.8065, -0.3097]],\n\n        [[ 0.1388,  0.0498, -0.2055,  ..., -0.0218,  0.0385,  0.0312]]]), tensor([[[-0.8866, -1.1458,  0.4816,  ..., -0.2999, -1.2026,  0.7538]],\n\n        [[ 0.3383, -1.6679, -0.0108,  ..., -0.3250,  0.6752,  0.3044]],\n\n        [[ 0.9515, -1.0855,  0.7353,  ..., -0.1509, -0.1007,  0.4642]],\n\n        ...,\n\n        [[-0.5375, -0.9538, -0.5101,  ..., -0.0157,  1.0665,  0.3272]],\n\n        [[ 0.4423, -0.0047, -0.4852,  ...,  0.1220,  1.0180, -0.6279]],\n\n        [[ 0.0357, -0.0250, -0.0915,  ...,  0.0430,  0.0432,  0.0476]]]), tensor([[[-0.3457, -0.9145,  0.0100,  ..., -0.4585, -0.9520,  1.1424]],\n\n        [[ 0.1617, -1.6695,  0.1308,  ..., -0.0765,  0.3736,  0.3521]],\n\n        [[ 0.5313, -1.2578,  0.4652,  ...,  0.0268,  0.3139,  0.6950]],\n\n        ...,\n\n        [[-0.1741, -1.1738,  0.1853,  ...,  0.0467,  0.7462,  0.8298]],\n\n        [[ 0.0781, -0.2275, -0.0092,  ...,  0.1665,  0.9218, -0.2769]],\n\n        [[-0.0587,  0.0722, -0.1692,  ...,  0.0122,  0.0301,  0.3692]]]))\nLength Ooutputstput[1]:\n12\nSample from Output[1], first hidden layer:\ntensor([[[ 0.0032, -0.0315,  0.0196,  ...,  0.0253, -0.0354, -0.0340]],\n\n        [[ 0.1135, -0.0894,  0.0006,  ..., -0.0089, -0.0294, -0.0396]],\n\n        [[ 0.0666, -0.0365,  0.0668,  ..., -0.0341, -0.0013, -0.0623]],\n\n        ...,\n\n        [[ 0.0128, -0.0286,  0.0141,  ...,  0.0234,  0.0169, -0.0086]],\n\n        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n\n        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]])\nSample shape, first hidden layer\ngetting the last tensor for XLNet\ntensor([-5.6340e-01,  5.4620e-01,  3.1552e-01,  1.5816e+00, -1.3336e+00,\n        -1.0118e+00,  9.4456e-01,  1.8598e+00,  1.7944e+00,  7.2665e-01,\n         4.8432e-01,  9.2451e-01, -1.0815e+00, -1.5505e+00,  2.7071e+00,\n        -1.2056e-01, -4.4374e-01,  3.7573e-01,  9.2600e-01, -1.5099e+00,\n        -1.5385e+00, -7.8572e-01, -1.3033e+00, -3.5597e-01, -4.6746e-01,\n         8.3187e-01, -1.1513e+00,  1.8677e+00,  1.3748e+00, -4.4839e-02,\n         2.4656e-01,  6.2983e-01,  1.9355e+00,  1.0237e+00, -4.1579e-01,\n        -1.7730e+00,  6.6198e-01,  1.0740e+00, -2.1239e-02,  7.3706e-02,\n         7.1257e-01, -1.7680e+00,  3.6624e-01,  8.6338e-01,  1.5589e-01,\n        -1.5063e+00, -2.5793e+00,  5.9018e-01,  1.9571e+00,  7.9683e-01,\n         3.4581e-01, -6.5445e-01, -7.1564e-01,  1.0645e+00, -1.7646e+00,\n         1.2823e-01,  1.1020e+00, -2.6268e-02,  4.4652e-01, -2.9979e-02,\n         1.1276e-01,  8.1930e-01,  7.2893e-01, -2.8677e-02,  2.4482e+00,\n        -8.6500e-01, -1.5604e+00,  4.1602e-02, -2.8725e-01,  7.3471e-01,\n        -1.7984e+00,  3.1678e-01, -9.6145e-01,  2.4665e+00, -4.6439e-01,\n        -1.4203e+00,  5.8086e-01,  5.7099e-01,  6.9574e-02,  1.7613e+00,\n         1.8406e-01,  1.6806e-01,  6.0762e-01, -1.5079e+00, -9.5369e-01,\n         1.2333e+00,  2.2940e+00,  3.6222e-02,  8.7696e-01, -1.4291e-01,\n        -2.5539e+00, -1.8685e+00,  2.2008e+00, -1.6360e+00,  2.0226e-01,\n        -8.0503e-01, -8.8203e-02,  9.2811e-01, -1.8785e+00, -1.6070e+00,\n        -2.5220e+00,  1.0201e+00,  1.5172e+00, -1.1400e+00,  1.5815e+00,\n         1.4226e-01,  1.2169e+00,  6.5631e-01, -5.9791e-01, -3.7215e-01,\n        -7.0063e-01,  5.8168e-01, -6.4061e-01, -5.1107e-01,  6.7252e-01,\n        -1.9088e+00, -2.3523e-01,  1.0407e+00,  3.5139e-01,  1.3031e+00,\n        -1.3376e+00,  3.5461e-02, -1.9937e+00,  1.1087e+00,  1.5493e+00,\n        -1.0698e+00, -1.7395e-01,  2.8134e+00, -2.4346e-01,  1.2568e+00,\n        -2.6055e-02,  6.6119e-01,  2.5923e-02, -2.8799e-01, -2.2844e+00,\n         1.2399e-01,  4.0604e+00, -6.2590e-01,  7.5848e-02, -9.4249e-01,\n         1.1064e+00, -9.1159e-01,  6.3403e-01,  1.5054e+00, -4.0865e-01,\n         5.5891e-01,  4.6720e-01, -1.2643e+00,  1.2767e+00, -7.2743e-01,\n         5.9424e-01,  2.7040e+00,  2.9955e-01, -1.5779e+00, -1.2790e-02,\n         1.3092e+00, -2.2365e+00, -9.4486e-01, -1.3099e+00, -8.8562e-01,\n        -6.5582e-01, -9.6588e-01, -8.3893e-02,  1.1587e+00, -1.0641e+00,\n         3.0509e-01, -9.0719e-01,  1.9833e+00,  1.5206e+00, -2.0217e-03,\n         6.1827e-01, -1.1462e+00, -2.8821e-01,  7.0114e-01, -6.6642e-01,\n        -2.3491e-01, -5.4321e-01,  5.1065e-01, -2.3769e-01,  1.8092e+00,\n        -2.0052e+00,  5.7280e-01, -6.0375e-01,  1.4579e+00,  1.7168e+00,\n        -4.9530e-01, -1.3065e+00,  6.3628e-02, -7.7475e-01, -8.7027e-02,\n        -3.5311e-01, -1.1715e+00,  9.0009e-01, -1.5558e+00, -2.4878e+00,\n        -1.7435e+00,  2.8073e-01, -6.3966e-01, -1.0609e+00,  6.5064e-01,\n         2.3217e-02, -5.6810e-02, -7.7952e-01, -1.7426e+00, -1.4025e+00,\n         2.0059e-01,  1.0144e+00,  6.6004e-02,  5.1000e-01, -1.6367e+00,\n         1.1259e+00, -8.1953e-01, -1.0998e+00, -1.4341e+00,  2.8128e-01,\n         6.3379e-01, -4.6003e-01,  2.1661e+00,  7.0999e-01, -8.0324e-01,\n         1.9483e+00, -1.3310e+00, -8.0426e-02, -2.5242e-01, -5.1304e-01,\n         1.3140e+00,  1.3888e+00,  1.0863e+00, -2.3767e+00, -1.8222e+00,\n         1.3579e+00,  5.9701e-02, -3.4830e-01, -1.0622e+00,  2.4783e-01,\n        -1.2656e-01, -5.3645e-02,  5.4698e-01, -1.4425e+00, -1.5387e-01,\n         1.3452e+00, -8.5250e-01, -2.3770e+00,  7.6673e-01, -1.7752e-01,\n         1.3714e-01, -9.2982e-01,  5.9749e-02, -1.1073e+00,  1.1123e+00,\n         1.0748e+00, -1.0272e+00, -8.3638e-01,  9.2109e-01, -1.0584e+00,\n        -1.0211e-02,  1.9993e+00,  4.6974e+00, -4.0831e-01, -9.3707e-01,\n        -7.1401e-01,  1.0740e+00,  3.0731e-01, -5.8282e-01, -1.2062e+00,\n         2.8030e+00, -3.5596e-01,  5.3162e-01,  2.7540e-01, -4.5738e-01,\n        -1.4518e+00, -2.7486e-01, -1.7112e+00,  1.0110e+00, -9.1193e-01,\n         1.5134e+00, -3.2265e-01,  3.5556e-01,  6.0632e-01, -3.7909e-01,\n        -2.5054e+00, -1.4056e+00,  5.8610e-01,  7.0314e-01, -2.2582e+00,\n        -8.0135e-01,  1.1842e-01,  1.0855e+00,  4.2684e-01,  4.0181e-01,\n        -4.8121e-01, -9.8547e-01, -7.2346e-01,  1.1838e-01,  1.8025e+00,\n         3.6027e-01,  3.0667e-01, -3.8774e-01, -7.2225e-01,  1.4519e+00,\n         8.1988e-01, -3.9185e-02, -5.2845e-01,  1.0973e+00, -1.3635e-01,\n         5.5853e-01, -1.2713e+00,  6.5841e-01,  1.4111e-01, -2.5811e+00,\n        -6.9869e-01, -5.3573e-02,  7.8396e-01,  2.3446e+00,  5.4330e-01,\n        -2.7144e-01,  2.9525e+00, -6.9048e-01,  1.2001e+00, -3.8937e-01,\n        -4.9897e-01,  1.3675e+00, -2.0083e+00, -1.5675e+00,  2.2157e-01,\n         2.3794e-01, -1.0388e+00,  2.7467e+00, -3.8873e+00, -2.6502e-01,\n        -9.6606e+00, -5.9348e-01, -1.4479e-01, -5.7996e-01,  2.8378e-01,\n        -6.4880e-01, -1.0081e-01,  2.0126e-01, -1.1731e+00,  1.0256e-01,\n         9.2357e-01, -3.4776e-01, -1.0120e+00, -4.6522e-01,  6.8410e-01,\n        -1.0182e+00,  5.6600e-01, -1.8339e-01, -3.9619e-01, -9.1456e-01,\n        -2.5741e-01, -1.8225e+00,  1.8661e+00,  3.8279e-01,  8.2525e-01,\n         7.8977e-01,  1.5266e-01,  5.6499e-01, -5.1718e-01,  2.8478e-01,\n        -9.9970e-01, -7.9534e-02,  3.2870e-01, -1.6902e+00, -1.0069e+00,\n        -1.0143e-02, -6.7640e-01, -7.7649e-01,  3.1708e-01,  7.9546e-01,\n        -1.1605e+00,  6.5981e-02,  3.9948e-01,  3.8233e-01, -4.7288e-01,\n         1.6168e-01,  6.1253e-02, -4.7404e-01, -4.4512e+00, -3.4346e-01,\n         1.3444e+00, -1.1948e+00,  1.1488e+00, -3.4423e-01,  1.6644e+00,\n         1.5687e-01, -4.1181e-01, -1.1530e+00, -2.6183e-01,  1.1150e-01,\n         4.3052e+00,  6.0349e-01,  2.3714e+00,  4.6238e-01,  9.0618e-01,\n         6.3349e-02, -4.6174e-01, -1.0912e+00, -7.6824e-01,  7.2882e-01,\n        -1.0336e+00,  2.9990e-01, -3.2962e-01,  6.9319e-01, -1.3842e+00,\n         1.3613e+00, -3.2927e-01, -1.0012e+00, -6.3671e-01, -1.2331e+00,\n        -8.9840e-01, -1.3108e-01, -4.5857e-01, -5.1646e-01, -3.0852e-01,\n        -9.1532e-01, -9.0787e-01,  7.0453e-01,  8.9357e-01,  1.2630e+00,\n        -6.6397e-01,  9.3415e-01, -2.1738e+00, -3.7549e-01,  1.8309e-01,\n        -1.9516e-01,  8.5774e-02,  5.3857e-01,  2.3138e-01,  6.5520e-01,\n         8.4304e-01,  3.9104e-01, -1.3173e+00, -3.9957e-02, -7.6671e-01,\n        -1.6139e-04,  8.2231e-01,  1.3444e-01,  1.5256e+00, -5.5060e-01,\n         1.5501e+00, -2.1642e+00, -1.6581e-01,  1.6237e-01, -7.4932e-01,\n        -7.2567e-01, -4.2176e-01,  9.1188e-01, -1.2781e+00,  1.2761e+00,\n         2.7769e+00, -5.4379e-01, -6.9946e-01, -9.4577e-01,  1.1028e+00,\n        -1.4783e+00,  1.2396e+00,  9.3830e-01, -2.7806e-01,  3.1066e-01,\n         5.5460e-01, -1.1151e+00, -1.5677e+00, -9.8466e-02, -2.0227e+00,\n         1.2355e+00, -3.8111e-01, -1.2068e+00,  4.3520e-01,  4.8661e-01,\n         6.6508e-01,  4.4343e-01, -2.6418e+00,  1.6217e+00, -4.1217e-01,\n         2.3851e-01, -5.9549e-01, -5.3489e-03,  8.9535e-01,  8.5437e-01,\n        -2.5202e-01,  2.5638e-02, -7.7384e-01, -3.9742e-02,  4.3216e-01,\n         5.0251e-01, -3.8212e-01, -1.6284e+00,  1.1610e+00,  1.2968e-02,\n         1.9151e+00,  9.1927e-01, -8.0474e-01, -2.0200e+00,  2.5872e-01,\n         1.3955e+00,  4.8002e-01,  4.2935e-01, -1.7940e+00,  9.1285e-01,\n         1.7561e-01,  2.6307e+00, -1.2033e+00,  1.0427e+00,  1.0299e-01,\n         1.9571e-01,  1.1479e+00, -1.7331e+00,  2.6530e-01,  2.6224e+00,\n         5.0947e-01,  3.0623e-01, -1.2380e+00, -1.2663e+00,  6.1917e-01,\n        -2.2350e+00,  8.8293e-01, -2.5414e+00, -3.6009e-01, -1.6403e+00,\n        -1.1937e+00, -1.4588e-01,  1.7032e+00,  5.7102e-01, -1.7384e+00,\n         6.2094e-01,  1.2254e+00, -6.2064e-01, -3.0466e-03, -1.0389e+00,\n         8.6172e-02,  4.9526e-01, -2.5902e+00, -4.5028e-01, -1.0996e+00,\n         8.1790e-01, -3.1968e-01,  3.3701e-01,  1.1005e-01, -3.3429e-01,\n        -5.8658e-01, -1.2449e+00,  4.9582e-02, -1.4436e+00, -2.7303e-01,\n        -8.0661e-01,  6.4248e-02, -1.5603e+00, -9.3392e-01,  1.7811e+00,\n         8.2889e-01,  8.7438e-01,  2.5193e-01, -2.7243e+00,  1.5126e-02,\n        -5.7911e-01,  1.4092e+00,  6.8062e-01,  2.2452e+00, -1.4814e+00,\n         9.3619e-01, -1.9130e+00,  7.6484e-02,  9.9398e-01,  6.1328e-01,\n         9.2806e-02, -1.5906e+00,  1.8497e-01,  8.5415e-03,  1.9208e+00,\n        -1.1258e+00, -7.8349e-01, -1.5292e-01, -3.2106e-01, -2.6442e-01,\n         1.0701e-01,  1.0627e+00, -5.9360e-01, -3.0019e-01,  4.2890e-01,\n         4.0532e-01, -3.6040e-03,  4.0988e-01, -1.2814e+00,  6.9750e-01,\n        -3.4515e-01,  1.4111e-01, -9.9115e-01, -6.2640e-01,  7.9327e-01,\n        -5.4913e-01,  1.1300e+00, -8.0169e-01, -5.4908e-02,  4.6548e-01,\n        -1.6730e+00,  2.3791e+00,  7.2490e-01,  4.0278e-01,  1.4245e+00,\n        -1.3135e+00,  8.5571e-01,  1.4155e-01, -1.3318e+00, -2.0723e+00,\n         1.1738e+00, -1.6748e-01,  9.4070e-02, -1.0273e+00,  7.9299e-02,\n        -8.7015e-01, -1.7434e+00, -3.9688e-01,  7.7927e-01,  1.1821e+00,\n        -1.5641e+00,  3.8913e-01,  5.0123e-02, -2.3070e-03, -1.5055e+00,\n         2.2283e-01, -1.0856e-01, -4.5791e-01,  1.2155e+00, -2.0217e-01,\n        -2.3409e-01, -1.6480e+00,  4.6490e-01, -1.8455e+00,  8.5810e-02,\n         4.4538e+00, -5.5297e-01,  8.4157e-01,  1.5460e+00,  6.2841e-02,\n         1.0328e+00, -2.7881e-01, -1.1837e+00,  1.0528e+00, -4.8044e-01,\n        -1.5454e+00,  8.0284e-01,  1.0022e+00,  9.3946e-01, -1.6799e+00,\n        -8.1240e-01,  8.5411e-01,  1.0926e+00,  8.6917e-01,  3.1300e+00,\n         7.6752e-02, -7.2190e-01, -2.7103e+00,  7.6199e-01, -9.9936e-01,\n        -1.6443e+00, -2.4657e-01,  6.6328e-02, -1.5902e+00,  7.0963e-01,\n        -7.5480e-01, -9.6329e-01, -1.8866e-01, -6.0614e-01, -9.3243e-01,\n         8.1193e-01,  5.8175e-01,  1.3248e+01,  9.2447e-02, -6.6336e-01,\n        -8.5773e-01,  3.4304e-01,  2.5214e-01,  6.4804e-01, -1.5559e+00,\n         1.1396e+00,  9.4969e-01,  4.5399e-01, -8.6353e-01, -1.0295e+00,\n         8.7815e-01,  1.0167e+00,  8.4648e-01,  9.2569e-02,  6.9583e-01,\n        -1.3432e+00,  5.3496e-01,  2.2850e-01, -3.0985e-01, -6.4024e-01,\n        -1.5585e+00,  4.0839e-01, -1.0386e-01,  2.1281e-01, -5.4636e-01,\n         5.6655e-01, -2.0596e+00,  4.8895e-01,  4.9249e-02,  1.7283e+00,\n         8.1911e-01,  1.6991e+00, -1.3161e-01, -1.0863e-02,  4.2764e-01,\n        -8.4353e-01,  2.3920e+00, -5.2509e+00,  1.0946e+00,  1.1218e+00,\n        -1.6437e+00, -9.3069e-02, -3.3674e+00, -1.0193e+00, -9.9315e-01,\n        -9.4794e-01,  6.8121e-01, -6.8653e-01, -1.5411e+00,  3.1381e-01,\n        -3.4103e-01,  1.1957e+00, -8.7758e-02, -9.2854e-01, -1.9329e-01,\n        -3.0204e-01, -1.5977e-01,  1.9324e+00, -1.3666e+00, -5.7511e-01,\n         1.3238e+00, -6.2199e-01,  5.1018e-01,  1.1443e+00, -1.4356e+00,\n         4.2924e-01,  3.3800e-01, -1.2854e-01,  5.2917e-01,  7.9780e-01,\n         3.0011e-01,  1.2732e+00, -8.4979e-01, -1.7483e-01,  3.8666e-01,\n         1.0725e-01,  1.9757e+00, -2.0912e+00, -1.7779e+00,  1.1942e+00,\n         1.6455e+00, -2.9675e-01, -1.8713e+00, -8.0101e-01,  1.5644e-01,\n         9.0353e-01, -1.0051e+00,  1.3614e-01,  7.1692e-01, -7.5955e-01,\n         7.4015e-01, -6.3756e-01, -1.3430e-01,  6.5226e-01,  1.0395e+00,\n        -4.2775e-01,  2.4666e+00, -6.1915e-01])\nText:\nmatuschek is, a gift....\ntext_encoding_tensor:\ntensor([[10714,   415,  2970,   267,    27,    19,    24,  2718,  1515,    25,\n            17, 13352,  3307,  1277,    19,    27,    18,  9363,    20,  1296,\n         20278,    17, 12759, 12138,    17,    10,  1653,  4316, 23869,  3432,\n            11,    21,    18,  3540,    17,  2582,    17,    68,    17, 17829,\n          3068,    17, 10503,   267,    17,    10,  1880,  2753,   993,    17,\n            23, 21841,  3207,    11,     9,    38,   154,    63,  5657,    17,\n          1121,  8707,  1167,   231,    86,    19,    57,    52,  1362,    24,\n           299, 15850,   773,    27,    17, 24535,    37,    18,   648,    29,\n           231,    51,    24,  2446,  6995,    17,  4985,    33,    59,    63,\n           750,   206,  3776,    13, 12843,    56,  3450,     9,  6926, 14321,\n            27,   797,   126,    21,  5956,    19,    21,   171,  1296, 20278,\n            21,    17, 17829,  3068,   750,    17, 19192,    23,    38,   154,\n            19,    63,  2986,    20, 20078,   492,    58,  4765,    19, 11796,\n            21,  4027,  6995,    17,  4985,     9, 20977,  3742,    27,  1365,\n          9433,    19,    21,    18,  1515,    27,  3919,     9,  1296, 20278,\n            54,    72,    33,    18,  1515,    28,   106,    92,    19,    21,\n            51,   426,    72,  3170,   143,    37,    17,    98,   213,     9,\n         10714,   415,  2970,   267,    17,    10,  5706,  8563,    17,    98,\n         13429,    11,    19,    57, 11388,    45,  5413,    51,  1318,     9,\n          1296, 20278,    27,    38,    24,  1059,    19,    21, 10714,   415,\n          2970,   267,  1685,    23,   124,  6267,    19,  1381,  2929,  1296,\n         20278,    29,    36,    74,    39,   252,   108,    43,   263,     9,\n         12023,    19,  1296, 20278, 16228,    45,   129, 22382,    21,   349,\n         16893,    22,  1251,    19,   208,    17, 17829,  3068,     9,    28,\n           497,    63,    41,  1723,    22,   231,    86,     9,   101,   206,\n            17, 15442,   492,    20,    18,  2446,  6995,    17,  4985,    23,\n            30,  1741,    28,    17,    46,     4,     3]])\nshape:\ntorch.Size([1, 267])\nattention_mask_tensor:\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1]])\nshape:\ntorch.Size([1, 267])\noutputs:\n(tensor([[[-1.5729,  2.4788, -0.5251,  ..., -0.3445, -1.7404,  1.4638],\n         [-1.4355,  3.2041,  0.6202,  ...,  0.1141, -1.1354,  1.8956],\n         [ 0.6052,  0.7297,  1.4513,  ...,  1.6763, -0.0070,  1.2746],\n         ...,\n         [-2.2236,  0.3547,  0.5447,  ..., -1.0407,  0.1137, -0.0772],\n         [-1.6431,  0.0714, -0.7630,  ..., -0.0843,  0.4525, -0.2077],\n         [-2.0352, -0.7459, -1.2483,  ..., -0.0471, -0.4214,  0.0954]]]), (tensor([[[-0.0392,  0.0757, -0.0086,  ..., -0.0604,  0.0133,  0.0358]],\n\n        [[ 0.0505,  0.1237,  0.0158,  ..., -0.0261, -0.0447,  0.1084]],\n\n        [[ 0.0773, -0.0132,  0.0771,  ...,  0.0277,  0.0370,  0.0216]],\n\n        ...,\n\n        [[-0.0162,  0.0391,  0.0709,  ...,  0.0444,  0.0109,  0.0179]],\n\n        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n\n        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]]), tensor([[[-1.2115,  1.9785,  0.2903,  ..., -0.8622, -0.4564, -0.0108]],\n\n        [[ 0.7098,  2.6262,  0.8038,  ..., -0.4182, -0.6905,  2.4941]],\n\n        [[ 1.0026, -0.1832,  1.3324,  ...,  0.4071,  0.5101,  0.7972]],\n\n        ...,\n\n        [[-0.8209,  0.4740,  1.1612,  ..., -0.0473,  0.7112,  0.9053]],\n\n        [[ 1.3529, -0.5091, -1.5718,  ...,  0.4551,  0.4148, -0.0103]],\n\n        [[ 0.6098, -0.2904, -2.3236,  ...,  0.4090, -0.1293,  0.5087]]]), tensor([[[-0.8195,  1.2538, -0.2572,  ..., -0.3343, -0.5468,  0.1199]],\n\n        [[ 0.2411,  1.9756,  0.8664,  ...,  0.3672, -0.1302,  1.7112]],\n\n        [[ 0.6303, -0.0187,  1.4790,  ...,  0.4507, -0.3045,  1.0809]],\n\n        ...,\n\n        [[-0.9600,  0.6611,  1.2513,  ..., -0.6252,  0.4691,  1.4018]],\n\n        [[ 0.6502, -0.3498, -1.8307,  ...,  0.1161,  0.8258, -0.1524]],\n\n        [[ 0.2857, -0.3537, -2.5651,  ...,  0.4118,  0.3615,  0.0625]]]), tensor([[[-0.9774,  2.0258, -0.5192,  ..., -0.4847, -1.1621, -0.9274]],\n\n        [[-0.6299,  3.0771, -0.0849,  ..., -0.0300, -0.5123,  1.0214]],\n\n        [[ 0.1901,  0.8570,  0.7137,  ..., -0.2537,  0.2619, -0.0977]],\n\n        ...,\n\n        [[-0.9468,  0.7066,  0.5687,  ..., -0.6514,  0.7711,  1.5194]],\n\n        [[ 0.2529,  0.0188, -2.0907,  ...,  1.3447,  0.7826, -1.3339]],\n\n        [[ 0.3697,  0.0879, -2.9578,  ...,  1.3799,  0.4307, -0.3897]]]), tensor([[[-0.3284,  1.0416, -0.4207,  ...,  0.1115, -1.3737,  0.6013]],\n\n        [[-0.4646,  2.3591, -0.1768,  ...,  0.8933, -1.2187,  0.8658]],\n\n        [[ 0.7439,  1.3181,  0.5248,  ...,  0.3616,  0.0745, -0.3426]],\n\n        ...,\n\n        [[-0.7401,  0.1942,  0.1285,  ..., -0.2034,  0.5322,  0.9569]],\n\n        [[ 0.6085, -0.2837, -2.1712,  ...,  1.5194,  0.4977, -0.0642]],\n\n        [[ 0.9046, -0.1253, -2.8362,  ...,  1.2891,  0.1769,  0.2049]]]), tensor([[[ 0.0106,  0.6846, -0.4819,  ...,  0.5094, -1.5359, -0.1225]],\n\n        [[-0.1395,  1.5570, -0.3152,  ...,  0.9482, -1.3454, -0.0386]],\n\n        [[ 1.0122,  1.0237,  0.4284,  ...,  0.4099, -0.7949, -0.6297]],\n\n        ...,\n\n        [[-0.6318,  0.4214,  0.3728,  ..., -0.8737, -0.5998,  0.2574]],\n\n        [[ 0.3580, -0.0380, -1.8166,  ...,  0.8651,  0.0330, -0.9261]],\n\n        [[ 0.7280, -0.0680, -2.3566,  ...,  0.5777, -0.0791, -0.6604]]]), tensor([[[ 2.3415e-01,  7.1741e-01, -5.1451e-01,  ...,  1.1997e+00,\n          -1.8645e+00, -9.6433e-01]],\n\n        [[ 4.8468e-01,  1.9302e+00, -8.4280e-01,  ...,  1.8714e+00,\n          -1.9203e+00, -4.9304e-01]],\n\n        [[ 1.5102e+00,  1.2773e+00,  1.8202e-01,  ...,  1.4033e+00,\n          -1.2912e+00, -1.6520e+00]],\n\n        ...,\n\n        [[-5.9990e-01,  8.0142e-01, -2.2753e-01,  ..., -4.0858e-01,\n          -9.9723e-01,  2.7263e-01]],\n\n        [[ 1.0486e+00, -1.0973e-01, -1.9405e+00,  ...,  1.0604e+00,\n          -4.6663e-01, -7.7064e-01]],\n\n        [[ 9.9877e-01, -3.0032e-04, -2.2294e+00,  ...,  9.9195e-01,\n          -5.2750e-01, -6.6116e-01]]]), tensor([[[-0.3658,  0.9465, -0.2965,  ...,  1.0092, -1.1862,  0.3574]],\n\n        [[-0.3149,  1.9972, -0.3163,  ...,  1.4112, -1.3222,  0.6516]],\n\n        [[ 0.8657,  1.1179,  0.5721,  ...,  0.9267, -0.5003, -0.1318]],\n\n        ...,\n\n        [[-0.9493,  0.7477,  0.2303,  ..., -0.0904, -0.1976,  0.3422]],\n\n        [[ 0.8116, -0.2182, -0.7876,  ...,  0.1908, -0.0359, -0.3593]],\n\n        [[ 0.7463,  0.0430, -1.0067,  ...,  0.1130, -0.0542, -0.3069]]]), tensor([[[-0.5842,  1.0725, -0.0313,  ...,  1.1746, -1.2310,  0.7964]],\n\n        [[-0.2417,  1.8543,  0.3225,  ...,  1.3880, -0.7392,  1.2264]],\n\n        [[ 0.9838,  0.8394,  1.0725,  ...,  0.9581,  0.0077,  0.5757]],\n\n        ...,\n\n        [[-1.2195,  0.6128, -0.1946,  ...,  0.2822,  0.0840,  0.3547]],\n\n        [[ 0.7887, -0.2896, -1.1089,  ...,  0.6447,  0.2938, -0.1511]],\n\n        [[ 0.4435,  0.0434, -0.7058,  ...,  0.2215,  0.0580,  0.0955]]]), tensor([[[-0.8100,  0.7600, -0.0728,  ...,  0.8736, -1.1206,  0.4758]],\n\n        [[-0.3688,  1.6599,  0.4291,  ...,  0.7459, -0.9190,  0.5342]],\n\n        [[ 0.5608,  0.9736,  0.8583,  ...,  0.4843, -0.5363,  0.3204]],\n\n        ...,\n\n        [[-1.3285,  0.7010,  0.0161,  ...,  0.3040,  0.3616,  0.0424]],\n\n        [[ 0.5251, -0.0748, -0.9575,  ...,  0.4385,  0.4900,  0.0722]],\n\n        [[ 0.1471,  0.0492, -0.2035,  ...,  0.0296,  0.0777,  0.0352]]]), tensor([[[-1.0817,  0.7821, -0.2266,  ...,  1.1681, -1.0209,  1.2305]],\n\n        [[-0.7482,  1.6433,  0.3871,  ...,  0.9771, -0.8650,  1.3901]],\n\n        [[ 0.1430,  0.6119,  0.9320,  ...,  1.0486, -0.5264,  0.9128]],\n\n        ...,\n\n        [[-1.6706,  0.9653,  0.3261,  ...,  0.4949,  0.1418,  0.2963]],\n\n        [[ 0.2421,  0.1603, -0.7450,  ...,  0.4342,  0.4886,  0.1687]],\n\n        [[ 0.0398, -0.0331, -0.1104,  ...,  0.0823,  0.0517,  0.0683]]]), tensor([[[-0.4428,  0.7513, -0.5056,  ...,  0.6072, -1.0234,  1.7888]],\n\n        [[-0.3533,  0.9564,  0.1720,  ...,  1.0431, -0.9805,  1.9370]],\n\n        [[ 0.5125, -0.0373,  0.5286,  ...,  1.0479, -0.8934,  1.1945]],\n\n        ...,\n\n        [[-1.1069,  0.6487,  0.3168,  ...,  0.2475, -0.1425,  0.0610]],\n\n        [[-0.0994, -0.0339, -0.4353,  ...,  0.3277,  0.2257,  0.5112]],\n\n        [[-0.0995, -0.0065, -0.1793,  ..., -0.0098,  0.0282,  0.3359]]])))\nLenght of outputs 2\noutputs[0]:\ntensor([[[-1.5729,  2.4788, -0.5251,  ..., -0.3445, -1.7404,  1.4638],\n         [-1.4355,  3.2041,  0.6202,  ...,  0.1141, -1.1354,  1.8956],\n         [ 0.6052,  0.7297,  1.4513,  ...,  1.6763, -0.0070,  1.2746],\n         ...,\n         [-2.2236,  0.3547,  0.5447,  ..., -1.0407,  0.1137, -0.0772],\n         [-1.6431,  0.0714, -0.7630,  ..., -0.0843,  0.4525, -0.2077],\n         [-2.0352, -0.7459, -1.2483,  ..., -0.0471, -0.4214,  0.0954]]])\noutputs[0].shape:\ntorch.Size([1, 267, 768])\noutputs[1]:\n(tensor([[[-0.0392,  0.0757, -0.0086,  ..., -0.0604,  0.0133,  0.0358]],\n\n        [[ 0.0505,  0.1237,  0.0158,  ..., -0.0261, -0.0447,  0.1084]],\n\n        [[ 0.0773, -0.0132,  0.0771,  ...,  0.0277,  0.0370,  0.0216]],\n\n        ...,\n\n        [[-0.0162,  0.0391,  0.0709,  ...,  0.0444,  0.0109,  0.0179]],\n\n        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n\n        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]]), tensor([[[-1.2115,  1.9785,  0.2903,  ..., -0.8622, -0.4564, -0.0108]],\n\n        [[ 0.7098,  2.6262,  0.8038,  ..., -0.4182, -0.6905,  2.4941]],\n\n        [[ 1.0026, -0.1832,  1.3324,  ...,  0.4071,  0.5101,  0.7972]],\n\n        ...,\n\n        [[-0.8209,  0.4740,  1.1612,  ..., -0.0473,  0.7112,  0.9053]],\n\n        [[ 1.3529, -0.5091, -1.5718,  ...,  0.4551,  0.4148, -0.0103]],\n\n        [[ 0.6098, -0.2904, -2.3236,  ...,  0.4090, -0.1293,  0.5087]]]), tensor([[[-0.8195,  1.2538, -0.2572,  ..., -0.3343, -0.5468,  0.1199]],\n\n        [[ 0.2411,  1.9756,  0.8664,  ...,  0.3672, -0.1302,  1.7112]],\n\n        [[ 0.6303, -0.0187,  1.4790,  ...,  0.4507, -0.3045,  1.0809]],\n\n        ...,\n\n        [[-0.9600,  0.6611,  1.2513,  ..., -0.6252,  0.4691,  1.4018]],\n\n        [[ 0.6502, -0.3498, -1.8307,  ...,  0.1161,  0.8258, -0.1524]],\n\n        [[ 0.2857, -0.3537, -2.5651,  ...,  0.4118,  0.3615,  0.0625]]]), tensor([[[-0.9774,  2.0258, -0.5192,  ..., -0.4847, -1.1621, -0.9274]],\n\n        [[-0.6299,  3.0771, -0.0849,  ..., -0.0300, -0.5123,  1.0214]],\n\n        [[ 0.1901,  0.8570,  0.7137,  ..., -0.2537,  0.2619, -0.0977]],\n\n        ...,\n\n        [[-0.9468,  0.7066,  0.5687,  ..., -0.6514,  0.7711,  1.5194]],\n\n        [[ 0.2529,  0.0188, -2.0907,  ...,  1.3447,  0.7826, -1.3339]],\n\n        [[ 0.3697,  0.0879, -2.9578,  ...,  1.3799,  0.4307, -0.3897]]]), tensor([[[-0.3284,  1.0416, -0.4207,  ...,  0.1115, -1.3737,  0.6013]],\n\n        [[-0.4646,  2.3591, -0.1768,  ...,  0.8933, -1.2187,  0.8658]],\n\n        [[ 0.7439,  1.3181,  0.5248,  ...,  0.3616,  0.0745, -0.3426]],\n\n        ...,\n\n        [[-0.7401,  0.1942,  0.1285,  ..., -0.2034,  0.5322,  0.9569]],\n\n        [[ 0.6085, -0.2837, -2.1712,  ...,  1.5194,  0.4977, -0.0642]],\n\n        [[ 0.9046, -0.1253, -2.8362,  ...,  1.2891,  0.1769,  0.2049]]]), tensor([[[ 0.0106,  0.6846, -0.4819,  ...,  0.5094, -1.5359, -0.1225]],\n\n        [[-0.1395,  1.5570, -0.3152,  ...,  0.9482, -1.3454, -0.0386]],\n\n        [[ 1.0122,  1.0237,  0.4284,  ...,  0.4099, -0.7949, -0.6297]],\n\n        ...,\n\n        [[-0.6318,  0.4214,  0.3728,  ..., -0.8737, -0.5998,  0.2574]],\n\n        [[ 0.3580, -0.0380, -1.8166,  ...,  0.8651,  0.0330, -0.9261]],\n\n        [[ 0.7280, -0.0680, -2.3566,  ...,  0.5777, -0.0791, -0.6604]]]), tensor([[[ 2.3415e-01,  7.1741e-01, -5.1451e-01,  ...,  1.1997e+00,\n          -1.8645e+00, -9.6433e-01]],\n\n        [[ 4.8468e-01,  1.9302e+00, -8.4280e-01,  ...,  1.8714e+00,\n          -1.9203e+00, -4.9304e-01]],\n\n        [[ 1.5102e+00,  1.2773e+00,  1.8202e-01,  ...,  1.4033e+00,\n          -1.2912e+00, -1.6520e+00]],\n\n        ...,\n\n        [[-5.9990e-01,  8.0142e-01, -2.2753e-01,  ..., -4.0858e-01,\n          -9.9723e-01,  2.7263e-01]],\n\n        [[ 1.0486e+00, -1.0973e-01, -1.9405e+00,  ...,  1.0604e+00,\n          -4.6663e-01, -7.7064e-01]],\n\n        [[ 9.9877e-01, -3.0032e-04, -2.2294e+00,  ...,  9.9195e-01,\n          -5.2750e-01, -6.6116e-01]]]), tensor([[[-0.3658,  0.9465, -0.2965,  ...,  1.0092, -1.1862,  0.3574]],\n\n        [[-0.3149,  1.9972, -0.3163,  ...,  1.4112, -1.3222,  0.6516]],\n\n        [[ 0.8657,  1.1179,  0.5721,  ...,  0.9267, -0.5003, -0.1318]],\n\n        ...,\n\n        [[-0.9493,  0.7477,  0.2303,  ..., -0.0904, -0.1976,  0.3422]],\n\n        [[ 0.8116, -0.2182, -0.7876,  ...,  0.1908, -0.0359, -0.3593]],\n\n        [[ 0.7463,  0.0430, -1.0067,  ...,  0.1130, -0.0542, -0.3069]]]), tensor([[[-0.5842,  1.0725, -0.0313,  ...,  1.1746, -1.2310,  0.7964]],\n\n        [[-0.2417,  1.8543,  0.3225,  ...,  1.3880, -0.7392,  1.2264]],\n\n        [[ 0.9838,  0.8394,  1.0725,  ...,  0.9581,  0.0077,  0.5757]],\n\n        ...,\n\n        [[-1.2195,  0.6128, -0.1946,  ...,  0.2822,  0.0840,  0.3547]],\n\n        [[ 0.7887, -0.2896, -1.1089,  ...,  0.6447,  0.2938, -0.1511]],\n\n        [[ 0.4435,  0.0434, -0.7058,  ...,  0.2215,  0.0580,  0.0955]]]), tensor([[[-0.8100,  0.7600, -0.0728,  ...,  0.8736, -1.1206,  0.4758]],\n\n        [[-0.3688,  1.6599,  0.4291,  ...,  0.7459, -0.9190,  0.5342]],\n\n        [[ 0.5608,  0.9736,  0.8583,  ...,  0.4843, -0.5363,  0.3204]],\n\n        ...,\n\n        [[-1.3285,  0.7010,  0.0161,  ...,  0.3040,  0.3616,  0.0424]],\n\n        [[ 0.5251, -0.0748, -0.9575,  ...,  0.4385,  0.4900,  0.0722]],\n\n        [[ 0.1471,  0.0492, -0.2035,  ...,  0.0296,  0.0777,  0.0352]]]), tensor([[[-1.0817,  0.7821, -0.2266,  ...,  1.1681, -1.0209,  1.2305]],\n\n        [[-0.7482,  1.6433,  0.3871,  ...,  0.9771, -0.8650,  1.3901]],\n\n        [[ 0.1430,  0.6119,  0.9320,  ...,  1.0486, -0.5264,  0.9128]],\n\n        ...,\n\n        [[-1.6706,  0.9653,  0.3261,  ...,  0.4949,  0.1418,  0.2963]],\n\n        [[ 0.2421,  0.1603, -0.7450,  ...,  0.4342,  0.4886,  0.1687]],\n\n        [[ 0.0398, -0.0331, -0.1104,  ...,  0.0823,  0.0517,  0.0683]]]), tensor([[[-0.4428,  0.7513, -0.5056,  ...,  0.6072, -1.0234,  1.7888]],\n\n        [[-0.3533,  0.9564,  0.1720,  ...,  1.0431, -0.9805,  1.9370]],\n\n        [[ 0.5125, -0.0373,  0.5286,  ...,  1.0479, -0.8934,  1.1945]],\n\n        ...,\n\n        [[-1.1069,  0.6487,  0.3168,  ...,  0.2475, -0.1425,  0.0610]],\n\n        [[-0.0994, -0.0339, -0.4353,  ...,  0.3277,  0.2257,  0.5112]],\n\n        [[-0.0995, -0.0065, -0.1793,  ..., -0.0098,  0.0282,  0.3359]]]))\nLength Ooutputstput[1]:\n12\nSample from Output[1], first hidden layer:\ntensor([[[-0.0392,  0.0757, -0.0086,  ..., -0.0604,  0.0133,  0.0358]],\n\n        [[ 0.0505,  0.1237,  0.0158,  ..., -0.0261, -0.0447,  0.1084]],\n\n        [[ 0.0773, -0.0132,  0.0771,  ...,  0.0277,  0.0370,  0.0216]],\n\n        ...,\n\n        [[-0.0162,  0.0391,  0.0709,  ...,  0.0444,  0.0109,  0.0179]],\n\n        [[ 0.0788, -0.0583, -0.0905,  ...,  0.0493,  0.0634, -0.0520]],\n\n        [[ 0.0181, -0.0015, -0.1494,  ...,  0.0012, -0.0009,  0.0188]]])\nSample shape, first hidden layer\ngetting the last tensor for XLNet\ntensor([-2.0352e+00, -7.4594e-01, -1.2483e+00,  2.7704e-01, -9.2360e-01,\n        -2.2569e+00,  4.6218e-02,  4.9753e-01, -6.9030e-01,  1.5298e+00,\n        -1.4199e-01,  4.6517e-01, -5.6921e-02,  1.0507e+00,  2.6407e+00,\n         9.1235e-01,  1.7678e+00,  1.4229e-01, -3.9071e-01, -1.4704e+00,\n        -2.8074e-01, -3.7374e-01, -2.1259e-01,  1.7680e+00, -8.0825e-01,\n         1.7710e+00, -1.0657e+00,  6.7874e-01, -1.7808e-01, -3.5737e-01,\n        -1.1293e+00,  1.0174e+00,  8.3484e-01,  1.6408e+00, -1.3627e+00,\n        -7.8317e-03, -6.8740e-02, -3.5971e-01,  2.6299e-01,  7.6126e-01,\n        -2.9896e-01, -2.1819e+00, -8.5582e-01,  8.7110e-01, -1.1669e-01,\n        -5.7015e-01, -4.9273e-01, -2.3181e-01, -1.3853e-01,  1.0813e+00,\n         1.5322e+00, -1.1378e+00, -7.4074e-01,  7.5063e-01, -7.3392e-01,\n        -1.2091e+00,  1.6927e-01,  1.6505e-01, -4.7577e-01, -1.1384e-01,\n        -1.6275e-01, -7.6925e-01,  1.2061e+00, -2.7114e-01, -3.6937e-01,\n         1.3982e+00, -1.2223e+00, -2.6805e-01, -9.4588e-02,  2.7273e-01,\n         5.7899e-01, -4.6098e-01, -8.0659e-01,  1.3129e+00, -9.2719e-01,\n        -5.1734e-01, -4.0346e-02,  2.8093e-01,  1.4545e+00,  8.3922e-02,\n         6.8335e-01,  4.8226e-01,  5.1209e-01, -9.2136e-01, -9.5376e-01,\n        -1.4782e-01,  2.6121e+00, -1.8462e+00,  1.9648e+00, -5.5698e-01,\n        -2.4213e+00, -8.0133e-01,  4.1877e+00, -1.4166e+00,  6.6741e-01,\n        -1.1393e+00, -1.0766e+00, -2.5456e-01, -1.3038e+00, -7.2826e-01,\n        -6.9968e-01,  8.5604e-01,  3.1408e-01, -1.5752e+00,  8.9733e-01,\n         1.4621e+00, -3.2451e-01, -1.1845e-02, -3.7351e-01,  7.2937e-02,\n        -1.6594e-02,  4.9549e-01,  1.0388e-01, -1.0027e+00,  2.0848e+00,\n        -2.0531e+00, -1.8724e+00, -5.4268e-01,  6.7576e-01, -1.1288e-01,\n        -1.1220e+00, -1.2811e+00,  1.3528e+00,  8.7804e-01,  6.1239e-01,\n        -7.8091e-01,  1.1542e-01,  4.2083e-01, -4.0674e-01,  2.6557e-01,\n        -8.6984e-01,  1.0670e+00,  1.7218e+00,  4.4172e-01, -1.2454e+00,\n        -3.0794e-01,  2.4978e+00,  1.7949e-01,  5.0063e-01, -2.2168e+00,\n         7.4135e-01, -4.4063e-01,  3.8813e-01,  5.7640e-01,  2.6053e+00,\n         1.4398e+00, -4.7703e-01, -2.1202e+00,  7.7339e-01, -2.0671e+00,\n         3.4432e-01,  1.1573e+00, -2.3420e-02,  8.5365e-01, -5.9780e-01,\n         1.0061e+00, -2.7079e-01, -1.3978e+00, -6.1293e-01,  2.5986e-01,\n        -1.6914e+00,  6.3186e-01,  2.4085e-01,  1.8748e+00, -6.5328e-01,\n        -2.2379e-01, -1.2164e-01,  1.4595e+00,  1.1505e-01, -1.0581e+00,\n        -8.2945e-01, -5.5500e-01,  1.2573e-01, -8.8654e-01, -1.0162e+00,\n        -3.5146e-01, -6.9959e-01,  1.0833e+00,  2.1035e-01,  2.1175e+00,\n        -3.2840e+00,  1.4162e+00, -1.6342e+00,  1.3701e+00,  1.4583e+00,\n        -7.6808e-01, -5.5679e-01, -9.7250e-01, -8.1496e-01,  1.1688e-01,\n        -1.4961e-01,  1.2244e+00, -3.4364e-01, -3.5357e+00, -4.1832e-01,\n        -1.2863e+00,  3.9762e-01,  1.1294e+00, -1.1765e+00, -5.9175e-01,\n        -6.4113e-01,  4.5662e-02, -3.6085e-01, -6.9537e-01, -4.0079e-01,\n        -5.0391e-01,  6.3814e-01,  5.1338e-01, -4.5915e-01,  4.1880e-01,\n         1.6801e+00, -1.6596e+00, -7.9370e-01, -5.3025e-01, -1.3840e+00,\n         8.1746e-01, -3.4401e-01,  3.0566e-01, -1.4877e-01,  1.8570e-01,\n         1.0220e+00, -1.0036e+00, -3.6699e-01, -2.5422e-02,  5.6186e-01,\n         9.0266e-01,  1.0022e+00, -5.8833e-02, -1.1116e+00, -9.0754e-01,\n         6.5724e-01, -9.5088e-01,  7.4474e-01, -6.0521e-01, -4.7405e-01,\n        -8.1487e-01,  1.4703e+00,  2.2047e-01, -1.9950e-01, -6.0142e-01,\n         1.2927e+00,  2.9186e-01, -1.4617e+00, -9.5453e-02,  7.5912e-01,\n         1.6503e-01,  6.1505e-01, -1.2207e+00, -1.1682e+00, -2.3239e-01,\n         5.9929e-01, -6.8289e-01, -3.0156e+00,  7.8811e-01,  2.7530e-02,\n        -7.3157e-02,  6.2444e-01,  8.3523e+00, -5.6549e-01, -1.6306e+00,\n        -1.1893e+00,  1.1225e+00,  1.5421e-01, -5.2743e-01, -4.0059e-01,\n         1.5519e+00,  1.5074e+00,  4.3914e-01,  9.5529e-01, -6.8576e-01,\n        -1.4680e+00,  6.9173e-01, -1.7689e+00,  1.3464e+00, -1.1263e+00,\n         1.2112e+00, -5.6610e-01, -6.4389e-01, -5.6690e-01, -4.7822e-01,\n        -6.1068e-01, -1.2266e+00,  2.0016e-01, -8.5185e-01, -1.1959e+00,\n        -1.6579e+00, -1.2856e+00,  1.1771e+00, -2.0377e-01,  1.2007e+00,\n         6.4889e-01, -1.1651e+00, -1.2093e+00,  1.5352e+00,  6.5407e-01,\n        -7.0839e-01, -9.2461e-01, -3.3411e-01,  3.3665e-01, -6.6227e-01,\n         9.4070e-01,  4.7289e-01, -4.5054e-01,  8.4006e-01, -4.0040e-01,\n         9.5890e-01,  4.8597e-01, -4.9718e-01,  1.2888e+00,  5.6052e-01,\n         1.5426e-01, -4.3352e-01,  1.4656e+00, -1.4544e-01,  5.1518e-01,\n        -9.9548e-01,  2.4424e+00, -1.3236e+00,  8.8920e-01, -1.0838e+00,\n         3.4093e-01,  1.0559e+00, -7.3650e-01,  5.3090e-01, -7.1494e-01,\n         2.3563e-02, -5.9014e-01,  1.0796e+00, -2.1440e+00,  1.9825e+00,\n        -9.1331e+00,  6.8671e-01, -9.5877e-02, -1.8320e+00,  1.2004e-01,\n         1.8678e+00,  8.3656e-01, -1.6186e+00,  1.8389e-01, -9.4188e-01,\n         1.0106e+00,  1.3786e+00,  1.0666e+00, -1.1857e+00,  2.1708e+00,\n        -7.4860e-02,  9.0395e-01, -1.2154e+00, -9.9090e-01, -1.2942e+00,\n        -7.9626e-02, -2.1783e-01,  1.4368e+00,  3.1796e-01,  6.5103e-01,\n        -5.7326e-01,  7.2539e-01,  9.8162e-01, -3.1983e-02,  8.7098e-01,\n         8.2037e-01,  4.3618e-01,  6.4968e-02, -2.5192e+00, -1.5899e+00,\n        -6.6330e-01,  7.8149e-01, -7.9762e-02,  2.3487e-01,  2.6889e-01,\n        -5.9201e-01,  1.0208e-01, -1.4536e+00,  1.0161e-01, -9.9719e-01,\n         7.4965e-01, -6.8792e-01, -6.7298e-01, -3.5189e+00, -2.2065e+00,\n         1.2324e-01, -1.6383e-01,  4.4152e-02,  5.5689e-01,  1.0553e+00,\n        -4.0996e-01, -1.3053e+00, -1.3536e+00, -6.8356e-01, -2.1172e-01,\n         1.4176e+00,  7.0120e-01,  1.8608e+00, -3.2196e-02,  3.7582e-01,\n        -7.8904e-01, -7.8323e-01, -3.8088e-01,  7.4794e-01,  3.3434e-01,\n        -6.7428e-02, -1.6129e-01, -1.0153e+00,  1.1600e+00, -1.4051e+00,\n         6.6113e-01, -6.1449e-02, -8.0859e-01,  2.0342e-01,  1.3062e+00,\n        -6.6693e-01, -4.4530e-02, -2.7038e+00, -8.0196e-01, -7.0162e-01,\n         2.8141e-01, -5.5724e-01,  1.6884e+00, -1.0385e-01,  5.0931e-01,\n        -1.0475e+00,  2.0039e+00, -4.9857e-01, -1.8011e-01, -3.9891e-01,\n         6.2017e-01, -1.9050e-01, -2.0116e-01,  1.6382e+00, -2.0847e-01,\n         2.6934e-01,  7.0485e-01,  1.9716e-01, -8.4203e-01,  2.8822e-01,\n        -6.1833e-01,  1.3639e+00,  2.0384e+00, -6.3023e-01,  6.0356e-01,\n         2.7525e-01, -9.4143e-01, -8.5422e-01,  1.1822e-02, -1.4065e+00,\n        -6.9361e-01, -1.6869e-01,  9.0843e-01, -2.7469e+00,  9.0725e-01,\n         1.2227e+00, -1.1545e+00, -1.2336e+00,  8.6200e-01,  3.6677e-01,\n         7.2761e-01, -3.0450e-01, -1.0072e-01, -2.2484e-01,  3.6996e-01,\n        -5.7640e-01, -4.3807e-01,  3.4032e-01, -9.1682e-02,  2.6710e-01,\n         4.1406e-01,  1.0100e+00, -8.4797e-01,  7.9077e-01, -3.6642e-01,\n        -1.1062e+00, -4.6594e-01, -1.5245e+00, -7.6405e-01,  1.3395e+00,\n        -5.3676e-01, -2.8124e-01,  3.1370e-01,  2.6333e-01, -8.2525e-01,\n        -2.5459e+00,  6.7103e-02, -1.9003e-01,  7.5516e-01,  6.9154e-01,\n         1.1726e+00,  6.0416e-01, -2.8138e+00,  9.8960e-01, -1.2905e+00,\n         2.2173e+00,  2.5493e+00, -2.8306e-01,  8.6396e-02, -1.7735e-01,\n         1.1203e-01,  1.5412e-01,  1.9901e-02, -6.7589e-01, -2.9238e-01,\n        -3.2283e-01,  8.9131e-01, -1.5200e+00, -6.2037e-01, -5.5975e-02,\n         7.4246e-01,  9.2218e-01, -5.3923e-01, -8.1739e-01,  1.3160e+00,\n        -1.8323e-01,  1.7299e+00, -1.0067e+00,  1.6091e+00,  2.1841e-01,\n        -1.4825e+00,  9.3424e-01, -1.2887e+00, -1.4811e-02, -3.2028e-01,\n         2.5107e-01,  1.8825e-01, -7.3573e-01, -1.3337e+00, -6.3765e-01,\n        -5.1511e-01,  7.1293e-01,  4.2068e-01, -5.2091e-02, -1.9354e+00,\n         2.0426e+00, -1.6377e-01, -1.2185e+00,  2.4280e-01, -6.2956e-01,\n         1.6176e-01,  7.2556e-04, -7.4231e-02,  8.0382e-01,  2.1332e-01,\n        -4.0409e-01, -1.9225e+00,  2.6454e-01,  1.7437e-01,  1.1330e+00,\n         2.6382e-01, -1.3843e+00,  8.0708e-02,  5.4398e-01,  6.4379e-01,\n         4.0142e-01,  1.5949e+00,  9.7710e-01, -1.9774e+00,  3.1894e-01,\n        -5.1257e-01,  1.1125e+00, -1.1732e+00, -3.7842e-01, -8.6253e-01,\n         5.0061e-01, -4.0398e-01,  2.9609e-01,  8.8540e-01,  2.8547e-01,\n        -6.0169e-01, -1.6752e+00, -7.8997e-01,  1.1717e+00,  1.1541e+00,\n        -8.8416e-01, -4.0280e-03, -3.3521e-01,  1.1131e+00, -1.8470e-01,\n         1.2370e-01,  8.7369e-01, -7.3121e-01, -6.7039e-01, -2.7838e-01,\n         1.3032e+00,  1.9925e-01,  3.9233e-01,  5.5377e-01,  2.7492e-01,\n         1.2779e+00,  5.9326e-01, -1.8216e-01,  3.7227e-01,  1.1062e+00,\n        -3.4917e-01, -6.0388e-01,  3.1725e-01,  8.5340e-01,  6.0764e-01,\n        -7.8278e-01,  7.2719e-01,  5.8284e-01, -1.7581e-01, -2.3554e+00,\n        -7.9922e-01,  1.1021e+00, -2.5595e-01, -1.3400e+00, -2.8750e-01,\n         3.4557e+00,  6.7567e-01,  4.5798e-01, -1.0375e+00, -1.3295e+00,\n         4.0707e-01, -1.1363e+00, -5.7930e-01,  5.1162e-01,  5.8013e-01,\n         6.2001e-01, -1.2609e+00,  5.9100e-01,  5.7265e-02, -8.0529e-01,\n        -1.4211e-01, -1.4243e-01, -2.0748e-01,  1.0908e+00,  2.1019e-01,\n         8.7740e-01, -1.8347e+00,  5.3997e-01, -8.1524e-02,  8.5377e-01,\n         4.3323e+00, -1.1751e+00, -8.0986e-01, -2.5803e-02,  2.5432e-01,\n         1.0033e+00,  5.1689e-03,  8.8013e-03,  9.7982e-01, -5.8224e-01,\n        -1.0044e-01, -2.9252e-01, -6.8143e-01, -8.3959e-01, -6.6590e-01,\n        -5.4197e-01,  3.1242e-01,  1.0714e-02,  9.7264e-01,  1.2738e+00,\n        -5.5882e-01, -4.3683e-01, -3.1451e+00,  7.3274e-01, -3.6776e-01,\n         4.9634e-01, -1.9859e-01, -1.4624e+00, -1.6135e+00, -1.5278e+00,\n        -1.8381e+00, -1.1693e-01, -1.4701e+00, -1.8395e-01, -1.6966e+00,\n        -2.1208e-01, -6.7903e-01,  4.0045e+01, -4.7230e-01, -6.7194e-01,\n        -5.6052e-01, -6.7642e-01,  1.0483e-01, -4.4741e-01,  1.7702e-01,\n        -6.5740e-01,  6.3357e-01, -5.7000e-01,  1.9963e-01,  5.0326e-01,\n        -1.5145e+00, -7.5576e-01,  7.1292e-01,  7.6210e-01, -5.8373e-01,\n        -4.6924e-02, -3.3170e-01,  3.8050e-01, -1.2203e-01, -4.3586e-01,\n         5.3789e-01, -7.5975e-01, -2.5392e-01,  1.0090e+00,  6.1319e-02,\n        -9.1317e-04, -1.6908e+00,  3.9303e-01,  2.2087e+00,  1.0489e+00,\n         2.3406e+00, -1.1987e-01,  7.5149e-01,  6.9739e-01,  2.3559e-01,\n        -1.3353e-01,  2.1592e-01, -1.3979e+00,  1.1730e+00,  7.3602e-01,\n        -2.0583e+00,  2.5878e+00, -2.6168e+00, -9.2105e-01,  1.3132e+00,\n        -1.4393e+00,  7.1269e-01, -5.6866e-01, -1.1218e+00,  5.6473e-01,\n        -4.4311e-01,  3.4880e-01, -8.5216e-01, -1.3676e+00, -1.3218e+00,\n        -5.4484e-01,  3.7176e-02,  1.5428e+00, -1.3672e+00,  9.4198e-01,\n         1.4508e-01, -1.2627e-01,  1.1064e+00, -2.9862e-01, -1.3265e+00,\n        -2.1401e-01, -5.9890e-01,  5.5767e-01,  5.0707e-01,  3.0541e-01,\n        -1.7802e-01,  4.1437e-01,  2.2955e-02, -4.0383e-01,  6.0431e-01,\n        -9.3289e-01,  3.7603e+00,  2.2909e-01,  3.9138e-01,  8.0270e-01,\n         1.5046e-01, -1.0833e+00, -9.4525e-01,  1.0055e+00, -7.9685e-01,\n        -1.7952e-02,  1.7609e-02,  8.7880e-02, -5.5387e-01, -7.8472e-01,\n         1.5111e+00,  1.2925e+00, -5.3705e-01, -4.7830e-01,  1.0529e-01,\n        -4.7062e-02, -4.2137e-01,  9.5441e-02])\nsample_embeddings.shape (3, 768)\n[[-1.5467558   0.41237524 -1.3485138  ... -1.2315857   0.12427384\n  -0.17078479]\n [-0.56340367  0.54620034  0.31551552 ... -0.42775282  2.4665885\n  -0.619147  ]\n [-2.0352373  -0.74594027 -1.2482884  ... -0.04706213 -0.4213665\n   0.09544098]]\n"
    }
   ],
   "source": [
    "# sample_embeddings=getXLNetEmbeddings_testMode(sampleDF,\"processed_synopsis_t1\",verbose=True)\n",
    "sample_embeddings=getXLNetEmbeddings(sampleDF,\"processed_synopsis_t1\",verbose=True)\n",
    "print(\"sample_embeddings.shape\",sample_embeddings.shape)\n",
    "print(sample_embeddings)\n"
   ]
  },
  {
   "source": [
    "HERE: make sure that the output is (3, 768). Where 3 is the number of texts we sent to the model and 768 is the output of embedding length for the model chosen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now We run the model for each text we have in our Dataset "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Type 1 Embeddings\n",
    "xlnet_embeddings_t1=getXLNetEmbeddings(mpstDF_processsed,\"processed_synopsis_t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \",xlnet_embeddings_t1.shape)\n",
    "print(\"XL Embedding for Type 1\")\n",
    "print(xlnet_embeddings_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"xl_embeddings_type1.npz\",xlnet_embeddings_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Type 2 Embeddings\n",
    "xlnet_embeddings_t2=getXLNetEmbeddings(mpstDF_processsed,\"processed_synopsis_t2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \",xlnet_embeddings_t2.shape)\n",
    "print(\"Embedding\")\n",
    "print(xlnet_embeddings_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"xl_embeddings_type1.npz\",xlnet_embeddings_t2)"
   ]
  },
  {
   "source": [
    "Saved the embeddings in different files\n",
    "Now saving them in the same file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"xl_embeddings.npz\",t1=xlnet_embeddings_t1,t2=xlnet_embeddings_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_check=np.load(\"xl_embeddings.npz\")\n",
    "print(\"t1\")\n",
    "print(em_check[\"t1\"])\n",
    "print(em_check[\"t1\"].shape)\n",
    "print(\"t2\")\n",
    "print(em_check[\"t2\"])\n",
    "print(em_check[\"t2\"].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "display_name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "870dc015ab544cf4fd4c91e2f8042f40253e24aac5eeb5e28336055566efd434"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}