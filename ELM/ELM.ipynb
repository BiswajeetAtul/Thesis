{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "main_start=time.time()\n",
    "time_log={\"main_start\":main_start}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpstDF= pd.read_csv(\"mpst.csv\")\n",
    "mpstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_mpstDF_train=mpstDF[mpstDF[\"split\"]==\"train\"]\n",
    "final_mpstDF_validation=mpstDF[mpstDF[\"split\"]==\"val\"]\n",
    "final_mpstDF_test=mpstDF[mpstDF[\"split\"]==\"test\"]\n",
    "print(\"Shape Train\",final_mpstDF_train.shape)\n",
    "print(\"Shape Validation\",final_mpstDF_validation.shape)\n",
    "print(\"Shape Test\",final_mpstDF_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mpstDF_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mpstDF_validation.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mpstDF_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING THE BERT EMBEDDINGS AND Y MATRIX\n",
    "bert_embedding=np.load(\"D:\\CodeRepo\\Thesis\\Thesis\\BERT\\embeddings.npz\")\n",
    "label_values=np.load(\"D:\\CodeRepo\\Thesis\\Thesis\\EDA\\Y.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type1_BERT_Embeddings=bert_embedding[\"t1\"]\n",
    "type2_BERT_Embeddings=bert_embedding[\"t2\"]\n",
    "label_values=label_values[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type1_BERT_Embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_Embeddings(x_t1,x_t2,y,df,partition_nm):\n",
    "    _df=df[df[\"split\"]==partition_nm]\n",
    "    index_list=list(_df.index)\n",
    "    temp_array_x_t1=[]\n",
    "    temp_array_x_t2=[]\n",
    "    temp_array_y=[]\n",
    "    for index in index_list:\n",
    "        temp_array_x_t1.append(x_t1[index,:])\n",
    "        temp_array_x_t2.append(x_t2[index,:])\n",
    "        temp_array_y.append(y[index,:])\n",
    "    temp_array_x_t1=np.array(temp_array_x_t1)\n",
    "    temp_array_x_t2=np.array(temp_array_x_t2)\n",
    "    temp_array_y=np.array(temp_array_y)\n",
    "    return temp_array_x_t1,temp_array_x_t2, temp_array_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TRAIN\n",
    "type1_BERT_Embeddings_Train,type2_BERT_Embeddings_Train,label_values_Train=get_partition_Embeddings(type1_BERT_Embeddings,type2_BERT_Embeddings,label_values,mpstDF,\"train\")\n",
    "# FOR VALIDATION\n",
    "type1_BERT_Embeddings_Val,type2_BERT_Embeddings_Val,label_values_Val=get_partition_Embeddings(type1_BERT_Embeddings,type2_BERT_Embeddings,label_values,mpstDF,\"val\")\n",
    "# FOR TEST\n",
    "type1_BERT_Embeddings_Test,type2_BERT_Embeddings_Test,label_values_Test=get_partition_Embeddings(type1_BERT_Embeddings,type2_BERT_Embeddings,label_values,mpstDF,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SEPARTAING THE TRAIN TEST AND VALIDATION ROWS\n",
    "print(type1_BERT_Embeddings_Train.shape)\n",
    "print(type2_BERT_Embeddings_Train.shape)\n",
    "print(type1_BERT_Embeddings_Val.shape)\n",
    "print(type2_BERT_Embeddings_Val.shape)\n",
    "print(type1_BERT_Embeddings_Test.shape)\n",
    "print(type2_BERT_Embeddings_Test.shape)\n",
    "print(label_values_Train.shape)\n",
    "print(label_values_Val.shape)\n",
    "print(label_values_Test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix, hamming_loss,classification_report\n",
    "\n",
    "# Activation Functions\n",
    "\n",
    "# def _identity(x):\n",
    "#     return x\n",
    "# def _binary_step(x, threshold = 0):\n",
    "#     return 1 if x=threshold else 0\n",
    "# def _biploar_step(x ,threshold = 0):\n",
    "#     return 1 if x>=threshold else -1\n",
    "# def _binary_sigmoid(x):\n",
    "#     return 1. / (1. + np.exp(-x))\n",
    "# def _bipolar_sigmoid(x):\n",
    "#     return (1. - np.exp(-x))/(1. + np.exp(-x))\n",
    "# def _relu_function(x):\n",
    "#     return np.max(0, x)\n",
    "# def _relu_leaky(x):\n",
    "#     return np.max(0.01*x, x)\n",
    "\n",
    "\n",
    "_identity =np.vectorize(lambda x: x)\n",
    "_binary_step =np.vectorize(lambda x,t=0: 1 if x>t else 0)\n",
    "_biploar_step =np.vectorize(lambda x,t=0: 1 if x>t else -1)\n",
    "_binary_sigmoid=np.vectorize(lambda x: 1. / (1. + np.exp(-x)))\n",
    "_bipolar_sigmoid=np.vectorize(lambda x: (1. - np.exp(-x))/(1. + np.exp(-x)))\n",
    "_relu_function=np.vectorize(lambda x: np.max([0, x]))\n",
    "_relu_leaky=np.vectorize(lambda x: np.max([0.01*x, x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ELM_MultiLabel:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, activation, bias=True, random_gen=\"uniform\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_nodes ([integer]): Number of Input nodes\n",
    "            hidden_nodes ([integer]): Number of hidden nodes\n",
    "            output_nodes ([integer]): Number of output nodes\n",
    "            activation ([function]): The function which will be used as the activation function in the hidden layer\n",
    "            bias ([boolean]): Flag to use bias, if True then randomly generate bias @random_gen else bias - 0.\n",
    "            random_gen (str, optional): The type way in which random weight are generated. Defaults to \"uniform\".\n",
    "        \"\"\"\n",
    "        self.__input_nodes = input_nodes\n",
    "        self.__hidden_nodes = hidden_nodes\n",
    "        self.__output_nodes = output_nodes\n",
    "        if random_gen == \"uniform\":\n",
    "            self.__beta = np.random.uniform(-1.,1.,size = (self.__hidden_nodes, self.__output_nodes))\n",
    "            self.__alpha = np.random.uniform(-1.,1.,size = (self.__input_nodes, self.__hidden_nodes))\n",
    "            self.__bias = np.random.uniform(size = (self.__hidden_nodes,))\n",
    "        else:\n",
    "            self.__beta = np.random.normal(-1.,1.,size=(self.__hidden_nodes, self.__output_nodes))\n",
    "            self.__alpha = np.random.normal(-1.,1.,size=(self.__input_nodes, self.__hidden_nodes))\n",
    "            self.__bias = np.random.normal(size=(self.__n_hidden_nodes,)) \n",
    "        self.__activation = activation # Sigmoid Function\n",
    "\n",
    "    def getInputNodes(self):\n",
    "        return  self.__input_nodes\n",
    "\n",
    "    def getHiddenNodes(self):\n",
    "        return  self.__hidden_nodes\n",
    "\n",
    "    def getOutputNodes(self):\n",
    "        return  self.__output_nodes\n",
    "    \n",
    "    def getBetaWeights(self):\n",
    "        return self.__beta\n",
    "    \n",
    "    def getAlphaWeight(self):\n",
    "        return self.__alphs\n",
    "    \n",
    "    def getBias(self):\n",
    "        return self.__bias\n",
    "\n",
    "    def __get_H_matrix(self, train_x, verbose=False):\n",
    "        # 1 Propagate data from Input to hidden Layer\n",
    "        if verbose:\n",
    "            print(\"Propagate data from Input to hidden Layer\")\n",
    "        inp = np.dot(train_x , self.__alpha)\n",
    "        if verbose:\n",
    "            print(inp)\n",
    "            print(\"Adding Biases\")\n",
    "        inp = inp  + self.__bias\n",
    "        if verbose:\n",
    "            print(inp)\n",
    "            print(\"Applyin activation function\")\n",
    "        inp_activation = np.apply_along_axis(self.__activation, 1, inp)\n",
    "        return inp_activation\n",
    "\n",
    "    def fit(self, train_x, train_y, verbose = False, show_metrics = True):\n",
    "        \"\"\"\n",
    "        This function calculates the Beta weights or the output weights\n",
    "        train_x : input matrix\n",
    "        train_y : output matrix to be predicted or learned upon unipolar\n",
    "\n",
    "        returns: if test_y is not given then\n",
    "                returns the predicted output\n",
    "                if test_y is given then returns predicted output and evaluation metrics dict \n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"train_x shape:\", train_x.shape)\n",
    "            print(\"train_y shape:\", train_y.shape)\n",
    "        inp_activation = self.__get_H_matrix(train_x, verbose)\n",
    "        # This is the H matrix getting its Moore Penrose Inverse\n",
    "        if verbose:\n",
    "            print(inp_activation)\n",
    "            print(\"Getting the Generalized Moore Penrose Inverse\")\n",
    "        generalizedInverse = np.linalg.pinv(inp_activation)\n",
    "        if verbose:\n",
    "            print(generalizedInverse)\n",
    "            print(\"Finding Beta, output weights\")\n",
    "        # Now find output weight matrix Beta \n",
    "        # convert input Y values according to the threshold using biploar step function\n",
    "        predicted_bipolar=  np.apply_along_axis(_biploar_step, 1, train_y)\n",
    "        self.__beta = np.dot(generalizedInverse, predicted_bipolar)\n",
    "        if verbose:\n",
    "            print(\"Beta Matrix Weights\")\n",
    "            print(self.__beta)\n",
    "\n",
    "        print(\"Model Metrics, for Training :\")\n",
    "        return self.predict(train_x, train_y,verbose,show_metrics)\n",
    "    \n",
    "    def predict(self, test_x, test_y = None, verbose = False, show_metrics= True):\n",
    "        \"\"\"\n",
    "        preditcts the output for the input test data\n",
    "        call this after calling the fit.\n",
    "        test_data shape should be (batch_size,768 or input_nodes)\n",
    "        output_shape will be (batch_size, 71 or output_nodes)\n",
    "\n",
    "        returns: if test_y is not given then\n",
    "                returns the predicted output\n",
    "                if test_y is given then returns predicted output and evaluation metrics dict\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"Predicting outputs\")\n",
    "        inp_activation = self.__get_H_matrix(test_x, verbose)\n",
    "        output_predicted = np.dot(inp_activation, self.__beta)\n",
    "        # convert predicted according to the threshold using biploar step function\n",
    "        predicted_bipolar =  np.apply_along_axis(_biploar_step, 1, output_predicted)\n",
    "        predicted_binary = np.apply_along_axis(_binary_step, 1, predicted_bipolar)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"predicted output\")\n",
    "            print(output_predicted)\n",
    "            print(\"predicted_bipolar\")\n",
    "            print(predicted_bipolar)\n",
    "            print(\"predicted_binary\")\n",
    "            print(predicted_binary)\n",
    "            print(\"Original Binary\")\n",
    "            print(test_y)\n",
    "\n",
    "        eval_dict={}\n",
    "        if (test_y is not None):\n",
    "            eval_dict=self.__evaluate(test_y,predicted_binary, for_test=False)\n",
    "        if(test_y is not None):\n",
    "            return predicted_binary, eval_dict\n",
    "        else:\n",
    "            return predicted_binary\n",
    "\n",
    "    def __evaluate(self, real, predicted, for_test=True):\n",
    "        \"\"\"\n",
    "        real values as 0,1\n",
    "        predicted values as 0,1\n",
    "        \"\"\"\n",
    "        # Now we find accuracy, precision, recall, Hamming Loss and F1 Measure\n",
    "        accuracy = accuracy_score(real, predicted)\n",
    "        hamLoss = hamming_loss(real, predicted)\n",
    "        # element wise correctness\n",
    "        term_wise_accuracy=np.sum(np.logical_not(np.logical_xor(real, predicted)))/real.size\n",
    "\n",
    "        macro_precision = precision_score(real, predicted, average='macro')\n",
    "        macro_recall = recall_score(real, predicted, average='macro')\n",
    "        macro_f1 = f1_score(real, predicted, average='macro')\n",
    "\n",
    "        micro_precision = precision_score(real, predicted, average='micro')\n",
    "        micro_recall = recall_score(real, predicted, average='micro')\n",
    "        micro_f1 = f1_score(real, predicted, average='micro')\n",
    "        \n",
    "        metricTable=prettytable.PrettyTable()\n",
    "        metricTable.field_names = [\"Metric\", \"Macro Value\", \"Micro Value\"]\n",
    "        metricTable.add_row([\"Hamming Loss\",\"{0:.3f}\".format(hamLoss) ,\"\"])\n",
    "        metricTable.add_row([\"Term Wise Accuracy\",\"{0:.3f}\".format(term_wise_accuracy) ,\"\"])\n",
    "\n",
    "        metricTable.add_row([\"Accuracy\",\"{0:.3f}\".format(accuracy),\"\"])\n",
    "        metricTable.add_row([\"Precision\",\"{0:.3f}\".format(macro_precision),\"{0:.3f}\".format(micro_precision)])\n",
    "        metricTable.add_row([\"Recall\",\"{0:.3f}\".format(macro_recall),\"{0:.3f}\".format(micro_recall)])\n",
    "        metricTable.add_row([\"F1-measure\",\"{0:.3f}\".format(macro_f1),\"{0:.3f}\".format(micro_f1)])\n",
    "\n",
    "        print(metricTable)\n",
    "\n",
    "        print(\"Metrics @ Literature\")\n",
    "        lit_HamminLosss, lit_accuracy, lit_precision, lit_recall, lit_f1 = self.get_eval_metrics(real,predicted)\n",
    "\n",
    "        return_dict = {\"HiddenNodes\": self.getHiddenNodes(),\n",
    "                \"lit_HamminLosss\": lit_HamminLosss,\n",
    "                \"lit_accuracy\": lit_accuracy,\n",
    "                \"lit_precision\": lit_precision,\n",
    "                \"lit_recall\": lit_recall,\n",
    "                \"lit_f1\": lit_f1,\n",
    "                \"sklearn_hamLoss\": lit_accuracy,\n",
    "                \"sklearn_accuracy\": accuracy,\n",
    "                \"sklearn_macro_precision\": macro_precision,\n",
    "                \"sklearn_micro_precision-\": lit_accuracy,\n",
    "                \"sklearn_macro_recall\": macro_recall,\n",
    "                \"sklearn_micro_precision\": micro_precision,\n",
    "                \"sklearn_macro_f1\": macro_f1,\n",
    "                \"sklearn_micro_f1\": micro_f1,\n",
    "                \"term_wise_accuracy\": term_wise_accuracy,\n",
    "                }\n",
    "\n",
    "\n",
    "        print(\"Test Classification Report\")\n",
    "        print(classification_report(real,predicted))\n",
    "\n",
    "        return return_dict\n",
    "\n",
    "    def get_eval_metrics(self, real, predicted, verbose= False):\n",
    "        err_cnt_accuracy=0\n",
    "        err_cnt_precision=0\n",
    "        err_cnt_recall=0\n",
    "        if verbose:\n",
    "            print(real)\n",
    "            print(predicted)\n",
    "        for x in range(real.shape[0]):\n",
    "            err_and= np.logical_and(real[x],predicted[x])\n",
    "            err_or = np.logical_or(real[x],predicted[x])\n",
    "            # Accuracy\n",
    "            err_cnt_accuracy +=(sum(err_and)/sum(err_or))\n",
    "\n",
    "            # Precision\n",
    "            if sum(err_and) != 0:\n",
    "                err_cnt_precision += (sum(err_and) / sum(predicted[x]))\n",
    "            # Recall\n",
    "            err_cnt_recall += (sum(err_and) / sum(real[x]))\n",
    "            if verbose:\n",
    "                print(\"Iteration :\",x)\n",
    "                print((sum(err_and)/sum(err_or)))\n",
    "                print(err_and)\n",
    "                print(err_or)\n",
    "                print(err_cnt)\n",
    "        \n",
    "        err_count_hamming = np.zeros((real.shape))\n",
    "\n",
    "        for i in range(real.shape[0]):\n",
    "            for j in range(real.shape[1]):\n",
    "                if real[i,j] != predicted[i,j]:\n",
    "                    err_count_hamming[1,j] = err_count_hamming[1,j]+1\n",
    "\n",
    "        sum_err = np.sum(err_count_hamming);\n",
    "        HammingLoss = sum_err/real.size;\n",
    "        accuracy = err_cnt_accuracy / real.shape[0]\n",
    "        precision = err_cnt_precision / real.shape[0]\n",
    "        recall = err_cnt_recall / real.shape[0]\n",
    "        f1 = 2*((precision*recall)/(precision+recall))\n",
    "        if verbose:\n",
    "            print(\"Final: \")\n",
    "            print(\"Hamming Loss: \", HammingLoss)\n",
    "            print(\"Accuracy: \",accuracy)\n",
    "            print(\"precision: \",precision)\n",
    "            print(\"recall: \",recall)\n",
    "            print(\"f1: \",f1)\n",
    "\n",
    "        metricTable=prettytable.PrettyTable()\n",
    "        metricTable.field_names = [\"Metric\", \"Value\"]\n",
    "        metricTable.add_row([\" Literature Hamming Loss\",\"{0:.3f}\".format(HammingLoss)])\n",
    "        metricTable.add_row([\"Literature Accuracy\",\"{0:.3f}\".format(accuracy)])\n",
    "\n",
    "        metricTable.add_row([\"Literature Precision\",\"{0:.3f}\".format(precision)])\n",
    "        metricTable.add_row([\"LiteratureRecall\",\"{0:.3f}\".format(recall)])\n",
    "        metricTable.add_row([\"LiteratureF1-measure\",\"{0:.3f}\".format(f1)])\n",
    "\n",
    "        print(metricTable)\n",
    "\n",
    "        return HammingLoss,accuracy,precision,recall,f1\n"
   ]
  },
  {
   "source": [
    "Now The preprocessing and is done. We will save the time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a object to store the mertics for each model created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_log[\"End preprocessing\"]=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing a test for ELM model with 100 nodes\n",
    "#TRAIN\n",
    "print(\"TRAINING \\n\")\n",
    "t1_emlMLTC_50= ELM_MultiLabel(input_nodes=768, hidden_nodes=50, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 100 hidden nodes\")\n",
    "t1_emlMLTC_50.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "\n",
    "#TEST\n",
    "print(\"TESTING \\n\")\n",
    "t1_emlMLTC_50.predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n"
   ]
  },
  {
   "source": [
    "Now We will run the model for all the three types data sets we have viz.\n",
    "- TRAIN X\n",
    "  - type1_BERT_Embeddings_Train\n",
    "  - type2_BERT_Embeddings_Train\n",
    "- VALIDATION X\n",
    "  - type1_BERT_Embeddings_Val\n",
    "  - type2_BERT_Embeddings_Val\n",
    "- TEST X\n",
    "  - type1_BERT_Embeddings_Test\n",
    "  - type2_BERT_Embeddings_Test\n",
    "- TRAIN Y\n",
    "  - label_values_Train.shape\n",
    "- VALIDATION Y\n",
    "  - label_values_Val.shape\n",
    "- TEST Y\n",
    "  - label_values_Test.shape"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models_hidden_nodes=[100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000, 10000, 15000, 20000]\n",
    "t1_models={}\n",
    "t2_models={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_list=[]\n",
    "\n",
    "def add_data_to_metric_list(eval_dict, activation, type, start, phase, end, metrics_dict_list=metrics_dict_list):\n",
    "    eval_dict[\"activation\"]=activation\n",
    "    eval_dict[\"type\"]=type\n",
    "    eval_dict[\"start\"]=start\n",
    "    eval_dict[\"phase\"]=phase\n",
    "    eval_dict[\"end\"]=end\n",
    "\n",
    "    metrics_dict_list.append(eval_dict)\n"
   ]
  },
  {
   "source": [
    "Testing the above function with a simple 50 hidden layer node model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"t1_\"   +str(50)+\"_bipolar_sigmoid_train_start \\n\")\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_start\"]=time.time()\n",
    "t1_models[\"t1_\"+str(50)+\"_bipolar_sigmoid\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=50, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "predicted, eval_dict=t1_models[\"t1_\"+str(50)+\"_bipolar_sigmoid\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_end\"]=time.time()\n",
    "\n",
    "add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_start\"], \"train\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_end\"])\n",
    "\n",
    "print( \"t1_\"   +str(50)+\"_bipolar_sigmoid_test_start \\n\")\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_start\"]=time.time()\n",
    "predicted, eval_dict=t1_models[\"t1_\"+str(50)+\"_bipolar_sigmoid\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_end\"]=time.time()\n",
    "\n",
    "add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_start\"], \"test\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_end\"])\n",
    "\n",
    "metrics_dict_list"
   ]
  },
  {
   "source": [
    "TRAINING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_of_models_hidden_nodes:\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_bipolar_sigmoid_train_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_start\"]=time.time()\n",
    "    t1_models[\"t1_\"+str(i)+\"_bipolar_sigmoid\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_bipolar_sigmoid\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_start\"], \"train\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_end\"])\n",
    "\n",
    "    \n",
    "    print( \"t1_\"   +str(i)+\"_relu_leaky_train_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_train_start\"]=time.time()\n",
    "    t1_models[\"t1_\"+str(i)+\"_relu_leaky\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_relu_leaky)\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_relu_leaky\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(i)+\"_relu_leaky_train_start\"], \"train\", time_log[\"t1_\" +str(i)+\"_relu_leaky_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_biploar_step_train_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_train_start\"]=time.time()\n",
    "    t1_models[\"t1_\"+str(i)+\"_biploar_step\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_biploar_step)\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_biploar_step\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(i)+\"_biploar_step_train_start\"], \"train\", time_log[\"t1_\" +str(i)+\"_biploar_step_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_bipolar_sigmoid_train_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_start\"]=time.time()\n",
    "    t2_models[\"t2_\"+str(i)+\"_bipolar_sigmoid\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_bipolar_sigmoid\"].fit(type2_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type2_BERT_Embeddings_Train\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_start\"], \"train\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_relu_leaky_train_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_train_start\"]=time.time()\n",
    "    t2_models[\"t2_\"+str(i)+\"_relu_leaky\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_relu_leaky)\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_relu_leaky\"].fit(type2_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type2_BERT_Embeddings_Train\", time_log[\"t2_\" +str(i)+\"_relu_leaky_train_start\"], \"train\", time_log[\"t2_\" +str(i)+\"_relu_leaky_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_biploar_step_train_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_train_start\"]=time.time()\n",
    "    t2_models[\"t2_\"+str(i)+\"_biploar_step\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_biploar_step)\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_biploar_step\"].fit(type2_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type2_BERT_Embeddings_Train\", time_log[\"t2_\" +str(i)+\"_biploar_step_train_start\"], \"train\", time_log[\"t2_\" +str(i)+\"_biploar_step_train_end\"])\n",
    "\n"
   ]
  },
  {
   "source": [
    "TESTING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_of_models_hidden_nodes:\n",
    "    print( \"t1_\"   +str(i)+\"_bipolar_sigmoid_test_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_bipolar_sigmoid\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_start\"], \"test\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_relu_leaky_test_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_relu_leaky\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(i)+\"_relu_leaky_test_start\"], \"test\", time_log[\"t1_\" +str(i)+\"_relu_leaky_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_biploar_step_test_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_biploar_step\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(i)+\"_biploar_step_test_start\"], \"test\", time_log[\"t1_\" +str(i)+\"_biploar_step_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_bipolar_sigmoid_test_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_bipolar_sigmoid\"].predict(type2_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type2_BERT_Embeddings_Test\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_start\"], \"test\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_relu_leaky_test_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_relu_leaky\"].predict(type2_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type2_BERT_Embeddings_Test\", time_log[\"t2_\" +str(i)+\"_relu_leaky_test_start\"], \"test\", time_log[\"t2_\" +str(i)+\"_relu_leaky_test_end\"])\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_biploar_step_test_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_biploar_step\"].predict(type2_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type2_BERT_Embeddings_Test\", time_log[\"t2_\" +str(i)+\"_biploar_step_test_start\"], \"test\", time_log[\"t2_\" +str(i)+\"_biploar_step_test_end\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_metrics_df= pd.DataFrame(metrics_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_metrics_df.to_csv(\"Final_ELM_Mertics_BERT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_500= ELM_MultiLabel(input_nodes=768, hidden_nodes=500, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 500 hidden nodes\")\n",
    "t1_emlMLTC_500.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_1000= ELM_MultiLabel(input_nodes=768, hidden_nodes=1000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 1000 hidden nodes\")\n",
    "t1_emlMLTC_1000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_2000= ELM_MultiLabel(input_nodes=768, hidden_nodes=2000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 2000 hidden nodes\")\n",
    "t1_emlMLTC_2000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_3000= ELM_MultiLabel(input_nodes=768, hidden_nodes=3000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 3000 hidden nodes\")\n",
    "t1_emlMLTC_3000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_4000= ELM_MultiLabel(input_nodes=768, hidden_nodes=4000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 4000 hidden nodes\")\n",
    "t1_emlMLTC_4000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_5000= ELM_MultiLabel(input_nodes=768, hidden_nodes=5000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 5000 hidden nodes\")\n",
    "t1_emlMLTC_5000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_10000= ELM_MultiLabel(input_nodes=768, hidden_nodes=10000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 10000 hidden nodes\")\n",
    "t1_emlMLTC_10000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_15000= ELM_MultiLabel(input_nodes=768, hidden_nodes=15000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 15000 hidden nodes\")\n",
    "t1_emlMLTC_15000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_20000=ELM_MultiLabel(input_nodes=768, hidden_nodes=20000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 20000 hidden nodes\")\n",
    "t1_emlMLTC_20000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "source": [
    "Doing Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_model_list={\"t1_emlMLTC_500\":t1_emlMLTC_500,\n",
    "\"t1_emlMLTC_1000\":t1_emlMLTC_1000,\n",
    "\"t1_emlMLTC_2000\":t1_emlMLTC_2000,\n",
    "\"t1_emlMLTC_3000\":t1_emlMLTC_3000,\n",
    "\"t1_emlMLTC_4000\":t1_emlMLTC_4000,\n",
    "\"t1_emlMLTC_5000\":t1_emlMLTC_5000,\n",
    "\"t1_emlMLTC_10000\":t1_emlMLTC_10000,\n",
    "\"t1_emlMLTC_15000\":t1_emlMLTC_15000,\n",
    "\"t1_emlMLTC_20000\":t1_emlMLTC_20000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_train=time.time()\n",
    "time_list_test=[start_time_train]\n",
    "\n",
    "for x in t1_model_list.keys():\n",
    "    print(\"For \"+ x)\n",
    "    t1_model_list[x].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_list_test.append(time.time())\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "display_name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "870dc015ab544cf4fd4c91e2f8042f40253e24aac5eeb5e28336055566efd434"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}