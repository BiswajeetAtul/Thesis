{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "main_start=time.time()\n",
    "time_log={\"main_start\":main_start}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         imdb_id                                          title  \\\n",
       "0      tt0057603                        I tre volti della paura   \n",
       "1      tt1733125  Dungeons & Dragons: The Book of Vile Darkness   \n",
       "2      tt0033045                     The Shop Around the Corner   \n",
       "3      tt0113862                             Mr. Holland's Opus   \n",
       "4      tt0086250                                       Scarface   \n",
       "...          ...                                            ...   \n",
       "14823  tt0219952                                  Lucky Numbers   \n",
       "14824  tt1371159                                     Iron Man 2   \n",
       "14825  tt0063443                                     Play Dirty   \n",
       "14826  tt0039464                                      High Wall   \n",
       "14827  tt0235166                               Against All Hope   \n",
       "\n",
       "                                           plot_synopsis  \\\n",
       "0      Note: this synopsis is for the orginal Italian...   \n",
       "1      Two thousand years ago, Nhagruul the Foul, a s...   \n",
       "2      Matuschek's, a gift store in Budapest, is the ...   \n",
       "3      Glenn Holland, not a morning person by anyone'...   \n",
       "4      In May 1980, a Cuban man named Tony Montana (A...   \n",
       "...                                                  ...   \n",
       "14823  In 1988 Russ Richards (John Travolta), the wea...   \n",
       "14824  In Russia, the media covers Tony Stark's discl...   \n",
       "14825  During the North African Campaign in World War...   \n",
       "14826  Steven Kenet catches his unfaithful wife in th...   \n",
       "14827  Sometime in the 1950s in Chicago a man, Cecil ...   \n",
       "\n",
       "                                                    tags  split  \\\n",
       "0              cult, horror, gothic, murder, atmospheric  train   \n",
       "1                                               violence  train   \n",
       "2                                               romantic   test   \n",
       "3                 inspiring, romantic, stupid, feel-good  train   \n",
       "4      cruelty, murder, dramatic, cult, violence, atm...    val   \n",
       "...                                                  ...    ...   \n",
       "14823                                     comedy, murder   test   \n",
       "14824                         good versus evil, violence  train   \n",
       "14825                                           anti war  train   \n",
       "14826                                             murder   test   \n",
       "14827                                     christian film   test   \n",
       "\n",
       "      synopsis_source  \n",
       "0                imdb  \n",
       "1                imdb  \n",
       "2                imdb  \n",
       "3                imdb  \n",
       "4                imdb  \n",
       "...               ...  \n",
       "14823       wikipedia  \n",
       "14824       wikipedia  \n",
       "14825       wikipedia  \n",
       "14826       wikipedia  \n",
       "14827       wikipedia  \n",
       "\n",
       "[14828 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imdb_id</th>\n      <th>title</th>\n      <th>plot_synopsis</th>\n      <th>tags</th>\n      <th>split</th>\n      <th>synopsis_source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>tt0057603</td>\n      <td>I tre volti della paura</td>\n      <td>Note: this synopsis is for the orginal Italian...</td>\n      <td>cult, horror, gothic, murder, atmospheric</td>\n      <td>train</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tt1733125</td>\n      <td>Dungeons &amp; Dragons: The Book of Vile Darkness</td>\n      <td>Two thousand years ago, Nhagruul the Foul, a s...</td>\n      <td>violence</td>\n      <td>train</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tt0033045</td>\n      <td>The Shop Around the Corner</td>\n      <td>Matuschek's, a gift store in Budapest, is the ...</td>\n      <td>romantic</td>\n      <td>test</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tt0113862</td>\n      <td>Mr. Holland's Opus</td>\n      <td>Glenn Holland, not a morning person by anyone'...</td>\n      <td>inspiring, romantic, stupid, feel-good</td>\n      <td>train</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>tt0086250</td>\n      <td>Scarface</td>\n      <td>In May 1980, a Cuban man named Tony Montana (A...</td>\n      <td>cruelty, murder, dramatic, cult, violence, atm...</td>\n      <td>val</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14823</th>\n      <td>tt0219952</td>\n      <td>Lucky Numbers</td>\n      <td>In 1988 Russ Richards (John Travolta), the wea...</td>\n      <td>comedy, murder</td>\n      <td>test</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14824</th>\n      <td>tt1371159</td>\n      <td>Iron Man 2</td>\n      <td>In Russia, the media covers Tony Stark's discl...</td>\n      <td>good versus evil, violence</td>\n      <td>train</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14825</th>\n      <td>tt0063443</td>\n      <td>Play Dirty</td>\n      <td>During the North African Campaign in World War...</td>\n      <td>anti war</td>\n      <td>train</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14826</th>\n      <td>tt0039464</td>\n      <td>High Wall</td>\n      <td>Steven Kenet catches his unfaithful wife in th...</td>\n      <td>murder</td>\n      <td>test</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14827</th>\n      <td>tt0235166</td>\n      <td>Against All Hope</td>\n      <td>Sometime in the 1950s in Chicago a man, Cecil ...</td>\n      <td>christian film</td>\n      <td>test</td>\n      <td>wikipedia</td>\n    </tr>\n  </tbody>\n</table>\n<p>14828 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "mpstDF= pd.read_csv(\"mpst.csv\")\n",
    "mpstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape Train (9489, 6)\nShape Validation (2373, 6)\nShape Test (2966, 6)\n"
     ]
    }
   ],
   "source": [
    "final_mpstDF_train=mpstDF[mpstDF[\"split\"]==\"train\"]\n",
    "final_mpstDF_validation=mpstDF[mpstDF[\"split\"]==\"val\"]\n",
    "final_mpstDF_test=mpstDF[mpstDF[\"split\"]==\"test\"]\n",
    "print(\"Shape Train\",final_mpstDF_train.shape)\n",
    "print(\"Shape Validation\",final_mpstDF_validation.shape)\n",
    "print(\"Shape Test\",final_mpstDF_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Int64Index([    0,     1,     3,     6,     7,     8,     9,    10,    11,\n",
       "               12,\n",
       "            ...\n",
       "            14810, 14811, 14813, 14815, 14817, 14818, 14820, 14822, 14824,\n",
       "            14825],\n",
       "           dtype='int64', length=9489)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "final_mpstDF_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Int64Index([    4,     5,    18,    20,    26,    32,    40,    48,    53,\n",
       "               65,\n",
       "            ...\n",
       "            14767, 14769, 14772, 14777, 14779, 14789, 14795, 14796, 14812,\n",
       "            14821],\n",
       "           dtype='int64', length=2373)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "final_mpstDF_validation.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Int64Index([    2,    15,    19,    24,    27,    31,    35,    37,    41,\n",
       "               42,\n",
       "            ...\n",
       "            14776, 14801, 14806, 14808, 14814, 14816, 14819, 14823, 14826,\n",
       "            14827],\n",
       "           dtype='int64', length=2966)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "final_mpstDF_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING THE BERT EMBEDDINGS AND Y MATRIX\n",
    "bert_embedding=np.load(\"D:\\CodeRepo\\Thesis\\Thesis\\BERT\\embeddings.npz\")\n",
    "label_values=np.load(\"D:\\CodeRepo\\Thesis\\Thesis\\EDA\\Y.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "type1_BERT_Embeddings=bert_embedding[\"t1\"]\n",
    "type2_BERT_Embeddings=bert_embedding[\"t2\"]\n",
    "label_values=label_values[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(14828, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "type1_BERT_Embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(14828, 71)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "label_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_Embeddings(x_t1,x_t2,y,df,partition_nm):\n",
    "    _df=df[df[\"split\"]==partition_nm]\n",
    "    index_list=list(_df.index)\n",
    "    temp_array_x_t1=[]\n",
    "    temp_array_x_t2=[]\n",
    "    temp_array_y=[]\n",
    "    for index in index_list:\n",
    "        temp_array_x_t1.append(x_t1[index,:])\n",
    "        temp_array_x_t2.append(x_t2[index,:])\n",
    "        temp_array_y.append(y[index,:])\n",
    "    temp_array_x_t1=np.array(temp_array_x_t1)\n",
    "    temp_array_x_t2=np.array(temp_array_x_t2)\n",
    "    temp_array_y=np.array(temp_array_y)\n",
    "    return temp_array_x_t1,temp_array_x_t2, temp_array_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TRAIN\n",
    "type1_BERT_Embeddings_Train,type2_BERT_Embeddings_Train,label_values_Train=get_partition_Embeddings(type1_BERT_Embeddings,type2_BERT_Embeddings,label_values,mpstDF,\"train\")\n",
    "# FOR VALIDATION\n",
    "type1_BERT_Embeddings_Val,type2_BERT_Embeddings_Val,label_values_Val=get_partition_Embeddings(type1_BERT_Embeddings,type2_BERT_Embeddings,label_values,mpstDF,\"val\")\n",
    "# FOR TEST\n",
    "type1_BERT_Embeddings_Test,type2_BERT_Embeddings_Test,label_values_Test=get_partition_Embeddings(type1_BERT_Embeddings,type2_BERT_Embeddings,label_values,mpstDF,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9489, 768)\n(9489, 768)\n(2373, 768)\n(2373, 768)\n(2966, 768)\n(2966, 768)\n(9489, 71)\n(2373, 71)\n(2966, 71)\n"
     ]
    }
   ],
   "source": [
    "# SEPARTAING THE TRAIN TEST AND VALIDATION ROWS\n",
    "print(type1_BERT_Embeddings_Train.shape)\n",
    "print(type2_BERT_Embeddings_Train.shape)\n",
    "print(type1_BERT_Embeddings_Val.shape)\n",
    "print(type2_BERT_Embeddings_Val.shape)\n",
    "print(type1_BERT_Embeddings_Test.shape)\n",
    "print(type2_BERT_Embeddings_Test.shape)\n",
    "print(label_values_Train.shape)\n",
    "print(label_values_Val.shape)\n",
    "print(label_values_Test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix, hamming_loss,classification_report\n",
    "\n",
    "# Activation Functions\n",
    "\n",
    "# def _identity(x):\n",
    "#     return x\n",
    "# def _binary_step(x, threshold = 0):\n",
    "#     return 1 if x=threshold else 0\n",
    "# def _biploar_step(x ,threshold = 0):\n",
    "#     return 1 if x>=threshold else -1\n",
    "# def _binary_sigmoid(x):\n",
    "#     return 1. / (1. + np.exp(-x))\n",
    "# def _bipolar_sigmoid(x):\n",
    "#     return (1. - np.exp(-x))/(1. + np.exp(-x))\n",
    "# def _relu_function(x):\n",
    "#     return np.max(0, x)\n",
    "# def _relu_leaky(x):\n",
    "#     return np.max(0.01*x, x)\n",
    "\n",
    "\n",
    "_identity =np.vectorize(lambda x: x)\n",
    "_binary_step =np.vectorize(lambda x,t=0: 1 if x>t else 0)\n",
    "_biploar_step =np.vectorize(lambda x,t=0: 1 if x>t else -1)\n",
    "_binary_sigmoid=np.vectorize(lambda x: 1. / (1. + np.exp(-x)))\n",
    "_bipolar_sigmoid=np.vectorize(lambda x: (1. - np.exp(-x))/(1. + np.exp(-x)))\n",
    "_relu_function=np.vectorize(lambda x: np.max([0, x]))\n",
    "_relu_leaky=np.vectorize(lambda x: np.max([0.01*x, x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ELM_MultiLabel:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, activation, bias=True, random_gen=\"uniform\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_nodes ([integer]): Number of Input nodes\n",
    "            hidden_nodes ([integer]): Number of hidden nodes\n",
    "            output_nodes ([integer]): Number of output nodes\n",
    "            activation ([function]): The function which will be used as the activation function in the hidden layer\n",
    "            bias ([boolean]): Flag to use bias, if True then randomly generate bias @random_gen else bias - 0.\n",
    "            random_gen (str, optional): The type way in which random weight are generated. Defaults to \"uniform\".\n",
    "        \"\"\"\n",
    "        self.__input_nodes = input_nodes\n",
    "        self.__hidden_nodes = hidden_nodes\n",
    "        self.__output_nodes = output_nodes\n",
    "        if random_gen == \"uniform\":\n",
    "            self.__beta = np.random.uniform(-1.,1.,size = (self.__hidden_nodes, self.__output_nodes))\n",
    "            self.__alpha = np.random.uniform(-1.,1.,size = (self.__input_nodes, self.__hidden_nodes))\n",
    "            self.__bias = np.random.uniform(size = (self.__hidden_nodes,))\n",
    "        else:\n",
    "            self.__beta = np.random.normal(-1.,1.,size=(self.__hidden_nodes, self.__output_nodes))\n",
    "            self.__alpha = np.random.normal(-1.,1.,size=(self.__input_nodes, self.__hidden_nodes))\n",
    "            self.__bias = np.random.normal(size=(self.__n_hidden_nodes,)) \n",
    "        self.__activation = activation # Sigmoid Function\n",
    "\n",
    "    def getInputNodes(self):\n",
    "        return  self.__input_nodes\n",
    "\n",
    "    def getHiddenNodes(self):\n",
    "        return  self.__hidden_nodes\n",
    "\n",
    "    def getOutputNodes(self):\n",
    "        return  self.__output_nodes\n",
    "    \n",
    "    def getBetaWeights(self):\n",
    "        return self.__beta\n",
    "    \n",
    "    def getAlphaWeight(self):\n",
    "        return self.__alphs\n",
    "    \n",
    "    def getBias(self):\n",
    "        return self.__bias\n",
    "\n",
    "    def __get_H_matrix(self, train_x, verbose=False):\n",
    "        # 1 Propagate data from Input to hidden Layer\n",
    "        if verbose:\n",
    "            print(\"Propagate data from Input to hidden Layer\")\n",
    "        inp = np.dot(train_x , self.__alpha)\n",
    "        if verbose:\n",
    "            print(inp)\n",
    "            print(\"Adding Biases\")\n",
    "        inp = inp  + self.__bias\n",
    "        if verbose:\n",
    "            print(inp)\n",
    "            print(\"Applyin activation function\")\n",
    "        inp_activation = np.apply_along_axis(self.__activation, 1, inp)\n",
    "        return inp_activation\n",
    "\n",
    "    def fit(self, train_x, train_y, verbose = False, show_metrics = True):\n",
    "        \"\"\"\n",
    "        This function calculates the Beta weights or the output weights\n",
    "        train_x : input matrix\n",
    "        train_y : output matrix to be predicted or learned upon unipolar\n",
    "\n",
    "        returns: if test_y is not given then\n",
    "                returns the predicted output\n",
    "                if test_y is given then returns predicted output and evaluation metrics dict \n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"train_x shape:\", train_x.shape)\n",
    "            print(\"train_y shape:\", train_y.shape)\n",
    "        inp_activation = self.__get_H_matrix(train_x, verbose)\n",
    "        # This is the H matrix getting its Moore Penrose Inverse\n",
    "        if verbose:\n",
    "            print(inp_activation)\n",
    "            print(\"Getting the Generalized Moore Penrose Inverse\")\n",
    "        generalizedInverse = np.linalg.pinv(inp_activation)\n",
    "        if verbose:\n",
    "            print(generalizedInverse)\n",
    "            print(\"Finding Beta, output weights\")\n",
    "        # Now find output weight matrix Beta \n",
    "        # convert input Y values according to the threshold using biploar step function\n",
    "        predicted_bipolar=  np.apply_along_axis(_biploar_step, 1, train_y)\n",
    "        self.__beta = np.dot(generalizedInverse, predicted_bipolar)\n",
    "        if verbose:\n",
    "            print(\"Beta Matrix Weights\")\n",
    "            print(self.__beta)\n",
    "\n",
    "        print(\"Model Metrics, for Training :\")\n",
    "        return self.predict(train_x, train_y,verbose,show_metrics)\n",
    "    \n",
    "    def predict(self, test_x, test_y = None, verbose = False, show_metrics= True):\n",
    "        \"\"\"\n",
    "        preditcts the output for the input test data\n",
    "        call this after calling the fit.\n",
    "        test_data shape should be (batch_size,768 or input_nodes)\n",
    "        output_shape will be (batch_size, 71 or output_nodes)\n",
    "\n",
    "        returns: if test_y is not given then\n",
    "                returns the predicted output\n",
    "                if test_y is given then returns predicted output and evaluation metrics dict\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"Predicting outputs\")\n",
    "        inp_activation = self.__get_H_matrix(test_x, verbose)\n",
    "        output_predicted = np.dot(inp_activation, self.__beta)\n",
    "        # convert predicted according to the threshold using biploar step function\n",
    "        predicted_bipolar =  np.apply_along_axis(_biploar_step, 1, output_predicted)\n",
    "        predicted_binary = np.apply_along_axis(_binary_step, 1, predicted_bipolar)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"predicted output\")\n",
    "            print(output_predicted)\n",
    "            print(\"predicted_bipolar\")\n",
    "            print(predicted_bipolar)\n",
    "            print(\"predicted_binary\")\n",
    "            print(predicted_binary)\n",
    "            print(\"Original Binary\")\n",
    "            print(test_y)\n",
    "\n",
    "        eval_dict={}\n",
    "        if (test_y is not None):\n",
    "            eval_dict=self.__evaluate(test_y,predicted_binary, for_test=False)\n",
    "        if(test_y is not None):\n",
    "            return predicted_binary, eval_dict\n",
    "        else:\n",
    "            return predicted_binary\n",
    "\n",
    "    def __evaluate(self, real, predicted, for_test=True):\n",
    "        \"\"\"\n",
    "        real values as 0,1\n",
    "        predicted values as 0,1\n",
    "        \"\"\"\n",
    "        # Now we find accuracy, precision, recall, Hamming Loss and F1 Measure\n",
    "        accuracy = accuracy_score(real, predicted)\n",
    "        hamLoss = hamming_loss(real, predicted)\n",
    "        # element wise correctness\n",
    "        term_wise_accuracy=np.sum(np.logical_not(np.logical_xor(real, predicted)))/real.size\n",
    "\n",
    "        macro_precision = precision_score(real, predicted, average='macro')\n",
    "        macro_recall = recall_score(real, predicted, average='macro')\n",
    "        macro_f1 = f1_score(real, predicted, average='macro')\n",
    "\n",
    "        micro_precision = precision_score(real, predicted, average='micro')\n",
    "        micro_recall = recall_score(real, predicted, average='micro')\n",
    "        micro_f1 = f1_score(real, predicted, average='micro')\n",
    "        \n",
    "        metricTable=prettytable.PrettyTable()\n",
    "        metricTable.field_names = [\"Metric\", \"Macro Value\", \"Micro Value\"]\n",
    "        metricTable.add_row([\"Hamming Loss\",\"{0:.3f}\".format(hamLoss) ,\"\"])\n",
    "        metricTable.add_row([\"Term Wise Accuracy\",\"{0:.3f}\".format(term_wise_accuracy) ,\"\"])\n",
    "\n",
    "        metricTable.add_row([\"Accuracy\",\"{0:.3f}\".format(accuracy),\"\"])\n",
    "        metricTable.add_row([\"Precision\",\"{0:.3f}\".format(macro_precision),\"{0:.3f}\".format(micro_precision)])\n",
    "        metricTable.add_row([\"Recall\",\"{0:.3f}\".format(macro_recall),\"{0:.3f}\".format(micro_recall)])\n",
    "        metricTable.add_row([\"F1-measure\",\"{0:.3f}\".format(macro_f1),\"{0:.3f}\".format(micro_f1)])\n",
    "\n",
    "        print(metricTable)\n",
    "\n",
    "        print(\"Metrics @ Literature\")\n",
    "        lit_HamminLosss, lit_accuracy, lit_precision, lit_recall, lit_f1 = self.get_eval_metrics(real,predicted)\n",
    "\n",
    "        return_dict = {\"HiddenNodes\": self.getHiddenNodes(),\n",
    "                \"lit_HamminLosss\": lit_HamminLosss,\n",
    "                \"lit_accuracy\": lit_accuracy,\n",
    "                \"lit_precision\": lit_precision,\n",
    "                \"lit_recall\": lit_recall,\n",
    "                \"lit_f1\": lit_f1,\n",
    "                \"sklearn_hamLoss\": hamLoss,\n",
    "                \"sklearn_accuracy\": accuracy,\n",
    "                \"term_wise_accuracy\": term_wise_accuracy,\n",
    "                \"sklearn_macro_precision\": macro_precision,\n",
    "                \"sklearn_micro_precision\": micro_precision,\n",
    "                \"sklearn_macro_recall\": macro_recall,\n",
    "                \"sklearn_micro_recall\": micro_recall,\n",
    "                \"sklearn_macro_f1\": macro_f1,\n",
    "                \"sklearn_micro_f1\": micro_f1,\n",
    "                }\n",
    "\n",
    "\n",
    "        # print(\"Test Classification Report\")\n",
    "        # print(classification_report(real,predicted))\n",
    "\n",
    "        return return_dict\n",
    "\n",
    "    def get_eval_metrics(self, real, predicted, verbose= False):\n",
    "        err_cnt_accuracy=0\n",
    "        err_cnt_precision=0\n",
    "        err_cnt_recall=0\n",
    "        if verbose:\n",
    "            print(real)\n",
    "            print(predicted)\n",
    "        for x in range(real.shape[0]):\n",
    "            err_and= np.logical_and(real[x],predicted[x])\n",
    "            err_or = np.logical_or(real[x],predicted[x])\n",
    "            # Accuracy\n",
    "            err_cnt_accuracy +=(sum(err_and)/sum(err_or))\n",
    "\n",
    "            # Precision\n",
    "            if sum(err_and) != 0:\n",
    "                err_cnt_precision += (sum(err_and) / sum(predicted[x]))\n",
    "            # Recall\n",
    "            err_cnt_recall += (sum(err_and) / sum(real[x]))\n",
    "            if verbose:\n",
    "                print(\"Iteration :\",x)\n",
    "                print((sum(err_and)/sum(err_or)))\n",
    "                print(err_and)\n",
    "                print(err_or)\n",
    "        \n",
    "        err_count_hamming = np.zeros((real.shape))\n",
    "\n",
    "        for i in range(real.shape[0]):\n",
    "            for j in range(real.shape[1]):\n",
    "                if real[i,j] != predicted[i,j]:\n",
    "                    err_count_hamming[1,j] = err_count_hamming[1,j]+1\n",
    "\n",
    "        sum_err = np.sum(err_count_hamming);\n",
    "        HammingLoss = sum_err/real.size;\n",
    "        accuracy = err_cnt_accuracy / real.shape[0]\n",
    "        precision = err_cnt_precision / real.shape[0]\n",
    "        recall = err_cnt_recall / real.shape[0]\n",
    "        f1 = 2*((precision*recall)/(precision+recall))\n",
    "        if verbose:\n",
    "            print(\"Final: \")\n",
    "            print(\"Hamming Loss: \", HammingLoss)\n",
    "            print(\"Accuracy: \",accuracy)\n",
    "            print(\"precision: \",precision)\n",
    "            print(\"recall: \",recall)\n",
    "            print(\"f1: \",f1)\n",
    "\n",
    "        metricTable=prettytable.PrettyTable()\n",
    "        metricTable.field_names = [\"Metric\", \"Value\"]\n",
    "        metricTable.add_row([\" Literature Hamming Loss\",\"{0:.3f}\".format(HammingLoss)])\n",
    "        metricTable.add_row([\"Literature Accuracy\",\"{0:.3f}\".format(accuracy)])\n",
    "\n",
    "        metricTable.add_row([\"Literature Precision\",\"{0:.3f}\".format(precision)])\n",
    "        metricTable.add_row([\"LiteratureRecall\",\"{0:.3f}\".format(recall)])\n",
    "        metricTable.add_row([\"LiteratureF1-measure\",\"{0:.3f}\".format(f1)])\n",
    "\n",
    "        print(metricTable)\n",
    "\n",
    "        return HammingLoss,accuracy,precision,recall,f1\n"
   ]
  },
  {
   "source": [
    "Now The preprocessing and is done. We will save the time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a object to store the mertics for each model created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_log[\"End preprocessing\"]=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAINING \n",
      "\n",
      "Training Model with 100 hidden nodes\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.041    |             |\n",
      "| Term Wise Accuracy |    0.959    |             |\n",
      "|      Accuracy      |    0.025    |             |\n",
      "|     Precision      |    0.031    |    0.586    |\n",
      "|       Recall       |    0.008    |    0.067    |\n",
      "|     F1-measure     |    0.011    |    0.121    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.041 |\n",
      "|   Literature Accuracy    | 0.072 |\n",
      "|   Literature Precision   | 0.153 |\n",
      "|     LiteratureRecall     | 0.080 |\n",
      "|   LiteratureF1-measure   | 0.106 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.00      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.00      0.00      0.00      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.59      0.36      0.44      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.69      0.01      0.02      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.00      0.00      0.00      1556\n",
      "          57       0.33      0.00      0.00      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.57      0.20      0.29      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.59      0.07      0.12     28022\n",
      "   macro avg       0.03      0.01      0.01     28022\n",
      "weighted avg       0.19      0.07      0.09     28022\n",
      " samples avg       0.15      0.08      0.09     28022\n",
      "\n",
      "TESTING \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.024    |             |\n",
      "|     Precision      |    0.032    |    0.551    |\n",
      "|       Recall       |    0.008    |    0.065    |\n",
      "|     F1-measure     |    0.010    |    0.116    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.068 |\n",
      "|   Literature Precision   | 0.150 |\n",
      "|     LiteratureRecall     | 0.078 |\n",
      "|   LiteratureF1-measure   | 0.102 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.57      0.36      0.44      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.50      0.00      0.01       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.67      0.00      0.01       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.51      0.19      0.27       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.55      0.07      0.12      9022\n",
      "   macro avg       0.03      0.01      0.01      9022\n",
      "weighted avg       0.19      0.07      0.08      9022\n",
      " samples avg       0.15      0.08      0.09      9022\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]),\n",
       " {'HiddenNodes': 50,\n",
       "  'lit_HamminLosss': 0.042329499586867124,\n",
       "  'lit_accuracy': 0.06806405373325566,\n",
       "  'lit_precision': 0.14969656102494944,\n",
       "  'lit_recall': 0.07792499709049033,\n",
       "  'lit_f1': 0.1024955999678164,\n",
       "  'sklearn_hamLoss': 0.06806405373325566,\n",
       "  'sklearn_accuracy': 0.023937963587322995,\n",
       "  'sklearn_macro_precision': 0.03163180980082388,\n",
       "  'sklearn_micro_precision-': 0.06806405373325566,\n",
       "  'sklearn_macro_recall': 0.007760401154627529,\n",
       "  'sklearn_micro_precision': 0.550656660412758,\n",
       "  'sklearn_macro_f1': 0.010209199808826468,\n",
       "  'sklearn_micro_f1': 0.11637589214908803,\n",
       "  'term_wise_accuracy': 0.9576705004131328})"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# doing a test for ELM model with 100 nodes\n",
    "#TRAIN\n",
    "print(\"TRAINING \\n\")\n",
    "t1_emlMLTC_50= ELM_MultiLabel(input_nodes=768, hidden_nodes=50, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 100 hidden nodes\")\n",
    "t1_emlMLTC_50.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "\n",
    "#TEST\n",
    "print(\"TESTING \\n\")\n",
    "t1_emlMLTC_50.predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n"
   ]
  },
  {
   "source": [
    "Now We will run the model for all the three types data sets we have viz.\n",
    "- TRAIN X\n",
    "  - type1_BERT_Embeddings_Train\n",
    "  - type2_BERT_Embeddings_Train\n",
    "- VALIDATION X\n",
    "  - type1_BERT_Embeddings_Val\n",
    "  - type2_BERT_Embeddings_Val\n",
    "- TEST X\n",
    "  - type1_BERT_Embeddings_Test\n",
    "  - type2_BERT_Embeddings_Test\n",
    "- TRAIN Y\n",
    "  - label_values_Train.shape\n",
    "- VALIDATION Y\n",
    "  - label_values_Val.shape\n",
    "- TEST Y\n",
    "  - label_values_Test.shape"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models_hidden_nodes=[100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000, 10000, 15000, 20000]\n",
    "t1_models={}\n",
    "t2_models={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_list=[]\n",
    "\n",
    "def add_data_to_metric_list(eval_dict, activation, type, start, phase, end, metrics_dict_list=metrics_dict_list):\n",
    "    eval_dict[\"activation\"]=activation\n",
    "    eval_dict[\"type\"]=type\n",
    "    eval_dict[\"start\"]=start\n",
    "    eval_dict[\"phase\"]=phase\n",
    "    eval_dict[\"end\"]=end\n",
    "    eval_dict[\"total_time\"]=end-start\n",
    "\n",
    "    metrics_dict_list.append(eval_dict)\n"
   ]
  },
  {
   "source": [
    "Testing the above function with a simple 50 hidden layer node model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t1_50_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.041    |             |\n",
      "| Term Wise Accuracy |    0.959    |             |\n",
      "|      Accuracy      |    0.025    |             |\n",
      "|     Precision      |    0.057    |    0.584    |\n",
      "|       Recall       |    0.008    |    0.070    |\n",
      "|     F1-measure     |    0.011    |    0.125    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.041 |\n",
      "|   Literature Accuracy    | 0.074 |\n",
      "|   Literature Precision   | 0.164 |\n",
      "|     LiteratureRecall     | 0.082 |\n",
      "|   LiteratureF1-measure   | 0.110 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.00      0.01       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.17      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.22      0.00      0.00      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.33      0.00      0.01       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.60      0.39      0.47      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.67      0.00      0.00      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.00      0.00      0.00      1556\n",
      "          57       0.53      0.01      0.02      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.56      0.17      0.27      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.58      0.07      0.13     28022\n",
      "   macro avg       0.06      0.01      0.01     28022\n",
      "weighted avg       0.23      0.07      0.09     28022\n",
      " samples avg       0.16      0.08      0.10     28022\n",
      "\n",
      "t1_50_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.024    |             |\n",
      "|     Precision      |    0.028    |    0.557    |\n",
      "|       Recall       |    0.008    |    0.069    |\n",
      "|     F1-measure     |    0.011    |    0.122    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.073 |\n",
      "|   Literature Precision   | 0.162 |\n",
      "|     LiteratureRecall     | 0.082 |\n",
      "|   LiteratureF1-measure   | 0.109 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.33      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.57      0.38      0.46      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.00      0.00      0.00       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.59      0.02      0.03       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.53      0.18      0.27       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.56      0.07      0.12      9022\n",
      "   macro avg       0.03      0.01      0.01      9022\n",
      "weighted avg       0.19      0.07      0.09      9022\n",
      " samples avg       0.16      0.08      0.09      9022\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'HiddenNodes': 50,\n",
       "  'lit_HamminLosss': 0.040752895495006075,\n",
       "  'lit_accuracy': 0.07442027059781058,\n",
       "  'lit_precision': 0.16423402536270065,\n",
       "  'lit_recall': 0.0822539101260439,\n",
       "  'lit_f1': 0.10961096927552347,\n",
       "  'sklearn_hamLoss': 0.07442027059781058,\n",
       "  'sklearn_accuracy': 0.025292443882390134,\n",
       "  'sklearn_macro_precision': 0.05729102025975336,\n",
       "  'sklearn_micro_precision-': 0.07442027059781058,\n",
       "  'sklearn_macro_recall': 0.008273497103546383,\n",
       "  'sklearn_micro_precision': 0.5841760856632957,\n",
       "  'sklearn_macro_f1': 0.011012448680238561,\n",
       "  'sklearn_micro_f1': 0.12515931684934997,\n",
       "  'term_wise_accuracy': 0.959247104504994,\n",
       "  'activation': '_bipolar_sigmoid',\n",
       "  'type': 'type1_BERT_Embeddings_Train',\n",
       "  'start': 1603016131.3887482,\n",
       "  'phase': 'train',\n",
       "  'end': 1603016141.7192724},\n",
       " {'HiddenNodes': 50,\n",
       "  'lit_HamminLosss': 0.042244023819247245,\n",
       "  'lit_accuracy': 0.07251667512937399,\n",
       "  'lit_precision': 0.16205888963812093,\n",
       "  'lit_recall': 0.08174585532955894,\n",
       "  'lit_f1': 0.10867419786258035,\n",
       "  'sklearn_hamLoss': 0.07251667512937399,\n",
       "  'sklearn_accuracy': 0.024275118004045852,\n",
       "  'sklearn_macro_precision': 0.028466079078010343,\n",
       "  'sklearn_micro_precision-': 0.07251667512937399,\n",
       "  'sklearn_macro_recall': 0.00821992892590464,\n",
       "  'sklearn_micro_precision': 0.5566546762589928,\n",
       "  'sklearn_macro_f1': 0.01078866183558055,\n",
       "  'sklearn_micro_f1': 0.1221630155910795,\n",
       "  'term_wise_accuracy': 0.9577559761807528,\n",
       "  'activation': '_bipolar_sigmoid',\n",
       "  'type': 'type1_BERT_Embeddings_Test',\n",
       "  'start': 1603016141.7202702,\n",
       "  'phase': 'test',\n",
       "  'end': 1603016144.4399993}]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "print( \"t1_\"   +str(50)+\"_bipolar_sigmoid_train_start \\n\")\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_start\"]=time.time()\n",
    "t1_models[\"t1_\"+str(50)+\"_bipolar_sigmoid\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=50, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "predicted, eval_dict=t1_models[\"t1_\"+str(50)+\"_bipolar_sigmoid\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_end\"]=time.time()\n",
    "\n",
    "add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_start\"], \"train\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_end\"])\n",
    "\n",
    "print( \"t1_\"   +str(50)+\"_bipolar_sigmoid_test_start \\n\")\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_start\"]=time.time()\n",
    "predicted, eval_dict=t1_models[\"t1_\"+str(50)+\"_bipolar_sigmoid\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_end\"]=time.time()\n",
    "\n",
    "add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_start\"], \"test\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_end\"])\n",
    "\n",
    "metrics_dict_list"
   ]
  },
  {
   "source": [
    "TRAINING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t1_100_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.032    |             |\n",
      "|     Precision      |    0.052    |    0.605    |\n",
      "|       Recall       |    0.010    |    0.082    |\n",
      "|     F1-measure     |    0.013    |    0.144    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.087 |\n",
      "|   Literature Precision   | 0.188 |\n",
      "|     LiteratureRecall     | 0.096 |\n",
      "|   LiteratureF1-measure   | 0.127 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.50      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.60      0.00      0.00      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.62      0.42      0.50      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.82      0.03      0.05      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.00      0.00      0.00      1556\n",
      "          57       0.56      0.01      0.02      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.57      0.24      0.34      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.61      0.08      0.14     28022\n",
      "   macro avg       0.05      0.01      0.01     28022\n",
      "weighted avg       0.28      0.08      0.10     28022\n",
      " samples avg       0.19      0.10      0.11     28022\n",
      "\n",
      "t1_100_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.039    |             |\n",
      "|     Precision      |    0.093    |    0.622    |\n",
      "|       Recall       |    0.011    |    0.087    |\n",
      "|     F1-measure     |    0.015    |    0.153    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.097 |\n",
      "|   Literature Precision   | 0.199 |\n",
      "|     LiteratureRecall     | 0.107 |\n",
      "|   LiteratureF1-measure   | 0.139 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       1.00      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       1.00      0.00      0.00      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.63      0.43      0.51      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.73      0.05      0.10      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       1.00      0.00      0.00      1556\n",
      "          57       0.62      0.03      0.06      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.60      0.26      0.36      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.62      0.09      0.15     28022\n",
      "   macro avg       0.09      0.01      0.01     28022\n",
      "weighted avg       0.41      0.09      0.11     28022\n",
      " samples avg       0.20      0.11      0.12     28022\n",
      "\n",
      "t1_100_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.041    |             |\n",
      "| Term Wise Accuracy |    0.959    |             |\n",
      "|      Accuracy      |    0.029    |             |\n",
      "|     Precision      |    0.125    |    0.606    |\n",
      "|       Recall       |    0.009    |    0.071    |\n",
      "|     F1-measure     |    0.013    |    0.126    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.041 |\n",
      "|   Literature Accuracy    | 0.077 |\n",
      "|   Literature Precision   | 0.165 |\n",
      "|     LiteratureRecall     | 0.085 |\n",
      "|   LiteratureF1-measure   | 0.112 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       1.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.50      0.01      0.03        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.33      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.50      0.00      0.01       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.60      0.00      0.00      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.50      0.01      0.01       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.61      0.37      0.46      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.73      0.01      0.02      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.50      0.01      0.01       140\n",
      "          56       1.00      0.00      0.00      1556\n",
      "          57       0.53      0.02      0.04      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.01       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.50      0.00      0.01       370\n",
      "          68       0.59      0.19      0.29      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.61      0.07      0.13     28022\n",
      "   macro avg       0.13      0.01      0.01     28022\n",
      "weighted avg       0.38      0.07      0.09     28022\n",
      " samples avg       0.17      0.08      0.10     28022\n",
      "\n",
      "t2_100_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.037    |             |\n",
      "|     Precision      |    0.086    |    0.629    |\n",
      "|       Recall       |    0.011    |    0.086    |\n",
      "|     F1-measure     |    0.014    |    0.151    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.095 |\n",
      "|   Literature Precision   | 0.200 |\n",
      "|     LiteratureRecall     | 0.104 |\n",
      "|   LiteratureF1-measure   | 0.137 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       1.00      0.01      0.03        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.00      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.50      0.00      0.00      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.64      0.44      0.52      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.73      0.03      0.05      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       1.00      0.00      0.00      1556\n",
      "          57       0.64      0.02      0.04      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       1.00      0.01      0.02       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.61      0.26      0.36      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.63      0.09      0.15     28022\n",
      "   macro avg       0.09      0.01      0.01     28022\n",
      "weighted avg       0.31      0.09      0.11     28022\n",
      " samples avg       0.20      0.10      0.12     28022\n",
      "\n",
      "t2_100_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.037    |             |\n",
      "|     Precision      |    0.176    |    0.623    |\n",
      "|       Recall       |    0.011    |    0.087    |\n",
      "|     F1-measure     |    0.015    |    0.153    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.097 |\n",
      "|   Literature Precision   | 0.202 |\n",
      "|     LiteratureRecall     | 0.107 |\n",
      "|   LiteratureF1-measure   | 0.140 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.01       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       1.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       1.00      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.71      0.00      0.01      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       1.00      0.00      0.01       298\n",
      "          43       0.64      0.44      0.52      3697\n",
      "          44       1.00      0.00      0.01       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       1.00      0.01      0.01       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.75      0.04      0.08      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.50      0.00      0.00      1556\n",
      "          57       0.68      0.01      0.03      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       1.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.67      0.00      0.01       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.59      0.26      0.36      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.62      0.09      0.15     28022\n",
      "   macro avg       0.18      0.01      0.01     28022\n",
      "weighted avg       0.48      0.09      0.11     28022\n",
      " samples avg       0.20      0.11      0.12     28022\n",
      "\n",
      "t2_100_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.041    |             |\n",
      "| Term Wise Accuracy |    0.959    |             |\n",
      "|      Accuracy      |    0.032    |             |\n",
      "|     Precision      |    0.157    |    0.602    |\n",
      "|       Recall       |    0.009    |    0.073    |\n",
      "|     F1-measure     |    0.013    |    0.130    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.041 |\n",
      "|   Literature Accuracy    | 0.080 |\n",
      "|   Literature Precision   | 0.172 |\n",
      "|     LiteratureRecall     | 0.087 |\n",
      "|   LiteratureF1-measure   | 0.116 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.33      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.33      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       1.00      0.00      0.01       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.67      0.00      0.00      1849\n",
      "          29       1.00      0.00      0.00       548\n",
      "          30       1.00      0.00      0.01       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       1.00      0.01      0.01       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.50      0.00      0.01       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       1.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.61      0.38      0.47      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.50      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.77      0.02      0.04      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.25      0.00      0.00      1556\n",
      "          57       0.60      0.02      0.04      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.50      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.50      0.00      0.01       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.58      0.20      0.30      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.60      0.07      0.13     28022\n",
      "   macro avg       0.16      0.01      0.01     28022\n",
      "weighted avg       0.39      0.07      0.10     28022\n",
      " samples avg       0.17      0.09      0.10     28022\n",
      "\n",
      "t1_200_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.043    |             |\n",
      "|     Precision      |    0.145    |    0.630    |\n",
      "|       Recall       |    0.012    |    0.092    |\n",
      "|     F1-measure     |    0.017    |    0.161    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.104 |\n",
      "|   Literature Precision   | 0.214 |\n",
      "|     LiteratureRecall     | 0.112 |\n",
      "|   LiteratureF1-measure   | 0.147 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       1.00      0.01      0.02       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       1.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.50      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.50      0.00      0.01       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.58      0.00      0.01      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       1.00      0.03      0.06        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.63      0.45      0.53      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.84      0.05      0.09      1184\n",
      "          53       1.00      0.01      0.01       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       1.00      0.00      0.00      1556\n",
      "          57       0.66      0.04      0.08      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       1.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.60      0.27      0.37      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.63      0.09      0.16     28022\n",
      "   macro avg       0.15      0.01      0.02     28022\n",
      "weighted avg       0.43      0.09      0.12     28022\n",
      " samples avg       0.21      0.11      0.13     28022\n",
      "\n",
      "t1_200_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.048    |             |\n",
      "|     Precision      |    0.100    |    0.642    |\n",
      "|       Recall       |    0.013    |    0.099    |\n",
      "|     F1-measure     |    0.017    |    0.172    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.113 |\n",
      "|   Literature Precision   | 0.233 |\n",
      "|     LiteratureRecall     | 0.123 |\n",
      "|   LiteratureF1-measure   | 0.161 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.00      0.01       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.01      0.01       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.67      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.70      0.01      0.01      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.64      0.46      0.54      3697\n",
      "          44       1.00      0.00      0.01       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.80      0.05      0.10      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.00      0.00      0.00      1556\n",
      "          57       0.66      0.07      0.12      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.62      0.31      0.41      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.64      0.10      0.17     28022\n",
      "   macro avg       0.10      0.01      0.02     28022\n",
      "weighted avg       0.34      0.10      0.13     28022\n",
      " samples avg       0.23      0.12      0.14     28022\n",
      "\n",
      "t1_200_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.039    |             |\n",
      "|     Precision      |    0.110    |    0.622    |\n",
      "|       Recall       |    0.012    |    0.092    |\n",
      "|     F1-measure     |    0.015    |    0.160    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.099 |\n",
      "|   Literature Precision   | 0.209 |\n",
      "|     LiteratureRecall     | 0.109 |\n",
      "|   LiteratureF1-measure   | 0.143 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.60      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.67      0.00      0.01      1849\n",
      "          29       1.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       1.00      0.00      0.00       527\n",
      "          38       0.50      0.01      0.01       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.63      0.44      0.52      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.74      0.03      0.06      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.44      0.00      0.01      1556\n",
      "          57       0.62      0.04      0.07      1850\n",
      "          58       0.50      0.01      0.02       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.50      0.01      0.01       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.61      0.29      0.39      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.62      0.09      0.16     28022\n",
      "   macro avg       0.11      0.01      0.02     28022\n",
      "weighted avg       0.38      0.09      0.12     28022\n",
      " samples avg       0.21      0.11      0.13     28022\n",
      "\n",
      "t2_200_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.052    |             |\n",
      "|     Precision      |    0.112    |    0.644    |\n",
      "|       Recall       |    0.014    |    0.103    |\n",
      "|     F1-measure     |    0.018    |    0.178    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.119 |\n",
      "|   Literature Precision   | 0.241 |\n",
      "|     LiteratureRecall     | 0.128 |\n",
      "|   LiteratureF1-measure   | 0.167 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.83      0.00      0.01      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.89      0.00      0.01      1849\n",
      "          29       1.00      0.01      0.01       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       1.00      0.01      0.02        99\n",
      "          33       1.00      0.01      0.01       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.65      0.48      0.55      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.80      0.05      0.10      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.50      0.00      0.00      1556\n",
      "          57       0.63      0.08      0.15      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.63      0.31      0.42      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.64      0.10      0.18     28022\n",
      "   macro avg       0.11      0.01      0.02     28022\n",
      "weighted avg       0.39      0.10      0.13     28022\n",
      " samples avg       0.24      0.13      0.15     28022\n",
      "\n",
      "t2_200_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.054    |             |\n",
      "|     Precision      |    0.214    |    0.645    |\n",
      "|       Recall       |    0.015    |    0.109    |\n",
      "|     F1-measure     |    0.020    |    0.187    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.125 |\n",
      "|   Literature Precision   | 0.252 |\n",
      "|     LiteratureRecall     | 0.135 |\n",
      "|   LiteratureF1-measure   | 0.176 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.01       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       1.00      0.01      0.01       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       1.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.89      0.00      0.01      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       1.00      0.00      0.01       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.67      0.01      0.02       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.44      0.00      0.01      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.64      0.49      0.56      3697\n",
      "          44       1.00      0.00      0.01       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.80      0.07      0.12      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.78      0.00      0.01      1556\n",
      "          57       0.65      0.09      0.16      1850\n",
      "          58       1.00      0.00      0.01       392\n",
      "          59       0.67      0.00      0.01       532\n",
      "          60       1.00      0.01      0.02       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.01       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.64      0.34      0.44      2813\n",
      "          69       1.00      0.02      0.04        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.65      0.11      0.19     28022\n",
      "   macro avg       0.21      0.02      0.02     28022\n",
      "weighted avg       0.49      0.11      0.14     28022\n",
      " samples avg       0.25      0.14      0.16     28022\n",
      "\n",
      "t2_200_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.042    |             |\n",
      "|     Precision      |    0.170    |    0.633    |\n",
      "|       Recall       |    0.012    |    0.091    |\n",
      "|     F1-measure     |    0.017    |    0.159    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.104 |\n",
      "|   Literature Precision   | 0.218 |\n",
      "|     LiteratureRecall     | 0.113 |\n",
      "|   LiteratureF1-measure   | 0.148 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       1.00      0.01      0.03        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       1.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.33      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.82      0.00      0.01      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       1.00      0.01      0.01       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.64      0.46      0.54      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.67      0.01      0.01       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.78      0.04      0.08      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.88      0.00      0.01      1556\n",
      "          57       0.69      0.04      0.07      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       1.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       1.00      0.04      0.07        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.67      0.01      0.01       370\n",
      "          68       0.60      0.25      0.35      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.63      0.09      0.16     28022\n",
      "   macro avg       0.17      0.01      0.02     28022\n",
      "weighted avg       0.45      0.09      0.12     28022\n",
      " samples avg       0.22      0.11      0.13     28022\n",
      "\n",
      "t1_300_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.049    |             |\n",
      "|     Precision      |    0.184    |    0.640    |\n",
      "|       Recall       |    0.014    |    0.104    |\n",
      "|     F1-measure     |    0.019    |    0.179    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.117 |\n",
      "|   Literature Precision   | 0.239 |\n",
      "|     LiteratureRecall     | 0.127 |\n",
      "|   LiteratureF1-measure   | 0.166 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.01       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.01      0.01       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.01      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.70      0.00      0.01      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       1.00      0.00      0.01       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.60      0.01      0.02      1849\n",
      "          29       1.00      0.01      0.01       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.65      0.47      0.55      3697\n",
      "          44       0.50      0.00      0.01       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.77      0.06      0.11      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.55      0.00      0.01      1556\n",
      "          57       0.66      0.06      0.11      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       1.00      0.01      0.01       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.01      0.01       370\n",
      "          68       0.62      0.33      0.43      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.64      0.10      0.18     28022\n",
      "   macro avg       0.18      0.01      0.02     28022\n",
      "weighted avg       0.44      0.10      0.13     28022\n",
      " samples avg       0.24      0.13      0.15     28022\n",
      "\n",
      "t1_300_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.052    |             |\n",
      "|     Precision      |    0.269    |    0.651    |\n",
      "|       Recall       |    0.015    |    0.109    |\n",
      "|     F1-measure     |    0.020    |    0.186    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.122 |\n",
      "|   Literature Precision   | 0.252 |\n",
      "|     LiteratureRecall     | 0.133 |\n",
      "|   LiteratureF1-measure   | 0.174 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.75      0.01      0.02       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.63      0.01      0.01      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       1.00      0.00      0.01       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       1.00      0.01      0.01       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.70      0.01      0.02      1849\n",
      "          29       1.00      0.00      0.01       548\n",
      "          30       1.00      0.00      0.01       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       1.00      0.01      0.01       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       1.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.65      0.49      0.56      3697\n",
      "          44       1.00      0.00      0.01       322\n",
      "          45       1.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       1.00      0.00      0.01       330\n",
      "          48       1.00      0.01      0.03       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.83      0.08      0.14      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.75      0.00      0.01      1556\n",
      "          57       0.63      0.08      0.14      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       1.00      0.00      0.00       532\n",
      "          60       1.00      0.01      0.01       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.50      0.00      0.01       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.00      0.01       370\n",
      "          68       0.63      0.33      0.44      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.65      0.11      0.19     28022\n",
      "   macro avg       0.27      0.02      0.02     28022\n",
      "weighted avg       0.53      0.11      0.14     28022\n",
      " samples avg       0.25      0.13      0.15     28022\n",
      "\n",
      "t1_300_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.045    |             |\n",
      "|     Precision      |    0.115    |    0.636    |\n",
      "|       Recall       |    0.013    |    0.099    |\n",
      "|     F1-measure     |    0.017    |    0.172    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.111 |\n",
      "|   Literature Precision   | 0.231 |\n",
      "|     LiteratureRecall     | 0.122 |\n",
      "|   LiteratureF1-measure   | 0.160 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.00      0.01       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       1.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.38      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.50      0.01      0.01      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.64      0.46      0.54      3697\n",
      "          44       1.00      0.01      0.01       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.78      0.05      0.10      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.67      0.00      0.01      1556\n",
      "          57       0.59      0.06      0.11      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.01      0.02       370\n",
      "          68       0.63      0.31      0.42      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.64      0.10      0.17     28022\n",
      "   macro avg       0.12      0.01      0.02     28022\n",
      "weighted avg       0.39      0.10      0.13     28022\n",
      " samples avg       0.23      0.12      0.14     28022\n",
      "\n",
      "t2_300_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.057    |             |\n",
      "|     Precision      |    0.233    |    0.646    |\n",
      "|       Recall       |    0.015    |    0.112    |\n",
      "|     F1-measure     |    0.020    |    0.191    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.130 |\n",
      "|   Literature Precision   | 0.260 |\n",
      "|     LiteratureRecall     | 0.141 |\n",
      "|   LiteratureF1-measure   | 0.182 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       1.00      0.01      0.02       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       1.00      0.01      0.03        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.75      0.00      0.01      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.79      0.01      0.02      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       1.00      0.00      0.01       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.76      0.01      0.02      1849\n",
      "          29       1.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       1.00      0.00      0.00       527\n",
      "          38       1.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.64      0.50      0.57      3697\n",
      "          44       0.67      0.01      0.01       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.81      0.08      0.14      1184\n",
      "          53       1.00      0.01      0.01       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.89      0.01      0.01      1556\n",
      "          57       0.62      0.07      0.13      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       1.00      0.00      0.01       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.00      0.01       370\n",
      "          68       0.64      0.35      0.45      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.65      0.11      0.19     28022\n",
      "   macro avg       0.23      0.02      0.02     28022\n",
      "weighted avg       0.54      0.11      0.14     28022\n",
      " samples avg       0.26      0.14      0.16     28022\n",
      "\n",
      "t2_300_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.060    |             |\n",
      "|     Precision      |    0.154    |    0.661    |\n",
      "|       Recall       |    0.016    |    0.118    |\n",
      "|     F1-measure     |    0.022    |    0.200    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.136 |\n",
      "|   Literature Precision   | 0.272 |\n",
      "|     LiteratureRecall     | 0.147 |\n",
      "|   LiteratureF1-measure   | 0.191 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.00      0.01       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       1.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.75      0.01      0.02      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.88      0.02      0.04       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.59      0.01      0.02      1849\n",
      "          29       0.67      0.01      0.02       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.67      0.52      0.58      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.77      0.09      0.17      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.65      0.01      0.03      1556\n",
      "          57       0.66      0.10      0.18      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       1.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.67      0.00      0.01       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.00      0.01       370\n",
      "          68       0.64      0.36      0.46      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.66      0.12      0.20     28022\n",
      "   macro avg       0.15      0.02      0.02     28022\n",
      "weighted avg       0.47      0.12      0.15     28022\n",
      " samples avg       0.27      0.15      0.17     28022\n",
      "\n",
      "t2_300_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.048    |             |\n",
      "|     Precision      |    0.124    |    0.644    |\n",
      "|       Recall       |    0.014    |    0.105    |\n",
      "|     F1-measure     |    0.018    |    0.180    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.118 |\n",
      "|   Literature Precision   | 0.241 |\n",
      "|     LiteratureRecall     | 0.128 |\n",
      "|   LiteratureF1-measure   | 0.168 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.01      0.02       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.80      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.68      0.01      0.02      1849\n",
      "          29       1.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.64      0.49      0.55      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       1.00      0.01      0.01       169\n",
      "          52       0.79      0.05      0.10      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.60      0.00      0.01      1556\n",
      "          57       0.66      0.08      0.14      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.01       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.63      0.32      0.42      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.64      0.10      0.18     28022\n",
      "   macro avg       0.12      0.01      0.02     28022\n",
      "weighted avg       0.41      0.10      0.13     28022\n",
      " samples avg       0.24      0.13      0.15     28022\n",
      "\n",
      "t1_400_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.057    |             |\n",
      "|     Precision      |    0.219    |    0.655    |\n",
      "|       Recall       |    0.016    |    0.113    |\n",
      "|     F1-measure     |    0.021    |    0.193    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.129 |\n",
      "|   Literature Precision   | 0.260 |\n",
      "|     LiteratureRecall     | 0.140 |\n",
      "|   LiteratureF1-measure   | 0.182 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.01       169\n",
      "           1       0.80      0.01      0.02       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.00      0.01       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.80      0.00      0.01      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.84      0.01      0.02      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       1.00      0.01      0.02       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.69      0.02      0.04      1849\n",
      "          29       1.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.65      0.49      0.56      3697\n",
      "          44       1.00      0.01      0.01       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       1.00      0.01      0.01       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.81      0.07      0.13      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.69      0.01      0.03      1556\n",
      "          57       0.67      0.08      0.15      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       1.00      0.01      0.01       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.00      0.01       370\n",
      "          68       0.64      0.36      0.46      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.66      0.11      0.19     28022\n",
      "   macro avg       0.22      0.02      0.02     28022\n",
      "weighted avg       0.52      0.11      0.14     28022\n",
      " samples avg       0.26      0.14      0.16     28022\n",
      "\n",
      "t1_400_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.060    |             |\n",
      "|     Precision      |    0.276    |    0.663    |\n",
      "|       Recall       |    0.017    |    0.118    |\n",
      "|     F1-measure     |    0.023    |    0.200    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.136 |\n",
      "|   Literature Precision   | 0.270 |\n",
      "|     LiteratureRecall     | 0.147 |\n",
      "|   LiteratureF1-measure   | 0.191 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       1.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.01      0.02       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       1.00      0.01      0.01       139\n",
      "          12       1.00      0.01      0.02       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       1.00      0.00      0.01      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.70      0.01      0.02      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       1.00      0.00      0.01       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.87      0.04      0.07       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.66      0.02      0.04      1849\n",
      "          29       1.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       1.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       1.00      0.00      0.01       298\n",
      "          43       0.66      0.49      0.56      3697\n",
      "          44       1.00      0.00      0.01       322\n",
      "          45       1.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.75      0.09      0.16      1184\n",
      "          53       1.00      0.01      0.01       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.64      0.01      0.02      1556\n",
      "          57       0.70      0.11      0.18      1850\n",
      "          58       1.00      0.00      0.01       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.01       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.66      0.38      0.48      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.66      0.12      0.20     28022\n",
      "   macro avg       0.28      0.02      0.02     28022\n",
      "weighted avg       0.58      0.12      0.15     28022\n",
      " samples avg       0.27      0.15      0.17     28022\n",
      "\n",
      "t1_400_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.050    |             |\n",
      "|     Precision      |    0.206    |    0.647    |\n",
      "|       Recall       |    0.014    |    0.105    |\n",
      "|     F1-measure     |    0.019    |    0.181    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.040 |\n",
      "|   Literature Accuracy    | 0.120 |\n",
      "|   Literature Precision   | 0.244 |\n",
      "|     LiteratureRecall     | 0.129 |\n",
      "|   LiteratureF1-measure   | 0.169 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       1.00      0.01      0.02       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.01      0.01       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       1.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.73      0.01      0.01      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       1.00      0.00      0.01       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.83      0.01      0.03       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.64      0.01      0.03      1849\n",
      "          29       1.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.65      0.48      0.55      3697\n",
      "          44       1.00      0.00      0.01       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.84      0.07      0.12      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.69      0.01      0.01      1556\n",
      "          57       0.64      0.08      0.14      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       1.00      0.01      0.01       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.00      0.01       370\n",
      "          68       0.63      0.31      0.42      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.65      0.11      0.18     28022\n",
      "   macro avg       0.21      0.01      0.02     28022\n",
      "weighted avg       0.48      0.11      0.13     28022\n",
      " samples avg       0.24      0.13      0.15     28022\n",
      "\n",
      "t2_400_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.061    |             |\n",
      "|     Precision      |    0.200    |    0.664    |\n",
      "|       Recall       |    0.017    |    0.117    |\n",
      "|     F1-measure     |    0.023    |    0.199    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.136 |\n",
      "|   Literature Precision   | 0.272 |\n",
      "|     LiteratureRecall     | 0.146 |\n",
      "|   LiteratureF1-measure   | 0.190 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       1.00      0.01      0.03        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.50      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       1.00      0.00      0.01      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.83      0.01      0.02      1680\n",
      "          21       1.00      0.01      0.02       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       1.00      0.03      0.06       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.60      0.01      0.03      1849\n",
      "          29       0.67      0.01      0.01       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       1.00      0.01      0.02       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.66      0.51      0.57      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       1.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.81      0.09      0.16      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.79      0.01      0.03      1556\n",
      "          57       0.69      0.10      0.18      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.01       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.01      0.01       370\n",
      "          68       0.65      0.36      0.46      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.66      0.12      0.20     28022\n",
      "   macro avg       0.20      0.02      0.02     28022\n",
      "weighted avg       0.50      0.12      0.15     28022\n",
      " samples avg       0.27      0.15      0.17     28022\n",
      "\n",
      "t2_400_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.066    |             |\n",
      "|     Precision      |    0.322    |    0.667    |\n",
      "|       Recall       |    0.020    |    0.126    |\n",
      "|     F1-measure     |    0.029    |    0.212    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.146 |\n",
      "|   Literature Precision   | 0.288 |\n",
      "|     LiteratureRecall     | 0.157 |\n",
      "|   LiteratureF1-measure   | 0.204 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.00      0.01       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.60      0.00      0.01      1194\n",
      "          18       1.00      0.02      0.03        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.65      0.02      0.03      1680\n",
      "          21       1.00      0.01      0.02       116\n",
      "          22       1.00      0.00      0.01       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       1.00      0.00      0.00       450\n",
      "          26       0.93      0.04      0.08       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.67      0.02      0.05      1849\n",
      "          29       1.00      0.01      0.02       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       1.00      0.01      0.01       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       1.00      0.00      0.01       298\n",
      "          43       0.67      0.52      0.59      3697\n",
      "          44       1.00      0.00      0.01       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       1.00      0.08      0.15        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       1.00      0.01      0.03       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       1.00      0.01      0.02       169\n",
      "          52       0.79      0.10      0.18      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.74      0.02      0.04      1556\n",
      "          57       0.67      0.13      0.22      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.50      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       1.00      0.01      0.02       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.01       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.01      0.01       370\n",
      "          68       0.65      0.38      0.48      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.67      0.13      0.21     28022\n",
      "   macro avg       0.32      0.02      0.03     28022\n",
      "weighted avg       0.56      0.13      0.16     28022\n",
      " samples avg       0.29      0.16      0.18     28022\n",
      "\n",
      "t2_400_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.059    |             |\n",
      "|     Precision      |    0.299    |    0.659    |\n",
      "|       Recall       |    0.016    |    0.112    |\n",
      "|     F1-measure     |    0.022    |    0.191    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.132 |\n",
      "|   Literature Precision   | 0.266 |\n",
      "|     LiteratureRecall     | 0.141 |\n",
      "|   LiteratureF1-measure   | 0.185 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.01      0.01       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.71      0.01      0.01      1680\n",
      "          21       1.00      0.01      0.02       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       1.00      0.01      0.02       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       1.00      0.00      0.01       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.71      0.01      0.03      1849\n",
      "          29       1.00      0.00      0.01       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       1.00      0.01      0.02       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       1.00      0.00      0.00       527\n",
      "          38       1.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       1.00      0.00      0.01       298\n",
      "          43       0.66      0.51      0.57      3697\n",
      "          44       0.50      0.00      0.01       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       1.00      0.01      0.01       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       1.00      0.01      0.02       104\n",
      "          51       1.00      0.01      0.01       169\n",
      "          52       0.81      0.07      0.13      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.75      0.01      0.02      1556\n",
      "          57       0.66      0.09      0.15      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       1.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.75      0.01      0.02       370\n",
      "          68       0.64      0.34      0.44      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.66      0.11      0.19     28022\n",
      "   macro avg       0.30      0.02      0.02     28022\n",
      "weighted avg       0.54      0.11      0.14     28022\n",
      " samples avg       0.27      0.14      0.16     28022\n",
      "\n",
      "t1_500_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.060    |             |\n",
      "|     Precision      |    0.219    |    0.660    |\n",
      "|       Recall       |    0.017    |    0.118    |\n",
      "|     F1-measure     |    0.023    |    0.200    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.136 |\n",
      "|   Literature Precision   | 0.271 |\n",
      "|     LiteratureRecall     | 0.146 |\n",
      "|   LiteratureF1-measure   | 0.190 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.75      0.00      0.01      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.68      0.02      0.03      1680\n",
      "          21       1.00      0.01      0.02       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       1.00      0.02      0.03       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.68      0.02      0.05      1849\n",
      "          29       1.00      0.01      0.01       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       1.00      0.01      0.01       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       1.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.65      0.50      0.57      3697\n",
      "          44       1.00      0.00      0.01       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       1.00      0.01      0.01       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.80      0.09      0.16      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.71      0.01      0.03      1556\n",
      "          57       0.65      0.11      0.19      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.01      0.01       370\n",
      "          68       0.65      0.37      0.47      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.66      0.12      0.20     28022\n",
      "   macro avg       0.22      0.02      0.02     28022\n",
      "weighted avg       0.51      0.12      0.15     28022\n",
      " samples avg       0.27      0.15      0.17     28022\n",
      "\n",
      "t1_500_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.067    |             |\n",
      "|     Precision      |    0.291    |    0.671    |\n",
      "|       Recall       |    0.020    |    0.128    |\n",
      "|     F1-measure     |    0.028    |    0.215    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.149 |\n",
      "|   Literature Precision   | 0.294 |\n",
      "|     LiteratureRecall     | 0.160 |\n",
      "|   LiteratureF1-measure   | 0.207 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       1.00      0.00      0.01       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.01      0.02       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.01      0.01       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       1.00      0.00      0.01      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.69      0.03      0.05      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.95      0.05      0.10       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.62      0.03      0.06      1849\n",
      "          29       0.92      0.02      0.04       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       1.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.66      0.53      0.59      3697\n",
      "          44       1.00      0.01      0.01       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       1.00      0.04      0.08        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       1.00      0.01      0.01       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.83      0.11      0.20      1184\n",
      "          53       1.00      0.01      0.02       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.67      0.02      0.04      1556\n",
      "          57       0.65      0.11      0.19      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       1.00      0.00      0.00       532\n",
      "          60       1.00      0.01      0.02       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.00       661\n",
      "          66       1.00      0.01      0.02        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.67      0.40      0.50      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.67      0.13      0.22     28022\n",
      "   macro avg       0.29      0.02      0.03     28022\n",
      "weighted avg       0.56      0.13      0.16     28022\n",
      " samples avg       0.29      0.16      0.19     28022\n",
      "\n",
      "t1_500_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.058    |             |\n",
      "|     Precision      |    0.178    |    0.661    |\n",
      "|       Recall       |    0.016    |    0.115    |\n",
      "|     F1-measure     |    0.021    |    0.196    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.131 |\n",
      "|   Literature Precision   | 0.264 |\n",
      "|     LiteratureRecall     | 0.142 |\n",
      "|   LiteratureF1-measure   | 0.185 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       1.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.75      0.02      0.05      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       1.00      0.02      0.04       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.68      0.02      0.03      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       1.00      0.00      0.01       298\n",
      "          43       0.66      0.50      0.57      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       1.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       1.00      0.01      0.01       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.81      0.07      0.12      1184\n",
      "          53       1.00      0.01      0.01       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.69      0.01      0.03      1556\n",
      "          57       0.66      0.10      0.17      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.75      0.01      0.02       370\n",
      "          68       0.65      0.36      0.47      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.66      0.11      0.20     28022\n",
      "   macro avg       0.18      0.02      0.02     28022\n",
      "weighted avg       0.48      0.11      0.15     28022\n",
      " samples avg       0.26      0.14      0.16     28022\n",
      "\n",
      "t2_500_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.067    |             |\n",
      "|     Precision      |    0.275    |    0.666    |\n",
      "|       Recall       |    0.019    |    0.126    |\n",
      "|     F1-measure     |    0.026    |    0.212    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.146 |\n",
      "|   Literature Precision   | 0.289 |\n",
      "|     LiteratureRecall     | 0.158 |\n",
      "|   LiteratureF1-measure   | 0.204 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.01      0.01       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       1.00      0.00      0.01      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.61      0.02      0.03      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       1.00      0.00      0.01       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       1.00      0.00      0.01       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       1.00      0.03      0.06       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.67      0.03      0.06      1849\n",
      "          29       1.00      0.01      0.03       548\n",
      "          30       1.00      0.00      0.01       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       1.00      0.01      0.02       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.66      0.52      0.59      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       1.00      0.01      0.01       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.78      0.10      0.18      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.68      0.03      0.05      1556\n",
      "          57       0.63      0.12      0.20      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.83      0.01      0.02       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       1.00      0.01      0.01       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       1.00      0.01      0.02       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.01      0.01       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.00      0.01       370\n",
      "          68       0.66      0.39      0.49      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.67      0.13      0.21     28022\n",
      "   macro avg       0.28      0.02      0.03     28022\n",
      "weighted avg       0.54      0.13      0.16     28022\n",
      " samples avg       0.29      0.16      0.18     28022\n",
      "\n",
      "t2_500_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.072    |             |\n",
      "|     Precision      |    0.290    |    0.671    |\n",
      "|       Recall       |    0.020    |    0.133    |\n",
      "|     F1-measure     |    0.028    |    0.223    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.156 |\n",
      "|   Literature Precision   | 0.304 |\n",
      "|     LiteratureRecall     | 0.168 |\n",
      "|   LiteratureF1-measure   | 0.216 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.00      0.01       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.01      0.01       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.00      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.88      0.01      0.02      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.71      0.03      0.06      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       1.00      0.01      0.02       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.91      0.06      0.11       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.61      0.03      0.06      1849\n",
      "          29       1.00      0.01      0.01       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       1.00      0.01      0.02        97\n",
      "          36       1.00      0.00      0.01       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.67      0.54      0.60      3697\n",
      "          44       1.00      0.00      0.01       322\n",
      "          45       1.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       1.00      0.00      0.01       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.81      0.11      0.19      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.70      0.03      0.06      1556\n",
      "          57       0.68      0.15      0.25      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       1.00      0.01      0.02       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       1.00      0.01      0.02       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.01       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.01      0.01       370\n",
      "          68       0.65      0.40      0.50      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.67      0.13      0.22     28022\n",
      "   macro avg       0.29      0.02      0.03     28022\n",
      "weighted avg       0.56      0.13      0.17     28022\n",
      " samples avg       0.30      0.17      0.19     28022\n",
      "\n",
      "t2_500_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.039    |             |\n",
      "| Term Wise Accuracy |    0.961    |             |\n",
      "|      Accuracy      |    0.062    |             |\n",
      "|     Precision      |    0.213    |    0.660    |\n",
      "|       Recall       |    0.017    |    0.121    |\n",
      "|     F1-measure     |    0.024    |    0.205    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.039 |\n",
      "|   Literature Accuracy    | 0.140 |\n",
      "|   Literature Precision   | 0.277 |\n",
      "|     LiteratureRecall     | 0.151 |\n",
      "|   LiteratureF1-measure   | 0.195 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       1.00      0.01      0.02       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.83      0.00      0.01      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.72      0.02      0.04      1680\n",
      "          21       1.00      0.01      0.02       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.89      0.02      0.04       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.59      0.02      0.04      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       1.00      0.01      0.02        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       1.00      0.00      0.01       298\n",
      "          43       0.65      0.51      0.57      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       1.00      0.00      0.01       330\n",
      "          48       1.00      0.01      0.01       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.83      0.08      0.15      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.63      0.03      0.05      1556\n",
      "          57       0.68      0.12      0.20      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       1.00      0.00      0.01       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.67      0.00      0.01       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.01      0.02       370\n",
      "          68       0.65      0.38      0.48      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.66      0.12      0.20     28022\n",
      "   macro avg       0.21      0.02      0.02     28022\n",
      "weighted avg       0.48      0.12      0.15     28022\n",
      " samples avg       0.28      0.15      0.17     28022\n",
      "\n",
      "t1_1000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.038    |             |\n",
      "| Term Wise Accuracy |    0.962    |             |\n",
      "|      Accuracy      |    0.079    |             |\n",
      "|     Precision      |    0.461    |    0.710    |\n",
      "|       Recall       |    0.029    |    0.161    |\n",
      "|     F1-measure     |    0.042    |    0.263    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.038 |\n",
      "|   Literature Accuracy    | 0.182 |\n",
      "|   Literature Precision   | 0.351 |\n",
      "|     LiteratureRecall     | 0.197 |\n",
      "|   LiteratureF1-measure   | 0.252 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.01       169\n",
      "           1       1.00      0.02      0.04       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.02      0.03       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.01      0.01       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.84      0.03      0.05      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       1.00      0.00      0.01       282\n",
      "          20       0.76      0.09      0.16      1680\n",
      "          21       1.00      0.01      0.02       116\n",
      "          22       1.00      0.02      0.03       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       1.00      0.00      0.00       450\n",
      "          26       0.96      0.06      0.12       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.78      0.11      0.19      1849\n",
      "          29       1.00      0.04      0.08       548\n",
      "          30       1.00      0.00      0.01       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.80      0.04      0.08        99\n",
      "          33       1.00      0.01      0.02       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       1.00      0.00      0.01       304\n",
      "          37       1.00      0.01      0.01       527\n",
      "          38       1.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.70      0.57      0.63      3697\n",
      "          44       1.00      0.01      0.02       322\n",
      "          45       1.00      0.00      0.01       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       1.00      0.01      0.01       330\n",
      "          48       1.00      0.01      0.01       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       1.00      0.01      0.02       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.81      0.16      0.26      1184\n",
      "          53       1.00      0.02      0.03       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.71      0.08      0.14      1556\n",
      "          57       0.70      0.18      0.29      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       1.00      0.01      0.03       532\n",
      "          60       1.00      0.02      0.03       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       1.00      0.01      0.02       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.02      0.04       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       1.00      0.01      0.02       370\n",
      "          68       0.68      0.45      0.54      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.71      0.16      0.26     28022\n",
      "   macro avg       0.46      0.03      0.04     28022\n",
      "weighted avg       0.71      0.16      0.21     28022\n",
      " samples avg       0.35      0.20      0.23     28022\n",
      "\n",
      "t1_1000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.037    |             |\n",
      "| Term Wise Accuracy |    0.963    |             |\n",
      "|      Accuracy      |    0.090    |             |\n",
      "|     Precision      |    0.543    |    0.717    |\n",
      "|       Recall       |    0.034    |    0.170    |\n",
      "|     F1-measure     |    0.051    |    0.275    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.037 |\n",
      "|   Literature Accuracy    | 0.194 |\n",
      "|   Literature Precision   | 0.367 |\n",
      "|     LiteratureRecall     | 0.209 |\n",
      "|   LiteratureF1-measure   | 0.266 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.02       169\n",
      "           1       0.82      0.02      0.04       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       1.00      0.01      0.02       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.02      0.03       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.02      0.04       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.02      0.04       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.90      0.03      0.06      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       1.00      0.01      0.01       282\n",
      "          20       0.76      0.09      0.17      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       1.00      0.01      0.02       257\n",
      "          23       1.00      0.01      0.02       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       1.00      0.00      0.00       450\n",
      "          26       0.95      0.16      0.27       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.70      0.11      0.18      1849\n",
      "          29       0.77      0.05      0.09       548\n",
      "          30       1.00      0.01      0.03       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       1.00      0.01      0.02        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       1.00      0.01      0.01       304\n",
      "          37       1.00      0.01      0.03       527\n",
      "          38       1.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       1.00      0.03      0.06        35\n",
      "          42       1.00      0.01      0.02       298\n",
      "          43       0.70      0.59      0.64      3697\n",
      "          44       1.00      0.02      0.03       322\n",
      "          45       1.00      0.02      0.03       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       1.00      0.01      0.01       330\n",
      "          48       1.00      0.03      0.06       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.86      0.15      0.26      1184\n",
      "          53       1.00      0.01      0.02       176\n",
      "          54       1.00      0.02      0.03        64\n",
      "          55       1.00      0.01      0.01       140\n",
      "          56       0.76      0.08      0.15      1556\n",
      "          57       0.69      0.20      0.31      1850\n",
      "          58       1.00      0.00      0.01       392\n",
      "          59       1.00      0.03      0.06       532\n",
      "          60       1.00      0.03      0.05       197\n",
      "          61       1.00      0.01      0.03       156\n",
      "          62       1.00      0.00      0.01       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.02      0.04       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.95      0.06      0.11       370\n",
      "          68       0.70      0.47      0.56      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.72      0.17      0.28     28022\n",
      "   macro avg       0.54      0.03      0.05     28022\n",
      "weighted avg       0.75      0.17      0.22     28022\n",
      " samples avg       0.37      0.21      0.24     28022\n",
      "\n",
      "t1_1000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.038    |             |\n",
      "| Term Wise Accuracy |    0.962    |             |\n",
      "|      Accuracy      |    0.081    |             |\n",
      "|     Precision      |    0.414    |    0.703    |\n",
      "|       Recall       |    0.027    |    0.157    |\n",
      "|     F1-measure     |    0.040    |    0.257    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.038 |\n",
      "|   Literature Accuracy    | 0.180 |\n",
      "|   Literature Precision   | 0.349 |\n",
      "|     LiteratureRecall     | 0.194 |\n",
      "|   LiteratureF1-measure   | 0.249 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       1.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.01      0.02       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.01      0.01       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.02      0.03       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.95      0.03      0.06      1194\n",
      "          18       1.00      0.02      0.03        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.69      0.06      0.11      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.80      0.02      0.03       257\n",
      "          23       1.00      0.01      0.02       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.96      0.08      0.14       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.71      0.10      0.17      1849\n",
      "          29       1.00      0.02      0.05       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       1.00      0.01      0.01       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       1.00      0.02      0.04        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       1.00      0.00      0.01       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       1.00      0.00      0.01       298\n",
      "          43       0.69      0.58      0.63      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       1.00      0.00      0.01       330\n",
      "          48       1.00      0.02      0.04       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       1.00      0.01      0.02       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.83      0.15      0.25      1184\n",
      "          53       1.00      0.02      0.03       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.76      0.06      0.11      1556\n",
      "          57       0.71      0.18      0.29      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.71      0.01      0.02       532\n",
      "          60       1.00      0.02      0.03       197\n",
      "          61       1.00      0.01      0.01       156\n",
      "          62       1.00      0.00      0.01       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.01      0.02       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.89      0.02      0.04       370\n",
      "          68       0.68      0.44      0.54      2813\n",
      "          69       1.00      0.02      0.04        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.70      0.16      0.26     28022\n",
      "   macro avg       0.41      0.03      0.04     28022\n",
      "weighted avg       0.63      0.16      0.20     28022\n",
      " samples avg       0.35      0.19      0.22     28022\n",
      "\n",
      "t2_1000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.037    |             |\n",
      "| Term Wise Accuracy |    0.963    |             |\n",
      "|      Accuracy      |    0.095    |             |\n",
      "|     Precision      |    0.601    |    0.709    |\n",
      "|       Recall       |    0.035    |    0.171    |\n",
      "|     F1-measure     |    0.052    |    0.276    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.037 |\n",
      "|   Literature Accuracy    | 0.198 |\n",
      "|   Literature Precision   | 0.371 |\n",
      "|     LiteratureRecall     | 0.212 |\n",
      "|   LiteratureF1-measure   | 0.270 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.01       169\n",
      "           1       0.92      0.03      0.06       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       1.00      0.01      0.02        93\n",
      "           4       1.00      0.02      0.03        64\n",
      "           5       1.00      0.01      0.02       129\n",
      "           6       1.00      0.01      0.03        71\n",
      "           7       1.00      0.02      0.04       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.01      0.01       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       1.00      0.01      0.02       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       1.00      0.02      0.04        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.89      0.04      0.08      1194\n",
      "          18       1.00      0.05      0.09        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.75      0.10      0.17      1680\n",
      "          21       1.00      0.02      0.03       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       1.00      0.00      0.01       254\n",
      "          25       1.00      0.01      0.02       450\n",
      "          26       0.92      0.15      0.26       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.72      0.10      0.17      1849\n",
      "          29       0.88      0.04      0.08       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       1.00      0.01      0.02        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       1.00      0.01      0.02       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       1.00      0.01      0.01       527\n",
      "          38       1.00      0.01      0.01       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       1.00      0.01      0.02       298\n",
      "          43       0.69      0.58      0.63      3697\n",
      "          44       1.00      0.02      0.03       322\n",
      "          45       1.00      0.01      0.01       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       1.00      0.01      0.01       330\n",
      "          48       1.00      0.01      0.03       154\n",
      "          49       1.00      0.02      0.03       123\n",
      "          50       1.00      0.01      0.02       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.82      0.15      0.26      1184\n",
      "          53       1.00      0.03      0.07       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.80      0.10      0.19      1556\n",
      "          57       0.69      0.22      0.34      1850\n",
      "          58       1.00      0.00      0.01       392\n",
      "          59       1.00      0.01      0.03       532\n",
      "          60       1.00      0.03      0.05       197\n",
      "          61       1.00      0.01      0.01       156\n",
      "          62       1.00      0.02      0.03       233\n",
      "          63       1.00      0.01      0.02       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.01      0.02       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.93      0.04      0.07       370\n",
      "          68       0.68      0.46      0.55      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       1.00      0.02      0.04        55\n",
      "\n",
      "   micro avg       0.71      0.17      0.28     28022\n",
      "   macro avg       0.60      0.03      0.05     28022\n",
      "weighted avg       0.74      0.17      0.22     28022\n",
      " samples avg       0.37      0.21      0.24     28022\n",
      "\n",
      "t2_1000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.037    |             |\n",
      "| Term Wise Accuracy |    0.963    |             |\n",
      "|      Accuracy      |    0.092    |             |\n",
      "|     Precision      |    0.550    |    0.716    |\n",
      "|       Recall       |    0.035    |    0.176    |\n",
      "|     F1-measure     |    0.053    |    0.282    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.037 |\n",
      "|   Literature Accuracy    | 0.201 |\n",
      "|   Literature Precision   | 0.381 |\n",
      "|     LiteratureRecall     | 0.217 |\n",
      "|   LiteratureF1-measure   | 0.276 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       1.00      0.04      0.09       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.01      0.02       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.01      0.01       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       1.00      0.01      0.01       139\n",
      "          12       1.00      0.01      0.01       330\n",
      "          13       1.00      0.01      0.03        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.87      0.05      0.10      1194\n",
      "          18       1.00      0.03      0.06        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.71      0.09      0.17      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       1.00      0.01      0.02       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       1.00      0.01      0.02       254\n",
      "          25       1.00      0.00      0.00       450\n",
      "          26       0.94      0.17      0.28       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.73      0.11      0.19      1849\n",
      "          29       0.88      0.05      0.10       548\n",
      "          30       0.67      0.01      0.01       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       1.00      0.01      0.02        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       1.00      0.01      0.01       527\n",
      "          38       1.00      0.01      0.01       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       1.00      0.01      0.01       298\n",
      "          43       0.70      0.60      0.65      3697\n",
      "          44       1.00      0.02      0.03       322\n",
      "          45       0.67      0.00      0.01       464\n",
      "          46       1.00      0.04      0.08        25\n",
      "          47       1.00      0.01      0.01       330\n",
      "          48       1.00      0.03      0.05       154\n",
      "          49       1.00      0.01      0.02       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.84      0.16      0.28      1184\n",
      "          53       1.00      0.02      0.03       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       1.00      0.01      0.03       140\n",
      "          56       0.76      0.09      0.16      1556\n",
      "          57       0.71      0.24      0.36      1850\n",
      "          58       1.00      0.00      0.01       392\n",
      "          59       1.00      0.03      0.07       532\n",
      "          60       1.00      0.04      0.08       197\n",
      "          61       1.00      0.01      0.03       156\n",
      "          62       1.00      0.00      0.01       233\n",
      "          63       1.00      0.01      0.02       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.01      0.02       661\n",
      "          66       1.00      0.01      0.02        80\n",
      "          67       0.94      0.04      0.08       370\n",
      "          68       0.69      0.46      0.55      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.72      0.18      0.28     28022\n",
      "   macro avg       0.55      0.04      0.05     28022\n",
      "weighted avg       0.74      0.18      0.23     28022\n",
      " samples avg       0.38      0.22      0.25     28022\n",
      "\n",
      "t2_1000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.038    |             |\n",
      "| Term Wise Accuracy |    0.962    |             |\n",
      "|      Accuracy      |    0.085    |             |\n",
      "|     Precision      |    0.496    |    0.707    |\n",
      "|       Recall       |    0.030    |    0.160    |\n",
      "|     F1-measure     |    0.046    |    0.261    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.038 |\n",
      "|   Literature Accuracy    | 0.184 |\n",
      "|   Literature Precision   | 0.346 |\n",
      "|     LiteratureRecall     | 0.198 |\n",
      "|   LiteratureF1-measure   | 0.252 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.02       169\n",
      "           1       1.00      0.01      0.02       423\n",
      "           2       1.00      0.01      0.02        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.75      0.02      0.05       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       1.00      0.00      0.01       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.88      0.02      0.04       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.78      0.03      0.06      1194\n",
      "          18       1.00      0.02      0.03        63\n",
      "          19       1.00      0.00      0.01       282\n",
      "          20       0.76      0.08      0.14      1680\n",
      "          21       0.75      0.03      0.05       116\n",
      "          22       1.00      0.01      0.02       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       1.00      0.00      0.01       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.96      0.07      0.12       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.75      0.09      0.16      1849\n",
      "          29       1.00      0.03      0.06       548\n",
      "          30       1.00      0.01      0.01       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       1.00      0.01      0.02       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       1.00      0.01      0.03       304\n",
      "          37       1.00      0.01      0.01       527\n",
      "          38       1.00      0.02      0.03       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       1.00      0.00      0.01       298\n",
      "          43       0.69      0.56      0.62      3697\n",
      "          44       1.00      0.02      0.04       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       1.00      0.01      0.01       330\n",
      "          48       0.83      0.03      0.06       154\n",
      "          49       1.00      0.02      0.05       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.86      0.15      0.25      1184\n",
      "          53       1.00      0.03      0.07       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.76      0.08      0.14      1556\n",
      "          57       0.69      0.20      0.31      1850\n",
      "          58       1.00      0.00      0.01       392\n",
      "          59       1.00      0.02      0.04       532\n",
      "          60       1.00      0.03      0.05       197\n",
      "          61       0.67      0.01      0.03       156\n",
      "          62       0.50      0.00      0.01       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.01      0.02       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.94      0.04      0.08       370\n",
      "          68       0.69      0.45      0.54      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.71      0.16      0.26     28022\n",
      "   macro avg       0.50      0.03      0.05     28022\n",
      "weighted avg       0.72      0.16      0.21     28022\n",
      " samples avg       0.35      0.20      0.23     28022\n",
      "\n",
      "t1_2000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.034    |             |\n",
      "| Term Wise Accuracy |    0.966    |             |\n",
      "|      Accuracy      |    0.135    |             |\n",
      "|     Precision      |    0.845    |    0.782    |\n",
      "|       Recall       |    0.076    |    0.259    |\n",
      "|     F1-measure     |    0.119    |    0.389    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.034 |\n",
      "|   Literature Accuracy    | 0.281 |\n",
      "|   Literature Precision   | 0.489 |\n",
      "|     LiteratureRecall     | 0.303 |\n",
      "|   LiteratureF1-measure   | 0.374 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.05       169\n",
      "           1       0.98      0.11      0.20       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       1.00      0.03      0.06        93\n",
      "           4       1.00      0.02      0.03        64\n",
      "           5       1.00      0.04      0.07       129\n",
      "           6       1.00      0.03      0.05        71\n",
      "           7       1.00      0.04      0.07       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.04      0.08       158\n",
      "          10       1.00      0.02      0.04        56\n",
      "          11       1.00      0.04      0.07       139\n",
      "          12       1.00      0.03      0.06       330\n",
      "          13       1.00      0.01      0.03        77\n",
      "          14       1.00      0.04      0.07        27\n",
      "          15       1.00      0.02      0.04        52\n",
      "          16       1.00      0.05      0.10        59\n",
      "          17       0.89      0.15      0.25      1194\n",
      "          18       1.00      0.03      0.06        63\n",
      "          19       1.00      0.02      0.05       282\n",
      "          20       0.79      0.26      0.39      1680\n",
      "          21       1.00      0.02      0.03       116\n",
      "          22       1.00      0.04      0.07       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       1.00      0.01      0.02       254\n",
      "          25       1.00      0.03      0.06       450\n",
      "          26       0.96      0.30      0.45       351\n",
      "          27       1.00      0.04      0.08        48\n",
      "          28       0.76      0.26      0.39      1849\n",
      "          29       0.94      0.14      0.25       548\n",
      "          30       1.00      0.04      0.07       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       1.00      0.07      0.13        99\n",
      "          33       1.00      0.05      0.09       171\n",
      "          34       0.88      0.07      0.13       102\n",
      "          35       1.00      0.03      0.06        97\n",
      "          36       1.00      0.02      0.04       304\n",
      "          37       1.00      0.05      0.09       527\n",
      "          38       1.00      0.04      0.07       413\n",
      "          39       1.00      0.03      0.06        68\n",
      "          40       1.00      0.02      0.04       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       1.00      0.01      0.02       298\n",
      "          43       0.74      0.66      0.70      3697\n",
      "          44       1.00      0.05      0.09       322\n",
      "          45       1.00      0.04      0.08       464\n",
      "          46       1.00      0.12      0.21        25\n",
      "          47       1.00      0.04      0.08       330\n",
      "          48       1.00      0.06      0.12       154\n",
      "          49       1.00      0.02      0.05       123\n",
      "          50       1.00      0.01      0.02       104\n",
      "          51       1.00      0.02      0.03       169\n",
      "          52       0.90      0.29      0.43      1184\n",
      "          53       1.00      0.07      0.13       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       1.00      0.01      0.03       140\n",
      "          56       0.81      0.24      0.37      1556\n",
      "          57       0.78      0.36      0.49      1850\n",
      "          58       1.00      0.02      0.04       392\n",
      "          59       0.98      0.11      0.19       532\n",
      "          60       0.94      0.16      0.28       197\n",
      "          61       1.00      0.02      0.04       156\n",
      "          62       1.00      0.03      0.07       233\n",
      "          63       1.00      0.02      0.04       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.98      0.08      0.16       661\n",
      "          66       1.00      0.03      0.05        80\n",
      "          67       0.95      0.10      0.18       370\n",
      "          68       0.74      0.58      0.65      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.78      0.26      0.39     28022\n",
      "   macro avg       0.85      0.08      0.12     28022\n",
      "weighted avg       0.86      0.26      0.33     28022\n",
      " samples avg       0.49      0.30      0.34     28022\n",
      "\n",
      "t1_2000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.033    |             |\n",
      "| Term Wise Accuracy |    0.967    |             |\n",
      "|      Accuracy      |    0.136    |             |\n",
      "|     Precision      |    0.903    |    0.791    |\n",
      "|       Recall       |    0.088    |    0.271    |\n",
      "|     F1-measure     |    0.139    |    0.404    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.033 |\n",
      "|   Literature Accuracy    | 0.291 |\n",
      "|   Literature Precision   | 0.499 |\n",
      "|     LiteratureRecall     | 0.315 |\n",
      "|   LiteratureF1-measure   | 0.386 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.03      0.06       169\n",
      "           1       0.99      0.16      0.27       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       1.00      0.04      0.08        93\n",
      "           4       1.00      0.03      0.06        64\n",
      "           5       1.00      0.07      0.13       129\n",
      "           6       1.00      0.04      0.08        71\n",
      "           7       1.00      0.07      0.13       264\n",
      "           8       1.00      0.04      0.07        28\n",
      "           9       1.00      0.04      0.08       158\n",
      "          10       1.00      0.02      0.04        56\n",
      "          11       1.00      0.02      0.04       139\n",
      "          12       1.00      0.06      0.11       330\n",
      "          13       1.00      0.04      0.08        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       1.00      0.06      0.11        52\n",
      "          16       1.00      0.02      0.03        59\n",
      "          17       0.90      0.16      0.28      1194\n",
      "          18       1.00      0.05      0.09        63\n",
      "          19       1.00      0.04      0.08       282\n",
      "          20       0.79      0.25      0.38      1680\n",
      "          21       1.00      0.01      0.02       116\n",
      "          22       1.00      0.05      0.09       257\n",
      "          23       1.00      0.02      0.05       125\n",
      "          24       1.00      0.03      0.06       254\n",
      "          25       1.00      0.05      0.09       450\n",
      "          26       0.95      0.36      0.52       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.79      0.29      0.43      1849\n",
      "          29       0.96      0.18      0.30       548\n",
      "          30       1.00      0.06      0.11       285\n",
      "          31       1.00      0.02      0.04        44\n",
      "          32       1.00      0.07      0.13        99\n",
      "          33       1.00      0.08      0.15       171\n",
      "          34       0.93      0.14      0.24       102\n",
      "          35       1.00      0.01      0.02        97\n",
      "          36       1.00      0.03      0.06       304\n",
      "          37       1.00      0.06      0.12       527\n",
      "          38       1.00      0.05      0.09       413\n",
      "          39       1.00      0.01      0.03        68\n",
      "          40       1.00      0.03      0.06       106\n",
      "          41       1.00      0.03      0.06        35\n",
      "          42       1.00      0.03      0.05       298\n",
      "          43       0.75      0.67      0.71      3697\n",
      "          44       1.00      0.07      0.12       322\n",
      "          45       0.94      0.07      0.13       464\n",
      "          46       1.00      0.16      0.28        25\n",
      "          47       1.00      0.04      0.08       330\n",
      "          48       1.00      0.07      0.13       154\n",
      "          49       1.00      0.05      0.09       123\n",
      "          50       1.00      0.01      0.02       104\n",
      "          51       1.00      0.02      0.03       169\n",
      "          52       0.91      0.31      0.46      1184\n",
      "          53       1.00      0.07      0.14       176\n",
      "          54       1.00      0.02      0.03        64\n",
      "          55       1.00      0.04      0.07       140\n",
      "          56       0.80      0.25      0.38      1556\n",
      "          57       0.77      0.35      0.48      1850\n",
      "          58       1.00      0.02      0.04       392\n",
      "          59       0.99      0.13      0.23       532\n",
      "          60       0.97      0.16      0.28       197\n",
      "          61       1.00      0.01      0.01       156\n",
      "          62       1.00      0.03      0.07       233\n",
      "          63       1.00      0.03      0.05       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.98      0.09      0.17       661\n",
      "          66       1.00      0.04      0.07        80\n",
      "          67       0.95      0.11      0.20       370\n",
      "          68       0.76      0.59      0.66      2813\n",
      "          69       1.00      0.02      0.04        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.79      0.27      0.40     28022\n",
      "   macro avg       0.90      0.09      0.14     28022\n",
      "weighted avg       0.87      0.27      0.35     28022\n",
      " samples avg       0.50      0.31      0.35     28022\n",
      "\n",
      "t1_2000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.034    |             |\n",
      "| Term Wise Accuracy |    0.966    |             |\n",
      "|      Accuracy      |    0.123    |             |\n",
      "|     Precision      |    0.817    |    0.778    |\n",
      "|       Recall       |    0.072    |    0.251    |\n",
      "|     F1-measure     |    0.114    |    0.380    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.034 |\n",
      "|   Literature Accuracy    | 0.266 |\n",
      "|   Literature Precision   | 0.467 |\n",
      "|     LiteratureRecall     | 0.288 |\n",
      "|   LiteratureF1-measure   | 0.356 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.03       169\n",
      "           1       0.98      0.10      0.18       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       1.00      0.05      0.10        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       1.00      0.05      0.09       129\n",
      "           6       1.00      0.03      0.05        71\n",
      "           7       1.00      0.05      0.09       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.04      0.07       158\n",
      "          10       1.00      0.02      0.04        56\n",
      "          11       1.00      0.03      0.06       139\n",
      "          12       1.00      0.05      0.10       330\n",
      "          13       1.00      0.01      0.03        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       1.00      0.04      0.07        52\n",
      "          16       1.00      0.03      0.07        59\n",
      "          17       0.87      0.12      0.21      1194\n",
      "          18       1.00      0.03      0.06        63\n",
      "          19       1.00      0.01      0.02       282\n",
      "          20       0.78      0.25      0.38      1680\n",
      "          21       1.00      0.03      0.05       116\n",
      "          22       1.00      0.04      0.07       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       1.00      0.02      0.05       254\n",
      "          25       1.00      0.02      0.05       450\n",
      "          26       0.96      0.25      0.40       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.75      0.26      0.39      1849\n",
      "          29       0.99      0.12      0.22       548\n",
      "          30       1.00      0.03      0.06       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       1.00      0.05      0.10        99\n",
      "          33       1.00      0.04      0.08       171\n",
      "          34       0.92      0.11      0.19       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       1.00      0.02      0.04       304\n",
      "          37       1.00      0.05      0.09       527\n",
      "          38       1.00      0.06      0.11       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       1.00      0.03      0.06       106\n",
      "          41       1.00      0.06      0.11        35\n",
      "          42       1.00      0.02      0.04       298\n",
      "          43       0.74      0.65      0.69      3697\n",
      "          44       1.00      0.04      0.08       322\n",
      "          45       0.97      0.07      0.13       464\n",
      "          46       1.00      0.08      0.15        25\n",
      "          47       1.00      0.04      0.07       330\n",
      "          48       1.00      0.06      0.12       154\n",
      "          49       1.00      0.02      0.03       123\n",
      "          50       1.00      0.01      0.02       104\n",
      "          51       1.00      0.04      0.07       169\n",
      "          52       0.88      0.28      0.42      1184\n",
      "          53       1.00      0.07      0.13       176\n",
      "          54       1.00      0.02      0.03        64\n",
      "          55       1.00      0.02      0.04       140\n",
      "          56       0.80      0.22      0.35      1556\n",
      "          57       0.77      0.34      0.47      1850\n",
      "          58       1.00      0.03      0.05       392\n",
      "          59       1.00      0.08      0.15       532\n",
      "          60       0.95      0.10      0.18       197\n",
      "          61       1.00      0.01      0.01       156\n",
      "          62       1.00      0.02      0.04       233\n",
      "          63       1.00      0.02      0.04       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.96      0.07      0.13       661\n",
      "          66       1.00      0.03      0.05        80\n",
      "          67       0.95      0.10      0.18       370\n",
      "          68       0.76      0.58      0.66      2813\n",
      "          69       1.00      0.02      0.04        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.78      0.25      0.38     28022\n",
      "   macro avg       0.82      0.07      0.11     28022\n",
      "weighted avg       0.85      0.25      0.33     28022\n",
      " samples avg       0.47      0.29      0.32     28022\n",
      "\n",
      "t2_2000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.034    |             |\n",
      "| Term Wise Accuracy |    0.966    |             |\n",
      "|      Accuracy      |    0.136    |             |\n",
      "|     Precision      |    0.829    |    0.783    |\n",
      "|       Recall       |    0.079    |    0.265    |\n",
      "|     F1-measure     |    0.123    |    0.396    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.034 |\n",
      "|   Literature Accuracy    | 0.288 |\n",
      "|   Literature Precision   | 0.497 |\n",
      "|     LiteratureRecall     | 0.311 |\n",
      "|   LiteratureF1-measure   | 0.383 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.03       169\n",
      "           1       0.91      0.12      0.21       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       1.00      0.06      0.12        93\n",
      "           4       1.00      0.03      0.06        64\n",
      "           5       1.00      0.05      0.09       129\n",
      "           6       1.00      0.06      0.11        71\n",
      "           7       1.00      0.03      0.07       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.03      0.06       158\n",
      "          10       1.00      0.02      0.04        56\n",
      "          11       1.00      0.04      0.07       139\n",
      "          12       0.93      0.04      0.08       330\n",
      "          13       1.00      0.03      0.05        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       1.00      0.10      0.18        52\n",
      "          16       1.00      0.02      0.03        59\n",
      "          17       0.88      0.17      0.29      1194\n",
      "          18       1.00      0.06      0.12        63\n",
      "          19       1.00      0.03      0.06       282\n",
      "          20       0.76      0.24      0.36      1680\n",
      "          21       1.00      0.02      0.03       116\n",
      "          22       1.00      0.05      0.09       257\n",
      "          23       1.00      0.01      0.02       125\n",
      "          24       1.00      0.04      0.07       254\n",
      "          25       1.00      0.02      0.04       450\n",
      "          26       0.95      0.28      0.44       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.78      0.28      0.41      1849\n",
      "          29       0.97      0.16      0.28       548\n",
      "          30       1.00      0.07      0.12       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       1.00      0.08      0.15        99\n",
      "          33       1.00      0.07      0.13       171\n",
      "          34       0.91      0.10      0.18       102\n",
      "          35       1.00      0.02      0.04        97\n",
      "          36       1.00      0.02      0.04       304\n",
      "          37       1.00      0.04      0.08       527\n",
      "          38       1.00      0.05      0.09       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       1.00      0.02      0.04       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       1.00      0.02      0.04       298\n",
      "          43       0.74      0.68      0.71      3697\n",
      "          44       1.00      0.04      0.08       322\n",
      "          45       0.97      0.06      0.12       464\n",
      "          46       1.00      0.08      0.15        25\n",
      "          47       1.00      0.02      0.05       330\n",
      "          48       1.00      0.04      0.08       154\n",
      "          49       1.00      0.04      0.08       123\n",
      "          50       1.00      0.01      0.02       104\n",
      "          51       1.00      0.01      0.02       169\n",
      "          52       0.89      0.29      0.44      1184\n",
      "          53       1.00      0.07      0.13       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       1.00      0.04      0.07       140\n",
      "          56       0.80      0.24      0.37      1556\n",
      "          57       0.78      0.38      0.51      1850\n",
      "          58       1.00      0.03      0.05       392\n",
      "          59       1.00      0.10      0.18       532\n",
      "          60       1.00      0.12      0.21       197\n",
      "          61       1.00      0.01      0.01       156\n",
      "          62       0.89      0.03      0.07       233\n",
      "          63       1.00      0.01      0.02       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.98      0.09      0.16       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.95      0.10      0.19       370\n",
      "          68       0.75      0.59      0.66      2813\n",
      "          69       1.00      0.02      0.04        49\n",
      "          70       1.00      0.02      0.04        55\n",
      "\n",
      "   micro avg       0.78      0.27      0.40     28022\n",
      "   macro avg       0.83      0.08      0.12     28022\n",
      "weighted avg       0.85      0.27      0.34     28022\n",
      " samples avg       0.50      0.31      0.35     28022\n",
      "\n",
      "t2_2000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.033    |             |\n",
      "| Term Wise Accuracy |    0.967    |             |\n",
      "|      Accuracy      |    0.149    |             |\n",
      "|     Precision      |    0.860    |    0.792    |\n",
      "|       Recall       |    0.092    |    0.279    |\n",
      "|     F1-measure     |    0.146    |    0.412    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.033 |\n",
      "|   Literature Accuracy    | 0.302 |\n",
      "|   Literature Precision   | 0.511 |\n",
      "|     LiteratureRecall     | 0.326 |\n",
      "|   LiteratureF1-measure   | 0.398 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.05       169\n",
      "           1       0.98      0.12      0.21       423\n",
      "           2       1.00      0.01      0.02        82\n",
      "           3       1.00      0.10      0.18        93\n",
      "           4       1.00      0.06      0.12        64\n",
      "           5       1.00      0.07      0.13       129\n",
      "           6       1.00      0.06      0.11        71\n",
      "           7       1.00      0.05      0.09       264\n",
      "           8       1.00      0.04      0.07        28\n",
      "           9       1.00      0.09      0.16       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       1.00      0.03      0.06       139\n",
      "          12       1.00      0.07      0.13       330\n",
      "          13       1.00      0.04      0.08        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       1.00      0.10      0.18        52\n",
      "          16       1.00      0.05      0.10        59\n",
      "          17       0.93      0.18      0.30      1194\n",
      "          18       1.00      0.08      0.15        63\n",
      "          19       1.00      0.04      0.07       282\n",
      "          20       0.80      0.29      0.43      1680\n",
      "          21       1.00      0.03      0.07       116\n",
      "          22       1.00      0.05      0.09       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       1.00      0.04      0.08       254\n",
      "          25       1.00      0.06      0.12       450\n",
      "          26       0.95      0.31      0.47       351\n",
      "          27       1.00      0.02      0.04        48\n",
      "          28       0.78      0.30      0.43      1849\n",
      "          29       0.98      0.15      0.26       548\n",
      "          30       0.88      0.08      0.15       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       1.00      0.08      0.15        99\n",
      "          33       1.00      0.08      0.14       171\n",
      "          34       0.92      0.12      0.21       102\n",
      "          35       1.00      0.03      0.06        97\n",
      "          36       1.00      0.03      0.06       304\n",
      "          37       1.00      0.06      0.11       527\n",
      "          38       1.00      0.07      0.13       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       1.00      0.05      0.09       106\n",
      "          41       1.00      0.03      0.06        35\n",
      "          42       1.00      0.04      0.08       298\n",
      "          43       0.75      0.69      0.72      3697\n",
      "          44       1.00      0.07      0.12       322\n",
      "          45       0.96      0.06      0.11       464\n",
      "          46       1.00      0.12      0.21        25\n",
      "          47       1.00      0.04      0.07       330\n",
      "          48       1.00      0.06      0.11       154\n",
      "          49       1.00      0.05      0.09       123\n",
      "          50       1.00      0.03      0.06       104\n",
      "          51       1.00      0.02      0.05       169\n",
      "          52       0.90      0.31      0.46      1184\n",
      "          53       1.00      0.07      0.13       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       1.00      0.04      0.07       140\n",
      "          56       0.81      0.26      0.39      1556\n",
      "          57       0.77      0.37      0.50      1850\n",
      "          58       1.00      0.05      0.10       392\n",
      "          59       1.00      0.14      0.25       532\n",
      "          60       1.00      0.18      0.30       197\n",
      "          61       1.00      0.01      0.03       156\n",
      "          62       1.00      0.03      0.07       233\n",
      "          63       1.00      0.01      0.02       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.98      0.10      0.18       661\n",
      "          66       1.00      0.01      0.02        80\n",
      "          67       0.93      0.11      0.20       370\n",
      "          68       0.75      0.59      0.66      2813\n",
      "          69       1.00      0.02      0.04        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.79      0.28      0.41     28022\n",
      "   macro avg       0.86      0.09      0.15     28022\n",
      "weighted avg       0.86      0.28      0.36     28022\n",
      " samples avg       0.51      0.33      0.36     28022\n",
      "\n",
      "t2_2000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.034    |             |\n",
      "| Term Wise Accuracy |    0.966    |             |\n",
      "|      Accuracy      |    0.132    |             |\n",
      "|     Precision      |    0.824    |    0.773    |\n",
      "|       Recall       |    0.072    |    0.254    |\n",
      "|     F1-measure     |    0.112    |    0.383    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.034 |\n",
      "|   Literature Accuracy    | 0.280 |\n",
      "|   Literature Precision   | 0.491 |\n",
      "|     LiteratureRecall     | 0.303 |\n",
      "|   LiteratureF1-measure   | 0.375 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.02       169\n",
      "           1       0.95      0.09      0.17       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       1.00      0.02      0.04        93\n",
      "           4       1.00      0.02      0.03        64\n",
      "           5       1.00      0.06      0.12       129\n",
      "           6       1.00      0.04      0.08        71\n",
      "           7       1.00      0.01      0.02       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.02      0.04       158\n",
      "          10       1.00      0.02      0.04        56\n",
      "          11       1.00      0.01      0.01       139\n",
      "          12       1.00      0.03      0.06       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       1.00      0.02      0.04        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.92      0.16      0.28      1194\n",
      "          18       1.00      0.08      0.15        63\n",
      "          19       1.00      0.02      0.04       282\n",
      "          20       0.77      0.21      0.34      1680\n",
      "          21       0.83      0.04      0.08       116\n",
      "          22       1.00      0.02      0.04       257\n",
      "          23       1.00      0.01      0.02       125\n",
      "          24       1.00      0.01      0.02       254\n",
      "          25       1.00      0.03      0.06       450\n",
      "          26       0.96      0.23      0.38       351\n",
      "          27       1.00      0.02      0.04        48\n",
      "          28       0.79      0.28      0.42      1849\n",
      "          29       0.96      0.13      0.23       548\n",
      "          30       1.00      0.04      0.08       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       1.00      0.07      0.13        99\n",
      "          33       1.00      0.04      0.08       171\n",
      "          34       0.91      0.10      0.18       102\n",
      "          35       1.00      0.02      0.04        97\n",
      "          36       1.00      0.02      0.03       304\n",
      "          37       1.00      0.04      0.08       527\n",
      "          38       1.00      0.03      0.07       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       1.00      0.03      0.06        35\n",
      "          42       1.00      0.03      0.06       298\n",
      "          43       0.74      0.67      0.70      3697\n",
      "          44       1.00      0.05      0.10       322\n",
      "          45       1.00      0.03      0.07       464\n",
      "          46       1.00      0.12      0.21        25\n",
      "          47       1.00      0.03      0.06       330\n",
      "          48       1.00      0.07      0.13       154\n",
      "          49       1.00      0.03      0.06       123\n",
      "          50       1.00      0.01      0.02       104\n",
      "          51       1.00      0.03      0.06       169\n",
      "          52       0.90      0.28      0.43      1184\n",
      "          53       1.00      0.03      0.07       176\n",
      "          54       1.00      0.02      0.03        64\n",
      "          55       1.00      0.01      0.03       140\n",
      "          56       0.78      0.24      0.37      1556\n",
      "          57       0.78      0.37      0.50      1850\n",
      "          58       1.00      0.02      0.03       392\n",
      "          59       0.98      0.09      0.17       532\n",
      "          60       1.00      0.09      0.16       197\n",
      "          61       0.67      0.01      0.03       156\n",
      "          62       0.88      0.03      0.06       233\n",
      "          63       1.00      0.03      0.05       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.05      0.09       661\n",
      "          66       1.00      0.04      0.07        80\n",
      "          67       0.94      0.09      0.17       370\n",
      "          68       0.72      0.56      0.63      2813\n",
      "          69       1.00      0.04      0.08        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.77      0.25      0.38     28022\n",
      "   macro avg       0.82      0.07      0.11     28022\n",
      "weighted avg       0.85      0.25      0.33     28022\n",
      " samples avg       0.49      0.30      0.34     28022\n",
      "\n",
      "t1_3000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.028    |             |\n",
      "| Term Wise Accuracy |    0.972    |             |\n",
      "|      Accuracy      |    0.200    |             |\n",
      "|     Precision      |    0.964    |    0.845    |\n",
      "|       Recall       |    0.178    |    0.388    |\n",
      "|     F1-measure     |    0.274    |    0.531    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.028 |\n",
      "|   Literature Accuracy    | 0.397 |\n",
      "|   Literature Precision   | 0.610 |\n",
      "|     LiteratureRecall     | 0.427 |\n",
      "|   LiteratureF1-measure   | 0.502 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.15       169\n",
      "           1       0.98      0.31      0.47       423\n",
      "           2       1.00      0.04      0.07        82\n",
      "           3       1.00      0.11      0.19        93\n",
      "           4       1.00      0.16      0.27        64\n",
      "           5       1.00      0.19      0.31       129\n",
      "           6       1.00      0.17      0.29        71\n",
      "           7       1.00      0.14      0.25       264\n",
      "           8       1.00      0.04      0.07        28\n",
      "           9       1.00      0.20      0.34       158\n",
      "          10       1.00      0.05      0.10        56\n",
      "          11       1.00      0.08      0.15       139\n",
      "          12       1.00      0.12      0.22       330\n",
      "          13       1.00      0.06      0.12        77\n",
      "          14       1.00      0.11      0.20        27\n",
      "          15       1.00      0.17      0.30        52\n",
      "          16       1.00      0.12      0.21        59\n",
      "          17       0.93      0.36      0.51      1194\n",
      "          18       1.00      0.16      0.27        63\n",
      "          19       1.00      0.12      0.22       282\n",
      "          20       0.83      0.43      0.57      1680\n",
      "          21       1.00      0.13      0.23       116\n",
      "          22       1.00      0.12      0.22       257\n",
      "          23       1.00      0.06      0.11       125\n",
      "          24       1.00      0.10      0.18       254\n",
      "          25       1.00      0.15      0.26       450\n",
      "          26       0.98      0.47      0.63       351\n",
      "          27       1.00      0.02      0.04        48\n",
      "          28       0.81      0.46      0.58      1849\n",
      "          29       0.97      0.30      0.46       548\n",
      "          30       0.98      0.21      0.35       285\n",
      "          31       1.00      0.02      0.04        44\n",
      "          32       1.00      0.11      0.20        99\n",
      "          33       1.00      0.20      0.34       171\n",
      "          34       0.96      0.22      0.35       102\n",
      "          35       1.00      0.05      0.10        97\n",
      "          36       1.00      0.18      0.30       304\n",
      "          37       1.00      0.17      0.29       527\n",
      "          38       0.97      0.18      0.31       413\n",
      "          39       1.00      0.10      0.19        68\n",
      "          40       1.00      0.07      0.12       106\n",
      "          41       1.00      0.11      0.21        35\n",
      "          42       1.00      0.10      0.18       298\n",
      "          43       0.79      0.74      0.76      3697\n",
      "          44       0.98      0.14      0.24       322\n",
      "          45       0.99      0.23      0.37       464\n",
      "          46       1.00      0.20      0.33        25\n",
      "          47       0.98      0.16      0.28       330\n",
      "          48       1.00      0.13      0.23       154\n",
      "          49       1.00      0.10      0.18       123\n",
      "          50       1.00      0.05      0.09       104\n",
      "          51       1.00      0.06      0.11       169\n",
      "          52       0.92      0.46      0.61      1184\n",
      "          53       1.00      0.15      0.26       176\n",
      "          54       1.00      0.06      0.12        64\n",
      "          55       1.00      0.08      0.15       140\n",
      "          56       0.85      0.42      0.56      1556\n",
      "          57       0.81      0.48      0.61      1850\n",
      "          58       1.00      0.12      0.22       392\n",
      "          59       0.99      0.31      0.47       532\n",
      "          60       1.00      0.35      0.52       197\n",
      "          61       1.00      0.08      0.15       156\n",
      "          62       1.00      0.09      0.17       233\n",
      "          63       1.00      0.05      0.10       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.99      0.25      0.40       661\n",
      "          66       1.00      0.11      0.20        80\n",
      "          67       0.97      0.23      0.37       370\n",
      "          68       0.79      0.67      0.73      2813\n",
      "          69       1.00      0.10      0.19        49\n",
      "          70       1.00      0.02      0.04        55\n",
      "\n",
      "   micro avg       0.85      0.39      0.53     28022\n",
      "   macro avg       0.96      0.18      0.27     28022\n",
      "weighted avg       0.90      0.39      0.49     28022\n",
      " samples avg       0.61      0.43      0.47     28022\n",
      "\n",
      "t1_3000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.028    |             |\n",
      "| Term Wise Accuracy |    0.972    |             |\n",
      "|      Accuracy      |    0.217    |             |\n",
      "|     Precision      |    0.965    |    0.853    |\n",
      "|       Recall       |    0.201    |    0.401    |\n",
      "|     F1-measure     |    0.309    |    0.546    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.028 |\n",
      "|   Literature Accuracy    | 0.411 |\n",
      "|   Literature Precision   | 0.620 |\n",
      "|     LiteratureRecall     | 0.441 |\n",
      "|   LiteratureF1-measure   | 0.515 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.12      0.22       169\n",
      "           1       0.99      0.34      0.51       423\n",
      "           2       1.00      0.05      0.09        82\n",
      "           3       1.00      0.14      0.25        93\n",
      "           4       1.00      0.14      0.25        64\n",
      "           5       1.00      0.18      0.30       129\n",
      "           6       1.00      0.21      0.35        71\n",
      "           7       1.00      0.19      0.32       264\n",
      "           8       1.00      0.04      0.07        28\n",
      "           9       1.00      0.22      0.36       158\n",
      "          10       1.00      0.07      0.13        56\n",
      "          11       1.00      0.12      0.22       139\n",
      "          12       1.00      0.20      0.33       330\n",
      "          13       1.00      0.12      0.21        77\n",
      "          14       1.00      0.11      0.20        27\n",
      "          15       1.00      0.17      0.30        52\n",
      "          16       1.00      0.12      0.21        59\n",
      "          17       0.90      0.35      0.51      1194\n",
      "          18       1.00      0.17      0.30        63\n",
      "          19       1.00      0.17      0.29       282\n",
      "          20       0.83      0.43      0.56      1680\n",
      "          21       1.00      0.12      0.22       116\n",
      "          22       1.00      0.18      0.30       257\n",
      "          23       1.00      0.07      0.13       125\n",
      "          24       1.00      0.13      0.22       254\n",
      "          25       1.00      0.19      0.31       450\n",
      "          26       0.98      0.51      0.67       351\n",
      "          27       1.00      0.04      0.08        48\n",
      "          28       0.83      0.46      0.59      1849\n",
      "          29       0.98      0.37      0.54       548\n",
      "          30       1.00      0.26      0.41       285\n",
      "          31       1.00      0.02      0.04        44\n",
      "          32       1.00      0.15      0.26        99\n",
      "          33       1.00      0.25      0.39       171\n",
      "          34       0.96      0.25      0.40       102\n",
      "          35       1.00      0.07      0.13        97\n",
      "          36       1.00      0.14      0.25       304\n",
      "          37       1.00      0.19      0.31       527\n",
      "          38       0.97      0.17      0.29       413\n",
      "          39       1.00      0.12      0.21        68\n",
      "          40       1.00      0.12      0.22       106\n",
      "          41       1.00      0.14      0.25        35\n",
      "          42       1.00      0.13      0.23       298\n",
      "          43       0.79      0.74      0.76      3697\n",
      "          44       1.00      0.15      0.26       322\n",
      "          45       0.99      0.22      0.36       464\n",
      "          46       1.00      0.32      0.48        25\n",
      "          47       0.97      0.17      0.29       330\n",
      "          48       1.00      0.17      0.29       154\n",
      "          49       1.00      0.13      0.23       123\n",
      "          50       1.00      0.05      0.09       104\n",
      "          51       1.00      0.12      0.21       169\n",
      "          52       0.93      0.48      0.63      1184\n",
      "          53       1.00      0.19      0.32       176\n",
      "          54       1.00      0.12      0.22        64\n",
      "          55       1.00      0.15      0.26       140\n",
      "          56       0.85      0.42      0.56      1556\n",
      "          57       0.82      0.51      0.63      1850\n",
      "          58       1.00      0.15      0.25       392\n",
      "          59       0.99      0.30      0.46       532\n",
      "          60       1.00      0.43      0.60       197\n",
      "          61       1.00      0.08      0.14       156\n",
      "          62       1.00      0.13      0.23       233\n",
      "          63       1.00      0.05      0.10       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.25      0.40       661\n",
      "          66       1.00      0.14      0.24        80\n",
      "          67       0.97      0.24      0.39       370\n",
      "          68       0.81      0.67      0.73      2813\n",
      "          69       1.00      0.06      0.12        49\n",
      "          70       1.00      0.11      0.20        55\n",
      "\n",
      "   micro avg       0.85      0.40      0.55     28022\n",
      "   macro avg       0.97      0.20      0.31     28022\n",
      "weighted avg       0.90      0.40      0.51     28022\n",
      " samples avg       0.62      0.44      0.48     28022\n",
      "\n",
      "t1_3000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.029    |             |\n",
      "| Term Wise Accuracy |    0.971    |             |\n",
      "|      Accuracy      |    0.193    |             |\n",
      "|     Precision      |    0.951    |    0.845    |\n",
      "|       Recall       |    0.163    |    0.375    |\n",
      "|     F1-measure     |    0.252    |    0.519    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.029 |\n",
      "|   Literature Accuracy    | 0.387 |\n",
      "|   Literature Precision   | 0.606 |\n",
      "|     LiteratureRecall     | 0.415 |\n",
      "|   LiteratureF1-measure   | 0.493 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.14       169\n",
      "           1       0.97      0.30      0.46       423\n",
      "           2       1.00      0.01      0.02        82\n",
      "           3       1.00      0.12      0.21        93\n",
      "           4       1.00      0.11      0.20        64\n",
      "           5       1.00      0.12      0.22       129\n",
      "           6       1.00      0.14      0.25        71\n",
      "           7       1.00      0.17      0.29       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.13      0.23       158\n",
      "          10       1.00      0.05      0.10        56\n",
      "          11       1.00      0.07      0.13       139\n",
      "          12       1.00      0.12      0.22       330\n",
      "          13       1.00      0.05      0.10        77\n",
      "          14       1.00      0.07      0.14        27\n",
      "          15       1.00      0.17      0.30        52\n",
      "          16       1.00      0.03      0.07        59\n",
      "          17       0.90      0.32      0.47      1194\n",
      "          18       1.00      0.13      0.23        63\n",
      "          19       1.00      0.09      0.16       282\n",
      "          20       0.83      0.41      0.55      1680\n",
      "          21       1.00      0.12      0.22       116\n",
      "          22       1.00      0.12      0.22       257\n",
      "          23       1.00      0.05      0.09       125\n",
      "          24       1.00      0.11      0.19       254\n",
      "          25       1.00      0.14      0.24       450\n",
      "          26       0.98      0.42      0.59       351\n",
      "          27       1.00      0.04      0.08        48\n",
      "          28       0.82      0.45      0.58      1849\n",
      "          29       0.97      0.33      0.49       548\n",
      "          30       1.00      0.19      0.32       285\n",
      "          31       1.00      0.02      0.04        44\n",
      "          32       1.00      0.09      0.17        99\n",
      "          33       1.00      0.20      0.34       171\n",
      "          34       0.95      0.18      0.30       102\n",
      "          35       1.00      0.08      0.15        97\n",
      "          36       1.00      0.11      0.19       304\n",
      "          37       1.00      0.18      0.30       527\n",
      "          38       1.00      0.15      0.25       413\n",
      "          39       1.00      0.06      0.11        68\n",
      "          40       1.00      0.10      0.19       106\n",
      "          41       1.00      0.11      0.21        35\n",
      "          42       1.00      0.12      0.21       298\n",
      "          43       0.79      0.74      0.77      3697\n",
      "          44       1.00      0.14      0.24       322\n",
      "          45       1.00      0.20      0.34       464\n",
      "          46       1.00      0.20      0.33        25\n",
      "          47       1.00      0.14      0.25       330\n",
      "          48       1.00      0.12      0.22       154\n",
      "          49       1.00      0.07      0.14       123\n",
      "          50       1.00      0.03      0.06       104\n",
      "          51       1.00      0.05      0.10       169\n",
      "          52       0.92      0.44      0.60      1184\n",
      "          53       1.00      0.14      0.25       176\n",
      "          54       1.00      0.03      0.06        64\n",
      "          55       1.00      0.06      0.11       140\n",
      "          56       0.84      0.41      0.55      1556\n",
      "          57       0.84      0.48      0.61      1850\n",
      "          58       1.00      0.11      0.19       392\n",
      "          59       0.98      0.24      0.39       532\n",
      "          60       1.00      0.34      0.50       197\n",
      "          61       1.00      0.08      0.15       156\n",
      "          62       1.00      0.13      0.23       233\n",
      "          63       1.00      0.05      0.09       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.99      0.22      0.36       661\n",
      "          66       1.00      0.09      0.16        80\n",
      "          67       0.97      0.19      0.31       370\n",
      "          68       0.79      0.66      0.72      2813\n",
      "          69       1.00      0.06      0.12        49\n",
      "          70       1.00      0.05      0.10        55\n",
      "\n",
      "   micro avg       0.84      0.37      0.52     28022\n",
      "   macro avg       0.95      0.16      0.25     28022\n",
      "weighted avg       0.90      0.37      0.48     28022\n",
      " samples avg       0.61      0.42      0.46     28022\n",
      "\n",
      "t2_3000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.028    |             |\n",
      "| Term Wise Accuracy |    0.972    |             |\n",
      "|      Accuracy      |    0.209    |             |\n",
      "|     Precision      |    0.965    |    0.852    |\n",
      "|       Recall       |    0.181    |    0.389    |\n",
      "|     F1-measure     |    0.280    |    0.534    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.028 |\n",
      "|   Literature Accuracy    | 0.404 |\n",
      "|   Literature Precision   | 0.622 |\n",
      "|     LiteratureRecall     | 0.434 |\n",
      "|   LiteratureF1-measure   | 0.511 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.11      0.20       169\n",
      "           1       0.97      0.33      0.49       423\n",
      "           2       1.00      0.07      0.14        82\n",
      "           3       1.00      0.15      0.26        93\n",
      "           4       1.00      0.12      0.22        64\n",
      "           5       1.00      0.16      0.28       129\n",
      "           6       1.00      0.24      0.39        71\n",
      "           7       1.00      0.12      0.22       264\n",
      "           8       1.00      0.04      0.07        28\n",
      "           9       1.00      0.22      0.35       158\n",
      "          10       1.00      0.05      0.10        56\n",
      "          11       1.00      0.09      0.17       139\n",
      "          12       1.00      0.15      0.26       330\n",
      "          13       1.00      0.05      0.10        77\n",
      "          14       1.00      0.19      0.31        27\n",
      "          15       1.00      0.12      0.21        52\n",
      "          16       1.00      0.08      0.16        59\n",
      "          17       0.92      0.38      0.54      1194\n",
      "          18       1.00      0.14      0.25        63\n",
      "          19       1.00      0.11      0.19       282\n",
      "          20       0.84      0.43      0.57      1680\n",
      "          21       1.00      0.11      0.20       116\n",
      "          22       1.00      0.12      0.21       257\n",
      "          23       1.00      0.07      0.13       125\n",
      "          24       1.00      0.07      0.13       254\n",
      "          25       0.99      0.17      0.29       450\n",
      "          26       0.96      0.46      0.62       351\n",
      "          27       1.00      0.06      0.12        48\n",
      "          28       0.82      0.43      0.57      1849\n",
      "          29       0.99      0.32      0.49       548\n",
      "          30       0.99      0.23      0.38       285\n",
      "          31       1.00      0.02      0.04        44\n",
      "          32       1.00      0.15      0.26        99\n",
      "          33       1.00      0.25      0.39       171\n",
      "          34       0.96      0.25      0.39       102\n",
      "          35       1.00      0.06      0.12        97\n",
      "          36       0.98      0.15      0.26       304\n",
      "          37       1.00      0.17      0.29       527\n",
      "          38       0.99      0.16      0.27       413\n",
      "          39       1.00      0.03      0.06        68\n",
      "          40       1.00      0.10      0.19       106\n",
      "          41       1.00      0.17      0.29        35\n",
      "          42       1.00      0.11      0.19       298\n",
      "          43       0.79      0.74      0.76      3697\n",
      "          44       1.00      0.15      0.25       322\n",
      "          45       0.98      0.19      0.31       464\n",
      "          46       1.00      0.24      0.39        25\n",
      "          47       1.00      0.15      0.26       330\n",
      "          48       1.00      0.19      0.32       154\n",
      "          49       1.00      0.07      0.14       123\n",
      "          50       1.00      0.06      0.11       104\n",
      "          51       1.00      0.11      0.20       169\n",
      "          52       0.94      0.48      0.63      1184\n",
      "          53       1.00      0.15      0.26       176\n",
      "          54       1.00      0.06      0.12        64\n",
      "          55       1.00      0.12      0.22       140\n",
      "          56       0.84      0.39      0.53      1556\n",
      "          57       0.84      0.51      0.64      1850\n",
      "          58       1.00      0.11      0.19       392\n",
      "          59       0.98      0.30      0.46       532\n",
      "          60       1.00      0.30      0.46       197\n",
      "          61       1.00      0.06      0.11       156\n",
      "          62       1.00      0.11      0.19       233\n",
      "          63       1.00      0.06      0.12       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.99      0.23      0.37       661\n",
      "          66       1.00      0.06      0.12        80\n",
      "          67       0.96      0.22      0.35       370\n",
      "          68       0.81      0.67      0.74      2813\n",
      "          69       1.00      0.10      0.19        49\n",
      "          70       1.00      0.04      0.07        55\n",
      "\n",
      "   micro avg       0.85      0.39      0.53     28022\n",
      "   macro avg       0.97      0.18      0.28     28022\n",
      "weighted avg       0.90      0.39      0.49     28022\n",
      " samples avg       0.62      0.43      0.47     28022\n",
      "\n",
      "t2_3000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.028    |             |\n",
      "| Term Wise Accuracy |    0.972    |             |\n",
      "|      Accuracy      |    0.223    |             |\n",
      "|     Precision      |    0.965    |    0.851    |\n",
      "|       Recall       |    0.200    |    0.402    |\n",
      "|     F1-measure     |    0.307    |    0.547    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.028 |\n",
      "|   Literature Accuracy    | 0.419 |\n",
      "|   Literature Precision   | 0.634 |\n",
      "|     LiteratureRecall     | 0.449 |\n",
      "|   LiteratureF1-measure   | 0.525 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.15      0.26       169\n",
      "           1       0.98      0.30      0.46       423\n",
      "           2       1.00      0.06      0.11        82\n",
      "           3       1.00      0.14      0.25        93\n",
      "           4       1.00      0.16      0.27        64\n",
      "           5       1.00      0.19      0.31       129\n",
      "           6       1.00      0.25      0.40        71\n",
      "           7       1.00      0.14      0.25       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       1.00      0.17      0.29       158\n",
      "          10       1.00      0.09      0.16        56\n",
      "          11       1.00      0.12      0.22       139\n",
      "          12       1.00      0.19      0.32       330\n",
      "          13       1.00      0.06      0.12        77\n",
      "          14       1.00      0.11      0.20        27\n",
      "          15       1.00      0.17      0.30        52\n",
      "          16       1.00      0.10      0.18        59\n",
      "          17       0.92      0.37      0.52      1194\n",
      "          18       1.00      0.17      0.30        63\n",
      "          19       1.00      0.14      0.25       282\n",
      "          20       0.83      0.44      0.57      1680\n",
      "          21       1.00      0.10      0.19       116\n",
      "          22       1.00      0.14      0.24       257\n",
      "          23       1.00      0.06      0.12       125\n",
      "          24       1.00      0.11      0.20       254\n",
      "          25       1.00      0.18      0.30       450\n",
      "          26       0.98      0.48      0.65       351\n",
      "          27       1.00      0.12      0.22        48\n",
      "          28       0.82      0.47      0.60      1849\n",
      "          29       0.97      0.36      0.53       548\n",
      "          30       0.99      0.29      0.45       285\n",
      "          31       1.00      0.05      0.09        44\n",
      "          32       1.00      0.13      0.23        99\n",
      "          33       1.00      0.29      0.45       171\n",
      "          34       0.96      0.26      0.42       102\n",
      "          35       1.00      0.07      0.13        97\n",
      "          36       1.00      0.14      0.25       304\n",
      "          37       1.00      0.19      0.31       527\n",
      "          38       0.97      0.17      0.30       413\n",
      "          39       1.00      0.10      0.19        68\n",
      "          40       1.00      0.09      0.17       106\n",
      "          41       1.00      0.14      0.25        35\n",
      "          42       1.00      0.14      0.24       298\n",
      "          43       0.79      0.75      0.77      3697\n",
      "          44       1.00      0.17      0.29       322\n",
      "          45       0.97      0.19      0.31       464\n",
      "          46       1.00      0.32      0.48        25\n",
      "          47       1.00      0.18      0.31       330\n",
      "          48       1.00      0.19      0.32       154\n",
      "          49       1.00      0.10      0.18       123\n",
      "          50       1.00      0.08      0.14       104\n",
      "          51       1.00      0.14      0.24       169\n",
      "          52       0.95      0.49      0.64      1184\n",
      "          53       1.00      0.19      0.32       176\n",
      "          54       1.00      0.11      0.20        64\n",
      "          55       1.00      0.14      0.24       140\n",
      "          56       0.85      0.43      0.57      1556\n",
      "          57       0.84      0.52      0.64      1850\n",
      "          58       1.00      0.12      0.21       392\n",
      "          59       0.98      0.33      0.49       532\n",
      "          60       0.99      0.36      0.52       197\n",
      "          61       1.00      0.12      0.22       156\n",
      "          62       0.97      0.12      0.22       233\n",
      "          63       1.00      0.08      0.15       111\n",
      "          64       1.00      0.04      0.07        28\n",
      "          65       0.98      0.22      0.36       661\n",
      "          66       1.00      0.10      0.18        80\n",
      "          67       0.97      0.25      0.40       370\n",
      "          68       0.79      0.67      0.72      2813\n",
      "          69       1.00      0.10      0.19        49\n",
      "          70       1.00      0.09      0.17        55\n",
      "\n",
      "   micro avg       0.85      0.40      0.55     28022\n",
      "   macro avg       0.96      0.20      0.31     28022\n",
      "weighted avg       0.90      0.40      0.51     28022\n",
      " samples avg       0.63      0.45      0.49     28022\n",
      "\n",
      "t2_3000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.029    |             |\n",
      "| Term Wise Accuracy |    0.971    |             |\n",
      "|      Accuracy      |    0.199    |             |\n",
      "|     Precision      |    0.962    |    0.840    |\n",
      "|       Recall       |    0.162    |    0.376    |\n",
      "|     F1-measure     |    0.252    |    0.520    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.029 |\n",
      "|   Literature Accuracy    | 0.392 |\n",
      "|   Literature Precision   | 0.612 |\n",
      "|     LiteratureRecall     | 0.421 |\n",
      "|   LiteratureF1-measure   | 0.499 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.14       169\n",
      "           1       0.98      0.24      0.39       423\n",
      "           2       1.00      0.05      0.09        82\n",
      "           3       1.00      0.11      0.19        93\n",
      "           4       1.00      0.12      0.22        64\n",
      "           5       1.00      0.14      0.24       129\n",
      "           6       1.00      0.18      0.31        71\n",
      "           7       1.00      0.12      0.21       264\n",
      "           8       1.00      0.04      0.07        28\n",
      "           9       1.00      0.11      0.19       158\n",
      "          10       1.00      0.04      0.07        56\n",
      "          11       1.00      0.09      0.16       139\n",
      "          12       1.00      0.13      0.23       330\n",
      "          13       1.00      0.06      0.12        77\n",
      "          14       1.00      0.04      0.07        27\n",
      "          15       1.00      0.10      0.18        52\n",
      "          16       1.00      0.14      0.24        59\n",
      "          17       0.91      0.33      0.49      1194\n",
      "          18       1.00      0.10      0.17        63\n",
      "          19       1.00      0.11      0.19       282\n",
      "          20       0.84      0.42      0.56      1680\n",
      "          21       0.92      0.10      0.19       116\n",
      "          22       1.00      0.09      0.16       257\n",
      "          23       1.00      0.05      0.09       125\n",
      "          24       1.00      0.09      0.17       254\n",
      "          25       1.00      0.17      0.29       450\n",
      "          26       0.98      0.44      0.61       351\n",
      "          27       1.00      0.06      0.12        48\n",
      "          28       0.82      0.43      0.56      1849\n",
      "          29       0.99      0.32      0.48       548\n",
      "          30       1.00      0.22      0.37       285\n",
      "          31       1.00      0.02      0.04        44\n",
      "          32       1.00      0.08      0.15        99\n",
      "          33       1.00      0.21      0.35       171\n",
      "          34       0.95      0.19      0.31       102\n",
      "          35       1.00      0.06      0.12        97\n",
      "          36       0.98      0.13      0.23       304\n",
      "          37       1.00      0.16      0.28       527\n",
      "          38       1.00      0.14      0.25       413\n",
      "          39       1.00      0.03      0.06        68\n",
      "          40       1.00      0.07      0.12       106\n",
      "          41       1.00      0.11      0.21        35\n",
      "          42       1.00      0.10      0.18       298\n",
      "          43       0.79      0.74      0.76      3697\n",
      "          44       1.00      0.13      0.24       322\n",
      "          45       0.99      0.16      0.27       464\n",
      "          46       1.00      0.16      0.28        25\n",
      "          47       1.00      0.18      0.31       330\n",
      "          48       1.00      0.10      0.19       154\n",
      "          49       1.00      0.06      0.11       123\n",
      "          50       1.00      0.06      0.11       104\n",
      "          51       1.00      0.10      0.18       169\n",
      "          52       0.92      0.44      0.60      1184\n",
      "          53       1.00      0.14      0.25       176\n",
      "          54       1.00      0.08      0.14        64\n",
      "          55       1.00      0.11      0.21       140\n",
      "          56       0.83      0.40      0.54      1556\n",
      "          57       0.82      0.51      0.63      1850\n",
      "          58       1.00      0.13      0.23       392\n",
      "          59       0.99      0.25      0.40       532\n",
      "          60       1.00      0.30      0.47       197\n",
      "          61       0.89      0.05      0.10       156\n",
      "          62       0.96      0.10      0.19       233\n",
      "          63       1.00      0.07      0.13       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.97      0.19      0.31       661\n",
      "          66       1.00      0.09      0.16        80\n",
      "          67       0.96      0.20      0.33       370\n",
      "          68       0.79      0.67      0.73      2813\n",
      "          69       1.00      0.04      0.08        49\n",
      "          70       1.00      0.04      0.07        55\n",
      "\n",
      "   micro avg       0.84      0.38      0.52     28022\n",
      "   macro avg       0.96      0.16      0.25     28022\n",
      "weighted avg       0.90      0.38      0.48     28022\n",
      " samples avg       0.61      0.42      0.46     28022\n",
      "\n",
      "t1_4000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.021    |             |\n",
      "| Term Wise Accuracy |    0.979    |             |\n",
      "|      Accuracy      |    0.324    |             |\n",
      "|     Precision      |    0.985    |    0.901    |\n",
      "|       Recall       |    0.390    |    0.562    |\n",
      "|     F1-measure     |    0.541    |    0.692    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.021 |\n",
      "|   Literature Accuracy    | 0.556 |\n",
      "|   Literature Precision   | 0.739 |\n",
      "|     LiteratureRecall     | 0.594 |\n",
      "|   LiteratureF1-measure   | 0.658 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.31      0.48       169\n",
      "           1       0.98      0.56      0.71       423\n",
      "           2       1.00      0.24      0.39        82\n",
      "           3       1.00      0.38      0.55        93\n",
      "           4       1.00      0.39      0.56        64\n",
      "           5       1.00      0.37      0.54       129\n",
      "           6       1.00      0.38      0.55        71\n",
      "           7       1.00      0.40      0.57       264\n",
      "           8       1.00      0.18      0.30        28\n",
      "           9       1.00      0.40      0.57       158\n",
      "          10       1.00      0.23      0.38        56\n",
      "          11       1.00      0.24      0.39       139\n",
      "          12       1.00      0.39      0.57       330\n",
      "          13       1.00      0.35      0.52        77\n",
      "          14       1.00      0.22      0.36        27\n",
      "          15       1.00      0.35      0.51        52\n",
      "          16       1.00      0.36      0.53        59\n",
      "          17       0.94      0.55      0.69      1194\n",
      "          18       1.00      0.38      0.55        63\n",
      "          19       1.00      0.37      0.54       282\n",
      "          20       0.88      0.60      0.72      1680\n",
      "          21       1.00      0.34      0.50       116\n",
      "          22       1.00      0.39      0.56       257\n",
      "          23       1.00      0.29      0.45       125\n",
      "          24       1.00      0.33      0.50       254\n",
      "          25       1.00      0.44      0.62       450\n",
      "          26       0.98      0.64      0.77       351\n",
      "          27       1.00      0.21      0.34        48\n",
      "          28       0.87      0.61      0.72      1849\n",
      "          29       0.98      0.54      0.69       548\n",
      "          30       1.00      0.48      0.65       285\n",
      "          31       1.00      0.14      0.24        44\n",
      "          32       1.00      0.33      0.50        99\n",
      "          33       1.00      0.55      0.71       171\n",
      "          34       0.98      0.55      0.70       102\n",
      "          35       1.00      0.21      0.34        97\n",
      "          36       1.00      0.41      0.58       304\n",
      "          37       1.00      0.45      0.62       527\n",
      "          38       0.98      0.37      0.54       413\n",
      "          39       1.00      0.37      0.54        68\n",
      "          40       1.00      0.23      0.37       106\n",
      "          41       1.00      0.34      0.51        35\n",
      "          42       0.99      0.40      0.57       298\n",
      "          43       0.83      0.80      0.81      3697\n",
      "          44       1.00      0.41      0.58       322\n",
      "          45       1.00      0.45      0.62       464\n",
      "          46       1.00      0.40      0.57        25\n",
      "          47       0.98      0.38      0.54       330\n",
      "          48       1.00      0.38      0.55       154\n",
      "          49       1.00      0.25      0.40       123\n",
      "          50       1.00      0.21      0.35       104\n",
      "          51       1.00      0.32      0.48       169\n",
      "          52       0.95      0.61      0.75      1184\n",
      "          53       1.00      0.31      0.48       176\n",
      "          54       1.00      0.31      0.48        64\n",
      "          55       1.00      0.29      0.44       140\n",
      "          56       0.89      0.59      0.71      1556\n",
      "          57       0.87      0.66      0.75      1850\n",
      "          58       1.00      0.35      0.52       392\n",
      "          59       1.00      0.54      0.70       532\n",
      "          60       1.00      0.61      0.76       197\n",
      "          61       1.00      0.30      0.46       156\n",
      "          62       1.00      0.33      0.50       233\n",
      "          63       1.00      0.25      0.40       111\n",
      "          64       1.00      0.04      0.07        28\n",
      "          65       0.99      0.44      0.61       661\n",
      "          66       1.00      0.38      0.55        80\n",
      "          67       0.98      0.45      0.62       370\n",
      "          68       0.84      0.75      0.79      2813\n",
      "          69       1.00      0.31      0.47        49\n",
      "          70       1.00      0.29      0.45        55\n",
      "\n",
      "   micro avg       0.90      0.56      0.69     28022\n",
      "   macro avg       0.98      0.39      0.54     28022\n",
      "weighted avg       0.92      0.56      0.68     28022\n",
      " samples avg       0.74      0.59      0.63     28022\n",
      "\n",
      "t1_4000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.021    |             |\n",
      "| Term Wise Accuracy |    0.979    |             |\n",
      "|      Accuracy      |    0.339    |             |\n",
      "|     Precision      |    0.985    |    0.904    |\n",
      "|       Recall       |    0.402    |    0.567    |\n",
      "|     F1-measure     |    0.552    |    0.697    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.021 |\n",
      "|   Literature Accuracy    | 0.561 |\n",
      "|   Literature Precision   | 0.738 |\n",
      "|     LiteratureRecall     | 0.595 |\n",
      "|   LiteratureF1-measure   | 0.659 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.35      0.52       169\n",
      "           1       0.98      0.53      0.69       423\n",
      "           2       1.00      0.13      0.24        82\n",
      "           3       1.00      0.37      0.54        93\n",
      "           4       1.00      0.52      0.68        64\n",
      "           5       1.00      0.40      0.57       129\n",
      "           6       1.00      0.49      0.66        71\n",
      "           7       1.00      0.45      0.62       264\n",
      "           8       1.00      0.14      0.25        28\n",
      "           9       1.00      0.47      0.64       158\n",
      "          10       1.00      0.29      0.44        56\n",
      "          11       1.00      0.24      0.38       139\n",
      "          12       1.00      0.40      0.57       330\n",
      "          13       1.00      0.35      0.52        77\n",
      "          14       1.00      0.15      0.26        27\n",
      "          15       1.00      0.33      0.49        52\n",
      "          16       1.00      0.44      0.61        59\n",
      "          17       0.95      0.57      0.71      1194\n",
      "          18       1.00      0.37      0.53        63\n",
      "          19       1.00      0.39      0.56       282\n",
      "          20       0.87      0.59      0.70      1680\n",
      "          21       1.00      0.34      0.50       116\n",
      "          22       1.00      0.39      0.56       257\n",
      "          23       1.00      0.28      0.44       125\n",
      "          24       1.00      0.35      0.51       254\n",
      "          25       1.00      0.44      0.62       450\n",
      "          26       0.98      0.70      0.82       351\n",
      "          27       1.00      0.27      0.43        48\n",
      "          28       0.85      0.60      0.71      1849\n",
      "          29       0.99      0.55      0.70       548\n",
      "          30       0.99      0.45      0.62       285\n",
      "          31       1.00      0.11      0.20        44\n",
      "          32       1.00      0.35      0.52        99\n",
      "          33       1.00      0.56      0.71       171\n",
      "          34       0.98      0.57      0.72       102\n",
      "          35       1.00      0.24      0.38        97\n",
      "          36       0.99      0.37      0.54       304\n",
      "          37       1.00      0.44      0.61       527\n",
      "          38       0.99      0.41      0.58       413\n",
      "          39       1.00      0.41      0.58        68\n",
      "          40       1.00      0.37      0.54       106\n",
      "          41       1.00      0.34      0.51        35\n",
      "          42       1.00      0.34      0.51       298\n",
      "          43       0.83      0.80      0.82      3697\n",
      "          44       1.00      0.41      0.58       322\n",
      "          45       1.00      0.46      0.63       464\n",
      "          46       1.00      0.40      0.57        25\n",
      "          47       0.99      0.41      0.58       330\n",
      "          48       1.00      0.39      0.56       154\n",
      "          49       1.00      0.26      0.41       123\n",
      "          50       1.00      0.23      0.38       104\n",
      "          51       1.00      0.38      0.55       169\n",
      "          52       0.95      0.65      0.77      1184\n",
      "          53       1.00      0.34      0.51       176\n",
      "          54       1.00      0.38      0.55        64\n",
      "          55       1.00      0.31      0.47       140\n",
      "          56       0.89      0.58      0.70      1556\n",
      "          57       0.88      0.65      0.75      1850\n",
      "          58       0.99      0.40      0.57       392\n",
      "          59       0.99      0.55      0.71       532\n",
      "          60       1.00      0.64      0.78       197\n",
      "          61       1.00      0.31      0.48       156\n",
      "          62       1.00      0.33      0.50       233\n",
      "          63       1.00      0.23      0.37       111\n",
      "          64       1.00      0.04      0.07        28\n",
      "          65       0.99      0.44      0.61       661\n",
      "          66       1.00      0.33      0.49        80\n",
      "          67       0.98      0.48      0.64       370\n",
      "          68       0.85      0.77      0.81      2813\n",
      "          69       1.00      0.33      0.49        49\n",
      "          70       1.00      0.25      0.41        55\n",
      "\n",
      "   micro avg       0.90      0.57      0.70     28022\n",
      "   macro avg       0.98      0.40      0.55     28022\n",
      "weighted avg       0.92      0.57      0.68     28022\n",
      " samples avg       0.74      0.60      0.63     28022\n",
      "\n",
      "t1_4000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.021    |             |\n",
      "| Term Wise Accuracy |    0.979    |             |\n",
      "|      Accuracy      |    0.311    |             |\n",
      "|     Precision      |    0.970    |    0.899    |\n",
      "|       Recall       |    0.364    |    0.551    |\n",
      "|     F1-measure     |    0.509    |    0.683    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.021 |\n",
      "|   Literature Accuracy    | 0.546 |\n",
      "|   Literature Precision   | 0.735 |\n",
      "|     LiteratureRecall     | 0.581 |\n",
      "|   LiteratureF1-measure   | 0.649 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.29      0.45       169\n",
      "           1       0.98      0.52      0.68       423\n",
      "           2       1.00      0.12      0.22        82\n",
      "           3       1.00      0.31      0.48        93\n",
      "           4       1.00      0.34      0.51        64\n",
      "           5       1.00      0.33      0.50       129\n",
      "           6       1.00      0.38      0.55        71\n",
      "           7       1.00      0.36      0.53       264\n",
      "           8       1.00      0.07      0.13        28\n",
      "           9       1.00      0.45      0.62       158\n",
      "          10       1.00      0.23      0.38        56\n",
      "          11       1.00      0.17      0.29       139\n",
      "          12       1.00      0.37      0.54       330\n",
      "          13       1.00      0.23      0.38        77\n",
      "          14       1.00      0.19      0.31        27\n",
      "          15       1.00      0.35      0.51        52\n",
      "          16       1.00      0.34      0.51        59\n",
      "          17       0.93      0.51      0.66      1194\n",
      "          18       1.00      0.25      0.41        63\n",
      "          19       1.00      0.37      0.54       282\n",
      "          20       0.88      0.61      0.72      1680\n",
      "          21       1.00      0.28      0.44       116\n",
      "          22       1.00      0.35      0.51       257\n",
      "          23       1.00      0.15      0.26       125\n",
      "          24       1.00      0.33      0.49       254\n",
      "          25       1.00      0.39      0.56       450\n",
      "          26       0.99      0.66      0.79       351\n",
      "          27       1.00      0.27      0.43        48\n",
      "          28       0.86      0.62      0.72      1849\n",
      "          29       0.98      0.53      0.69       548\n",
      "          30       1.00      0.44      0.61       285\n",
      "          31       1.00      0.09      0.17        44\n",
      "          32       1.00      0.28      0.44        99\n",
      "          33       1.00      0.49      0.65       171\n",
      "          34       0.98      0.53      0.69       102\n",
      "          35       1.00      0.21      0.34        97\n",
      "          36       0.99      0.41      0.58       304\n",
      "          37       1.00      0.41      0.59       527\n",
      "          38       0.99      0.38      0.55       413\n",
      "          39       1.00      0.29      0.45        68\n",
      "          40       1.00      0.25      0.41       106\n",
      "          41       1.00      0.31      0.48        35\n",
      "          42       0.99      0.34      0.50       298\n",
      "          43       0.83      0.80      0.81      3697\n",
      "          44       1.00      0.39      0.57       322\n",
      "          45       0.99      0.43      0.60       464\n",
      "          46       1.00      0.44      0.61        25\n",
      "          47       1.00      0.38      0.55       330\n",
      "          48       1.00      0.35      0.52       154\n",
      "          49       1.00      0.22      0.36       123\n",
      "          50       1.00      0.23      0.38       104\n",
      "          51       1.00      0.30      0.46       169\n",
      "          52       0.96      0.60      0.74      1184\n",
      "          53       1.00      0.32      0.48       176\n",
      "          54       1.00      0.22      0.36        64\n",
      "          55       1.00      0.25      0.40       140\n",
      "          56       0.89      0.58      0.70      1556\n",
      "          57       0.87      0.64      0.74      1850\n",
      "          58       1.00      0.34      0.51       392\n",
      "          59       1.00      0.54      0.70       532\n",
      "          60       0.98      0.59      0.74       197\n",
      "          61       0.98      0.26      0.41       156\n",
      "          62       1.00      0.36      0.53       233\n",
      "          63       1.00      0.18      0.31       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.49      0.65       661\n",
      "          66       1.00      0.24      0.38        80\n",
      "          67       0.98      0.44      0.60       370\n",
      "          68       0.84      0.76      0.80      2813\n",
      "          69       1.00      0.29      0.44        49\n",
      "          70       1.00      0.36      0.53        55\n",
      "\n",
      "   micro avg       0.90      0.55      0.68     28022\n",
      "   macro avg       0.97      0.36      0.51     28022\n",
      "weighted avg       0.92      0.55      0.66     28022\n",
      " samples avg       0.74      0.58      0.62     28022\n",
      "\n",
      "t2_4000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.021    |             |\n",
      "| Term Wise Accuracy |    0.979    |             |\n",
      "|      Accuracy      |    0.334    |             |\n",
      "|     Precision      |    0.984    |    0.904    |\n",
      "|       Recall       |    0.387    |    0.564    |\n",
      "|     F1-measure     |    0.538    |    0.694    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.021 |\n",
      "|   Literature Accuracy    | 0.563 |\n",
      "|   Literature Precision   | 0.749 |\n",
      "|     LiteratureRecall     | 0.598 |\n",
      "|   LiteratureF1-measure   | 0.665 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.36      0.53       169\n",
      "           1       0.98      0.55      0.70       423\n",
      "           2       1.00      0.16      0.27        82\n",
      "           3       1.00      0.32      0.49        93\n",
      "           4       1.00      0.44      0.61        64\n",
      "           5       1.00      0.40      0.57       129\n",
      "           6       1.00      0.52      0.69        71\n",
      "           7       0.99      0.34      0.51       264\n",
      "           8       1.00      0.18      0.30        28\n",
      "           9       1.00      0.44      0.61       158\n",
      "          10       1.00      0.25      0.40        56\n",
      "          11       1.00      0.22      0.36       139\n",
      "          12       1.00      0.35      0.52       330\n",
      "          13       1.00      0.30      0.46        77\n",
      "          14       1.00      0.26      0.41        27\n",
      "          15       1.00      0.29      0.45        52\n",
      "          16       1.00      0.25      0.41        59\n",
      "          17       0.93      0.56      0.70      1194\n",
      "          18       1.00      0.32      0.48        63\n",
      "          19       1.00      0.35      0.52       282\n",
      "          20       0.90      0.59      0.71      1680\n",
      "          21       0.97      0.33      0.49       116\n",
      "          22       1.00      0.33      0.50       257\n",
      "          23       1.00      0.25      0.40       125\n",
      "          24       1.00      0.43      0.60       254\n",
      "          25       1.00      0.44      0.61       450\n",
      "          26       0.98      0.69      0.81       351\n",
      "          27       1.00      0.31      0.48        48\n",
      "          28       0.85      0.62      0.72      1849\n",
      "          29       0.99      0.55      0.70       548\n",
      "          30       1.00      0.47      0.64       285\n",
      "          31       1.00      0.09      0.17        44\n",
      "          32       1.00      0.26      0.42        99\n",
      "          33       1.00      0.57      0.72       171\n",
      "          34       0.98      0.42      0.59       102\n",
      "          35       1.00      0.24      0.38        97\n",
      "          36       0.99      0.39      0.56       304\n",
      "          37       1.00      0.42      0.59       527\n",
      "          38       0.99      0.38      0.54       413\n",
      "          39       1.00      0.34      0.51        68\n",
      "          40       1.00      0.27      0.43       106\n",
      "          41       1.00      0.37      0.54        35\n",
      "          42       0.99      0.35      0.52       298\n",
      "          43       0.84      0.82      0.83      3697\n",
      "          44       1.00      0.36      0.53       322\n",
      "          45       0.98      0.42      0.59       464\n",
      "          46       1.00      0.44      0.61        25\n",
      "          47       1.00      0.41      0.58       330\n",
      "          48       1.00      0.46      0.63       154\n",
      "          49       1.00      0.22      0.36       123\n",
      "          50       1.00      0.23      0.38       104\n",
      "          51       1.00      0.31      0.47       169\n",
      "          52       0.97      0.61      0.75      1184\n",
      "          53       1.00      0.36      0.53       176\n",
      "          54       1.00      0.27      0.42        64\n",
      "          55       1.00      0.31      0.47       140\n",
      "          56       0.90      0.60      0.72      1556\n",
      "          57       0.89      0.67      0.77      1850\n",
      "          58       0.99      0.33      0.50       392\n",
      "          59       0.99      0.51      0.68       532\n",
      "          60       1.00      0.56      0.72       197\n",
      "          61       0.98      0.29      0.45       156\n",
      "          62       0.99      0.34      0.50       233\n",
      "          63       1.00      0.20      0.33       111\n",
      "          64       1.00      0.18      0.30        28\n",
      "          65       0.98      0.47      0.64       661\n",
      "          66       1.00      0.28      0.43        80\n",
      "          67       0.98      0.45      0.61       370\n",
      "          68       0.84      0.76      0.80      2813\n",
      "          69       1.00      0.37      0.54        49\n",
      "          70       1.00      0.33      0.49        55\n",
      "\n",
      "   micro avg       0.90      0.56      0.69     28022\n",
      "   macro avg       0.98      0.39      0.54     28022\n",
      "weighted avg       0.93      0.56      0.68     28022\n",
      " samples avg       0.75      0.60      0.63     28022\n",
      "\n",
      "t2_4000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.020    |             |\n",
      "| Term Wise Accuracy |    0.980    |             |\n",
      "|      Accuracy      |    0.351    |             |\n",
      "|     Precision      |    0.985    |    0.906    |\n",
      "|       Recall       |    0.413    |    0.569    |\n",
      "|     F1-measure     |    0.567    |    0.699    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.020 |\n",
      "|   Literature Accuracy    | 0.570 |\n",
      "|   Literature Precision   | 0.748 |\n",
      "|     LiteratureRecall     | 0.603 |\n",
      "|   LiteratureF1-measure   | 0.668 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.41      0.58       169\n",
      "           1       0.98      0.59      0.73       423\n",
      "           2       1.00      0.21      0.34        82\n",
      "           3       1.00      0.43      0.60        93\n",
      "           4       1.00      0.47      0.64        64\n",
      "           5       1.00      0.50      0.66       129\n",
      "           6       1.00      0.55      0.71        71\n",
      "           7       1.00      0.38      0.55       264\n",
      "           8       1.00      0.25      0.40        28\n",
      "           9       0.97      0.44      0.60       158\n",
      "          10       1.00      0.27      0.42        56\n",
      "          11       1.00      0.28      0.44       139\n",
      "          12       1.00      0.41      0.58       330\n",
      "          13       1.00      0.39      0.56        77\n",
      "          14       1.00      0.26      0.41        27\n",
      "          15       1.00      0.40      0.58        52\n",
      "          16       1.00      0.37      0.54        59\n",
      "          17       0.94      0.58      0.72      1194\n",
      "          18       1.00      0.38      0.55        63\n",
      "          19       1.00      0.32      0.49       282\n",
      "          20       0.87      0.59      0.71      1680\n",
      "          21       1.00      0.38      0.55       116\n",
      "          22       1.00      0.36      0.53       257\n",
      "          23       1.00      0.26      0.42       125\n",
      "          24       1.00      0.36      0.53       254\n",
      "          25       1.00      0.45      0.62       450\n",
      "          26       0.98      0.68      0.81       351\n",
      "          27       1.00      0.42      0.59        48\n",
      "          28       0.86      0.60      0.71      1849\n",
      "          29       0.98      0.53      0.69       548\n",
      "          30       1.00      0.49      0.66       285\n",
      "          31       1.00      0.14      0.24        44\n",
      "          32       1.00      0.31      0.48        99\n",
      "          33       1.00      0.59      0.74       171\n",
      "          34       0.98      0.46      0.63       102\n",
      "          35       1.00      0.27      0.42        97\n",
      "          36       0.99      0.39      0.56       304\n",
      "          37       1.00      0.42      0.59       527\n",
      "          38       0.99      0.39      0.56       413\n",
      "          39       1.00      0.38      0.55        68\n",
      "          40       1.00      0.34      0.51       106\n",
      "          41       1.00      0.40      0.57        35\n",
      "          42       1.00      0.37      0.54       298\n",
      "          43       0.85      0.83      0.84      3697\n",
      "          44       1.00      0.42      0.59       322\n",
      "          45       0.98      0.41      0.58       464\n",
      "          46       1.00      0.48      0.65        25\n",
      "          47       1.00      0.38      0.55       330\n",
      "          48       1.00      0.36      0.53       154\n",
      "          49       1.00      0.24      0.38       123\n",
      "          50       1.00      0.23      0.38       104\n",
      "          51       1.00      0.43      0.60       169\n",
      "          52       0.96      0.64      0.77      1184\n",
      "          53       1.00      0.39      0.56       176\n",
      "          54       1.00      0.38      0.55        64\n",
      "          55       1.00      0.38      0.55       140\n",
      "          56       0.88      0.56      0.69      1556\n",
      "          57       0.87      0.64      0.74      1850\n",
      "          58       1.00      0.36      0.53       392\n",
      "          59       0.99      0.53      0.69       532\n",
      "          60       1.00      0.57      0.72       197\n",
      "          61       1.00      0.38      0.56       156\n",
      "          62       1.00      0.36      0.53       233\n",
      "          63       1.00      0.28      0.44       111\n",
      "          64       1.00      0.07      0.13        28\n",
      "          65       0.99      0.43      0.60       661\n",
      "          66       1.00      0.30      0.46        80\n",
      "          67       0.98      0.39      0.56       370\n",
      "          68       0.86      0.77      0.81      2813\n",
      "          69       1.00      0.27      0.42        49\n",
      "          70       1.00      0.38      0.55        55\n",
      "\n",
      "   micro avg       0.91      0.57      0.70     28022\n",
      "   macro avg       0.98      0.41      0.57     28022\n",
      "weighted avg       0.93      0.57      0.68     28022\n",
      " samples avg       0.75      0.60      0.64     28022\n",
      "\n",
      "t2_4000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.021    |             |\n",
      "| Term Wise Accuracy |    0.979    |             |\n",
      "|      Accuracy      |    0.313    |             |\n",
      "|     Precision      |    0.985    |    0.901    |\n",
      "|       Recall       |    0.364    |    0.547    |\n",
      "|     F1-measure     |    0.513    |    0.680    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.021 |\n",
      "|   Literature Accuracy    | 0.543 |\n",
      "|   Literature Precision   | 0.732 |\n",
      "|     LiteratureRecall     | 0.578 |\n",
      "|   LiteratureF1-measure   | 0.646 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.34      0.51       169\n",
      "           1       0.97      0.52      0.68       423\n",
      "           2       1.00      0.17      0.29        82\n",
      "           3       1.00      0.31      0.48        93\n",
      "           4       1.00      0.31      0.48        64\n",
      "           5       1.00      0.35      0.52       129\n",
      "           6       1.00      0.44      0.61        71\n",
      "           7       1.00      0.31      0.47       264\n",
      "           8       1.00      0.11      0.19        28\n",
      "           9       1.00      0.35      0.52       158\n",
      "          10       1.00      0.16      0.28        56\n",
      "          11       1.00      0.18      0.30       139\n",
      "          12       1.00      0.37      0.54       330\n",
      "          13       1.00      0.25      0.40        77\n",
      "          14       1.00      0.22      0.36        27\n",
      "          15       1.00      0.29      0.45        52\n",
      "          16       1.00      0.29      0.45        59\n",
      "          17       0.94      0.57      0.71      1194\n",
      "          18       1.00      0.37      0.53        63\n",
      "          19       1.00      0.34      0.51       282\n",
      "          20       0.88      0.60      0.71      1680\n",
      "          21       1.00      0.28      0.44       116\n",
      "          22       1.00      0.32      0.48       257\n",
      "          23       1.00      0.25      0.40       125\n",
      "          24       1.00      0.33      0.50       254\n",
      "          25       1.00      0.40      0.57       450\n",
      "          26       0.98      0.62      0.76       351\n",
      "          27       1.00      0.31      0.48        48\n",
      "          28       0.87      0.60      0.71      1849\n",
      "          29       0.98      0.54      0.70       548\n",
      "          30       1.00      0.45      0.62       285\n",
      "          31       1.00      0.11      0.20        44\n",
      "          32       1.00      0.32      0.49        99\n",
      "          33       1.00      0.51      0.67       171\n",
      "          34       0.98      0.44      0.61       102\n",
      "          35       1.00      0.23      0.37        97\n",
      "          36       0.99      0.42      0.59       304\n",
      "          37       1.00      0.38      0.55       527\n",
      "          38       0.99      0.36      0.53       413\n",
      "          39       1.00      0.28      0.44        68\n",
      "          40       1.00      0.30      0.46       106\n",
      "          41       1.00      0.34      0.51        35\n",
      "          42       1.00      0.32      0.49       298\n",
      "          43       0.84      0.80      0.82      3697\n",
      "          44       1.00      0.39      0.57       322\n",
      "          45       0.99      0.39      0.56       464\n",
      "          46       1.00      0.44      0.61        25\n",
      "          47       1.00      0.38      0.55       330\n",
      "          48       0.98      0.33      0.50       154\n",
      "          49       1.00      0.20      0.34       123\n",
      "          50       1.00      0.25      0.40       104\n",
      "          51       1.00      0.28      0.44       169\n",
      "          52       0.95      0.60      0.74      1184\n",
      "          53       1.00      0.35      0.52       176\n",
      "          54       1.00      0.31      0.48        64\n",
      "          55       1.00      0.34      0.51       140\n",
      "          56       0.89      0.55      0.68      1556\n",
      "          57       0.88      0.65      0.75      1850\n",
      "          58       1.00      0.38      0.55       392\n",
      "          59       0.99      0.52      0.68       532\n",
      "          60       1.00      0.53      0.70       197\n",
      "          61       1.00      0.31      0.48       156\n",
      "          62       1.00      0.31      0.47       233\n",
      "          63       1.00      0.23      0.38       111\n",
      "          64       1.00      0.04      0.07        28\n",
      "          65       0.99      0.43      0.60       661\n",
      "          66       1.00      0.29      0.45        80\n",
      "          67       0.98      0.40      0.57       370\n",
      "          68       0.84      0.76      0.80      2813\n",
      "          69       1.00      0.16      0.28        49\n",
      "          70       1.00      0.25      0.41        55\n",
      "\n",
      "   micro avg       0.90      0.55      0.68     28022\n",
      "   macro avg       0.98      0.36      0.51     28022\n",
      "weighted avg       0.92      0.55      0.66     28022\n",
      " samples avg       0.73      0.58      0.61     28022\n",
      "\n",
      "t1_5000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.012    |             |\n",
      "| Term Wise Accuracy |    0.988    |             |\n",
      "|      Accuracy      |    0.519    |             |\n",
      "|     Precision      |    0.989    |    0.943    |\n",
      "|       Recall       |    0.705    |    0.756    |\n",
      "|     F1-measure     |    0.820    |    0.839    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.012 |\n",
      "|   Literature Accuracy    | 0.734 |\n",
      "|   Literature Precision   | 0.855 |\n",
      "|     LiteratureRecall     | 0.766 |\n",
      "|   LiteratureF1-measure   | 0.808 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80       169\n",
      "           1       0.97      0.78      0.87       423\n",
      "           2       1.00      0.67      0.80        82\n",
      "           3       1.00      0.67      0.80        93\n",
      "           4       1.00      0.81      0.90        64\n",
      "           5       1.00      0.73      0.84       129\n",
      "           6       1.00      0.87      0.93        71\n",
      "           7       1.00      0.69      0.81       264\n",
      "           8       1.00      0.68      0.81        28\n",
      "           9       1.00      0.80      0.89       158\n",
      "          10       1.00      0.55      0.71        56\n",
      "          11       1.00      0.58      0.73       139\n",
      "          12       1.00      0.70      0.82       330\n",
      "          13       1.00      0.69      0.82        77\n",
      "          14       1.00      0.48      0.65        27\n",
      "          15       1.00      0.69      0.82        52\n",
      "          16       1.00      0.73      0.84        59\n",
      "          17       0.96      0.73      0.83      1194\n",
      "          18       1.00      0.73      0.84        63\n",
      "          19       1.00      0.71      0.83       282\n",
      "          20       0.91      0.74      0.82      1680\n",
      "          21       1.00      0.64      0.78       116\n",
      "          22       1.00      0.69      0.82       257\n",
      "          23       1.00      0.69      0.82       125\n",
      "          24       1.00      0.69      0.82       254\n",
      "          25       0.99      0.73      0.84       450\n",
      "          26       0.99      0.82      0.90       351\n",
      "          27       1.00      0.77      0.87        48\n",
      "          28       0.91      0.74      0.82      1849\n",
      "          29       0.99      0.76      0.86       548\n",
      "          30       1.00      0.76      0.87       285\n",
      "          31       1.00      0.61      0.76        44\n",
      "          32       1.00      0.68      0.81        99\n",
      "          33       1.00      0.84      0.91       171\n",
      "          34       0.99      0.78      0.87       102\n",
      "          35       1.00      0.57      0.72        97\n",
      "          36       1.00      0.68      0.81       304\n",
      "          37       1.00      0.69      0.82       527\n",
      "          38       0.99      0.66      0.79       413\n",
      "          39       1.00      0.81      0.89        68\n",
      "          40       1.00      0.71      0.83       106\n",
      "          41       1.00      0.71      0.83        35\n",
      "          42       0.99      0.59      0.74       298\n",
      "          43       0.88      0.86      0.87      3697\n",
      "          44       1.00      0.69      0.82       322\n",
      "          45       1.00      0.70      0.83       464\n",
      "          46       1.00      0.60      0.75        25\n",
      "          47       1.00      0.73      0.84       330\n",
      "          48       1.00      0.74      0.85       154\n",
      "          49       1.00      0.54      0.70       123\n",
      "          50       1.00      0.59      0.74       104\n",
      "          51       0.99      0.70      0.82       169\n",
      "          52       0.98      0.80      0.88      1184\n",
      "          53       1.00      0.64      0.78       176\n",
      "          54       1.00      0.77      0.87        64\n",
      "          55       1.00      0.65      0.79       140\n",
      "          56       0.92      0.74      0.82      1556\n",
      "          57       0.92      0.78      0.85      1850\n",
      "          58       1.00      0.68      0.81       392\n",
      "          59       1.00      0.77      0.87       532\n",
      "          60       1.00      0.81      0.89       197\n",
      "          61       0.99      0.71      0.83       156\n",
      "          62       1.00      0.71      0.83       233\n",
      "          63       1.00      0.67      0.80       111\n",
      "          64       1.00      0.50      0.67        28\n",
      "          65       1.00      0.71      0.83       661\n",
      "          66       1.00      0.70      0.82        80\n",
      "          67       0.98      0.71      0.82       370\n",
      "          68       0.89      0.83      0.86      2813\n",
      "          69       1.00      0.73      0.85        49\n",
      "          70       1.00      0.71      0.83        55\n",
      "\n",
      "   micro avg       0.94      0.76      0.84     28022\n",
      "   macro avg       0.99      0.71      0.82     28022\n",
      "weighted avg       0.95      0.76      0.84     28022\n",
      " samples avg       0.85      0.77      0.79     28022\n",
      "\n",
      "t1_5000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.012    |             |\n",
      "| Term Wise Accuracy |    0.988    |             |\n",
      "|      Accuracy      |    0.521    |             |\n",
      "|     Precision      |    0.989    |    0.943    |\n",
      "|       Recall       |    0.680    |    0.747    |\n",
      "|     F1-measure     |    0.802    |    0.833    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.012 |\n",
      "|   Literature Accuracy    | 0.727 |\n",
      "|   Literature Precision   | 0.848 |\n",
      "|     LiteratureRecall     | 0.758 |\n",
      "|   LiteratureF1-measure   | 0.800 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.66      0.80       169\n",
      "           1       0.98      0.78      0.87       423\n",
      "           2       1.00      0.57      0.73        82\n",
      "           3       1.00      0.61      0.76        93\n",
      "           4       1.00      0.77      0.87        64\n",
      "           5       1.00      0.72      0.84       129\n",
      "           6       1.00      0.85      0.92        71\n",
      "           7       1.00      0.65      0.79       264\n",
      "           8       1.00      0.61      0.76        28\n",
      "           9       1.00      0.75      0.86       158\n",
      "          10       1.00      0.61      0.76        56\n",
      "          11       1.00      0.57      0.72       139\n",
      "          12       1.00      0.68      0.81       330\n",
      "          13       1.00      0.70      0.82        77\n",
      "          14       1.00      0.41      0.58        27\n",
      "          15       1.00      0.75      0.86        52\n",
      "          16       1.00      0.75      0.85        59\n",
      "          17       0.96      0.71      0.81      1194\n",
      "          18       1.00      0.62      0.76        63\n",
      "          19       1.00      0.66      0.80       282\n",
      "          20       0.92      0.72      0.81      1680\n",
      "          21       1.00      0.62      0.77       116\n",
      "          22       1.00      0.65      0.79       257\n",
      "          23       1.00      0.59      0.74       125\n",
      "          24       1.00      0.69      0.82       254\n",
      "          25       1.00      0.74      0.85       450\n",
      "          26       0.99      0.83      0.90       351\n",
      "          27       1.00      0.71      0.83        48\n",
      "          28       0.91      0.76      0.83      1849\n",
      "          29       0.99      0.75      0.85       548\n",
      "          30       1.00      0.75      0.86       285\n",
      "          31       1.00      0.52      0.69        44\n",
      "          32       1.00      0.66      0.79        99\n",
      "          33       1.00      0.82      0.90       171\n",
      "          34       0.99      0.79      0.88       102\n",
      "          35       1.00      0.57      0.72        97\n",
      "          36       1.00      0.68      0.81       304\n",
      "          37       1.00      0.68      0.81       527\n",
      "          38       0.99      0.68      0.80       413\n",
      "          39       1.00      0.74      0.85        68\n",
      "          40       1.00      0.63      0.77       106\n",
      "          41       1.00      0.71      0.83        35\n",
      "          42       0.99      0.63      0.77       298\n",
      "          43       0.88      0.86      0.87      3697\n",
      "          44       1.00      0.70      0.83       322\n",
      "          45       0.99      0.67      0.80       464\n",
      "          46       1.00      0.64      0.78        25\n",
      "          47       0.99      0.66      0.79       330\n",
      "          48       1.00      0.72      0.84       154\n",
      "          49       1.00      0.51      0.68       123\n",
      "          50       1.00      0.59      0.74       104\n",
      "          51       1.00      0.62      0.77       169\n",
      "          52       0.97      0.79      0.87      1184\n",
      "          53       1.00      0.61      0.76       176\n",
      "          54       1.00      0.69      0.81        64\n",
      "          55       1.00      0.59      0.74       140\n",
      "          56       0.93      0.74      0.82      1556\n",
      "          57       0.92      0.78      0.84      1850\n",
      "          58       1.00      0.62      0.76       392\n",
      "          59       0.99      0.77      0.87       532\n",
      "          60       0.99      0.81      0.89       197\n",
      "          61       1.00      0.68      0.81       156\n",
      "          62       1.00      0.70      0.82       233\n",
      "          63       1.00      0.54      0.70       111\n",
      "          64       1.00      0.43      0.60        28\n",
      "          65       1.00      0.69      0.82       661\n",
      "          66       1.00      0.61      0.76        80\n",
      "          67       0.99      0.67      0.80       370\n",
      "          68       0.89      0.85      0.87      2813\n",
      "          69       1.00      0.71      0.83        49\n",
      "          70       1.00      0.71      0.83        55\n",
      "\n",
      "   micro avg       0.94      0.75      0.83     28022\n",
      "   macro avg       0.99      0.68      0.80     28022\n",
      "weighted avg       0.95      0.75      0.83     28022\n",
      " samples avg       0.85      0.76      0.78     28022\n",
      "\n",
      "t1_5000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.012    |             |\n",
      "| Term Wise Accuracy |    0.988    |             |\n",
      "|      Accuracy      |    0.503    |             |\n",
      "|     Precision      |    0.989    |    0.941    |\n",
      "|       Recall       |    0.699    |    0.748    |\n",
      "|     F1-measure     |    0.816    |    0.834    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.012 |\n",
      "|   Literature Accuracy    | 0.723 |\n",
      "|   Literature Precision   | 0.847 |\n",
      "|     LiteratureRecall     | 0.756 |\n",
      "|   LiteratureF1-measure   | 0.799 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.81       169\n",
      "           1       0.99      0.79      0.88       423\n",
      "           2       1.00      0.62      0.77        82\n",
      "           3       1.00      0.70      0.82        93\n",
      "           4       1.00      0.86      0.92        64\n",
      "           5       1.00      0.78      0.87       129\n",
      "           6       1.00      0.77      0.87        71\n",
      "           7       1.00      0.71      0.83       264\n",
      "           8       1.00      0.68      0.81        28\n",
      "           9       1.00      0.72      0.84       158\n",
      "          10       1.00      0.64      0.78        56\n",
      "          11       1.00      0.63      0.77       139\n",
      "          12       1.00      0.68      0.81       330\n",
      "          13       1.00      0.81      0.89        77\n",
      "          14       1.00      0.52      0.68        27\n",
      "          15       1.00      0.71      0.83        52\n",
      "          16       1.00      0.80      0.89        59\n",
      "          17       0.96      0.72      0.82      1194\n",
      "          18       1.00      0.67      0.80        63\n",
      "          19       1.00      0.65      0.79       282\n",
      "          20       0.91      0.72      0.81      1680\n",
      "          21       1.00      0.59      0.74       116\n",
      "          22       1.00      0.65      0.78       257\n",
      "          23       1.00      0.63      0.77       125\n",
      "          24       1.00      0.67      0.80       254\n",
      "          25       1.00      0.71      0.83       450\n",
      "          26       0.99      0.81      0.89       351\n",
      "          27       1.00      0.77      0.87        48\n",
      "          28       0.91      0.74      0.82      1849\n",
      "          29       0.99      0.73      0.84       548\n",
      "          30       1.00      0.72      0.84       285\n",
      "          31       1.00      0.68      0.81        44\n",
      "          32       1.00      0.67      0.80        99\n",
      "          33       1.00      0.77      0.87       171\n",
      "          34       0.99      0.83      0.90       102\n",
      "          35       1.00      0.59      0.74        97\n",
      "          36       1.00      0.66      0.79       304\n",
      "          37       1.00      0.70      0.83       527\n",
      "          38       0.99      0.66      0.79       413\n",
      "          39       1.00      0.76      0.87        68\n",
      "          40       1.00      0.75      0.85       106\n",
      "          41       1.00      0.69      0.81        35\n",
      "          42       0.99      0.69      0.81       298\n",
      "          43       0.88      0.86      0.87      3697\n",
      "          44       1.00      0.65      0.79       322\n",
      "          45       0.99      0.75      0.85       464\n",
      "          46       1.00      0.60      0.75        25\n",
      "          47       1.00      0.71      0.83       330\n",
      "          48       1.00      0.68      0.81       154\n",
      "          49       1.00      0.54      0.70       123\n",
      "          50       1.00      0.60      0.75       104\n",
      "          51       1.00      0.71      0.83       169\n",
      "          52       0.97      0.77      0.86      1184\n",
      "          53       1.00      0.65      0.79       176\n",
      "          54       1.00      0.75      0.86        64\n",
      "          55       1.00      0.65      0.79       140\n",
      "          56       0.93      0.72      0.81      1556\n",
      "          57       0.92      0.77      0.84      1850\n",
      "          58       1.00      0.62      0.77       392\n",
      "          59       1.00      0.76      0.86       532\n",
      "          60       0.99      0.81      0.89       197\n",
      "          61       0.99      0.64      0.78       156\n",
      "          62       1.00      0.66      0.80       233\n",
      "          63       1.00      0.59      0.75       111\n",
      "          64       1.00      0.43      0.60        28\n",
      "          65       1.00      0.72      0.83       661\n",
      "          66       1.00      0.68      0.81        80\n",
      "          67       0.98      0.68      0.80       370\n",
      "          68       0.89      0.83      0.86      2813\n",
      "          69       1.00      0.73      0.85        49\n",
      "          70       1.00      0.75      0.85        55\n",
      "\n",
      "   micro avg       0.94      0.75      0.83     28022\n",
      "   macro avg       0.99      0.70      0.82     28022\n",
      "weighted avg       0.95      0.75      0.83     28022\n",
      " samples avg       0.85      0.76      0.78     28022\n",
      "\n",
      "t2_5000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.012    |             |\n",
      "| Term Wise Accuracy |    0.988    |             |\n",
      "|      Accuracy      |    0.521    |             |\n",
      "|     Precision      |    0.990    |    0.944    |\n",
      "|       Recall       |    0.694    |    0.750    |\n",
      "|     F1-measure     |    0.813    |    0.835    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.012 |\n",
      "|   Literature Accuracy    | 0.735 |\n",
      "|   Literature Precision   | 0.858 |\n",
      "|     LiteratureRecall     | 0.768 |\n",
      "|   LiteratureF1-measure   | 0.810 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.66      0.80       169\n",
      "           1       0.98      0.77      0.86       423\n",
      "           2       1.00      0.56      0.72        82\n",
      "           3       1.00      0.68      0.81        93\n",
      "           4       1.00      0.81      0.90        64\n",
      "           5       1.00      0.75      0.86       129\n",
      "           6       1.00      0.86      0.92        71\n",
      "           7       1.00      0.66      0.79       264\n",
      "           8       1.00      0.64      0.78        28\n",
      "           9       1.00      0.70      0.82       158\n",
      "          10       1.00      0.61      0.76        56\n",
      "          11       1.00      0.60      0.75       139\n",
      "          12       1.00      0.65      0.79       330\n",
      "          13       1.00      0.65      0.79        77\n",
      "          14       1.00      0.52      0.68        27\n",
      "          15       1.00      0.69      0.82        52\n",
      "          16       1.00      0.75      0.85        59\n",
      "          17       0.96      0.72      0.82      1194\n",
      "          18       1.00      0.75      0.85        63\n",
      "          19       1.00      0.59      0.74       282\n",
      "          20       0.92      0.72      0.81      1680\n",
      "          21       1.00      0.68      0.81       116\n",
      "          22       0.99      0.68      0.81       257\n",
      "          23       1.00      0.65      0.79       125\n",
      "          24       1.00      0.70      0.82       254\n",
      "          25       1.00      0.69      0.81       450\n",
      "          26       0.99      0.82      0.90       351\n",
      "          27       1.00      0.77      0.87        48\n",
      "          28       0.91      0.75      0.82      1849\n",
      "          29       0.99      0.77      0.87       548\n",
      "          30       1.00      0.75      0.86       285\n",
      "          31       1.00      0.57      0.72        44\n",
      "          32       1.00      0.69      0.81        99\n",
      "          33       1.00      0.85      0.92       171\n",
      "          34       0.99      0.81      0.89       102\n",
      "          35       1.00      0.65      0.79        97\n",
      "          36       1.00      0.69      0.81       304\n",
      "          37       1.00      0.68      0.81       527\n",
      "          38       1.00      0.66      0.80       413\n",
      "          39       1.00      0.74      0.85        68\n",
      "          40       1.00      0.68      0.81       106\n",
      "          41       1.00      0.69      0.81        35\n",
      "          42       0.99      0.69      0.81       298\n",
      "          43       0.88      0.86      0.87      3697\n",
      "          44       1.00      0.64      0.78       322\n",
      "          45       0.99      0.69      0.81       464\n",
      "          46       1.00      0.68      0.81        25\n",
      "          47       1.00      0.65      0.78       330\n",
      "          48       1.00      0.74      0.85       154\n",
      "          49       1.00      0.55      0.71       123\n",
      "          50       1.00      0.51      0.68       104\n",
      "          51       1.00      0.70      0.83       169\n",
      "          52       0.98      0.78      0.87      1184\n",
      "          53       1.00      0.66      0.79       176\n",
      "          54       1.00      0.69      0.81        64\n",
      "          55       1.00      0.64      0.78       140\n",
      "          56       0.92      0.73      0.81      1556\n",
      "          57       0.93      0.78      0.85      1850\n",
      "          58       1.00      0.62      0.77       392\n",
      "          59       0.99      0.75      0.85       532\n",
      "          60       1.00      0.83      0.91       197\n",
      "          61       0.99      0.69      0.81       156\n",
      "          62       1.00      0.64      0.78       233\n",
      "          63       1.00      0.61      0.76       111\n",
      "          64       1.00      0.50      0.67        28\n",
      "          65       0.99      0.70      0.82       661\n",
      "          66       1.00      0.62      0.77        80\n",
      "          67       0.99      0.71      0.83       370\n",
      "          68       0.90      0.85      0.87      2813\n",
      "          69       1.00      0.73      0.85        49\n",
      "          70       1.00      0.76      0.87        55\n",
      "\n",
      "   micro avg       0.94      0.75      0.84     28022\n",
      "   macro avg       0.99      0.69      0.81     28022\n",
      "weighted avg       0.95      0.75      0.83     28022\n",
      " samples avg       0.86      0.77      0.79     28022\n",
      "\n",
      "t2_5000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.013    |             |\n",
      "| Term Wise Accuracy |    0.987    |             |\n",
      "|      Accuracy      |    0.524    |             |\n",
      "|     Precision      |    0.989    |    0.942    |\n",
      "|       Recall       |    0.674    |    0.745    |\n",
      "|     F1-measure     |    0.797    |    0.832    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.013 |\n",
      "|   Literature Accuracy    | 0.728 |\n",
      "|   Literature Precision   | 0.850 |\n",
      "|     LiteratureRecall     | 0.759 |\n",
      "|   LiteratureF1-measure   | 0.802 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.66      0.79       169\n",
      "           1       0.99      0.76      0.86       423\n",
      "           2       1.00      0.63      0.78        82\n",
      "           3       1.00      0.67      0.80        93\n",
      "           4       1.00      0.77      0.87        64\n",
      "           5       1.00      0.75      0.86       129\n",
      "           6       1.00      0.82      0.90        71\n",
      "           7       1.00      0.66      0.79       264\n",
      "           8       1.00      0.64      0.78        28\n",
      "           9       1.00      0.70      0.83       158\n",
      "          10       1.00      0.59      0.74        56\n",
      "          11       1.00      0.55      0.71       139\n",
      "          12       1.00      0.62      0.77       330\n",
      "          13       1.00      0.65      0.79        77\n",
      "          14       1.00      0.44      0.62        27\n",
      "          15       1.00      0.58      0.73        52\n",
      "          16       1.00      0.71      0.83        59\n",
      "          17       0.96      0.72      0.82      1194\n",
      "          18       1.00      0.59      0.74        63\n",
      "          19       1.00      0.63      0.78       282\n",
      "          20       0.93      0.74      0.82      1680\n",
      "          21       1.00      0.68      0.81       116\n",
      "          22       1.00      0.71      0.83       257\n",
      "          23       1.00      0.62      0.77       125\n",
      "          24       1.00      0.65      0.79       254\n",
      "          25       1.00      0.71      0.83       450\n",
      "          26       0.99      0.81      0.89       351\n",
      "          27       1.00      0.69      0.81        48\n",
      "          28       0.90      0.74      0.81      1849\n",
      "          29       0.99      0.76      0.86       548\n",
      "          30       0.99      0.78      0.87       285\n",
      "          31       1.00      0.48      0.65        44\n",
      "          32       1.00      0.61      0.75        99\n",
      "          33       1.00      0.81      0.89       171\n",
      "          34       0.99      0.75      0.85       102\n",
      "          35       1.00      0.46      0.63        97\n",
      "          36       1.00      0.66      0.79       304\n",
      "          37       1.00      0.72      0.84       527\n",
      "          38       0.99      0.63      0.77       413\n",
      "          39       1.00      0.79      0.89        68\n",
      "          40       1.00      0.64      0.78       106\n",
      "          41       1.00      0.60      0.75        35\n",
      "          42       1.00      0.64      0.78       298\n",
      "          43       0.89      0.86      0.87      3697\n",
      "          44       1.00      0.66      0.80       322\n",
      "          45       0.99      0.67      0.80       464\n",
      "          46       1.00      0.68      0.81        25\n",
      "          47       1.00      0.65      0.78       330\n",
      "          48       1.00      0.75      0.86       154\n",
      "          49       1.00      0.48      0.65       123\n",
      "          50       1.00      0.52      0.68       104\n",
      "          51       0.99      0.69      0.82       169\n",
      "          52       0.97      0.78      0.86      1184\n",
      "          53       1.00      0.64      0.78       176\n",
      "          54       1.00      0.66      0.79        64\n",
      "          55       1.00      0.61      0.76       140\n",
      "          56       0.92      0.74      0.82      1556\n",
      "          57       0.91      0.78      0.84      1850\n",
      "          58       1.00      0.62      0.77       392\n",
      "          59       0.99      0.75      0.86       532\n",
      "          60       0.99      0.80      0.89       197\n",
      "          61       1.00      0.69      0.81       156\n",
      "          62       0.99      0.64      0.78       233\n",
      "          63       1.00      0.65      0.79       111\n",
      "          64       1.00      0.43      0.60        28\n",
      "          65       0.99      0.71      0.82       661\n",
      "          66       1.00      0.68      0.81        80\n",
      "          67       0.99      0.67      0.80       370\n",
      "          68       0.89      0.83      0.86      2813\n",
      "          69       1.00      0.67      0.80        49\n",
      "          70       1.00      0.73      0.84        55\n",
      "\n",
      "   micro avg       0.94      0.74      0.83     28022\n",
      "   macro avg       0.99      0.67      0.80     28022\n",
      "weighted avg       0.95      0.74      0.83     28022\n",
      " samples avg       0.85      0.76      0.78     28022\n",
      "\n",
      "t2_5000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.013    |             |\n",
      "| Term Wise Accuracy |    0.987    |             |\n",
      "|      Accuracy      |    0.496    |             |\n",
      "|     Precision      |    0.988    |    0.938    |\n",
      "|       Recall       |    0.666    |    0.740    |\n",
      "|     F1-measure     |    0.791    |    0.827    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.013 |\n",
      "|   Literature Accuracy    | 0.720 |\n",
      "|   Literature Precision   | 0.846 |\n",
      "|     LiteratureRecall     | 0.755 |\n",
      "|   LiteratureF1-measure   | 0.798 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.83       169\n",
      "           1       0.99      0.76      0.86       423\n",
      "           2       1.00      0.62      0.77        82\n",
      "           3       1.00      0.65      0.78        93\n",
      "           4       1.00      0.80      0.89        64\n",
      "           5       1.00      0.70      0.82       129\n",
      "           6       1.00      0.75      0.85        71\n",
      "           7       1.00      0.69      0.81       264\n",
      "           8       1.00      0.71      0.83        28\n",
      "           9       1.00      0.67      0.80       158\n",
      "          10       1.00      0.66      0.80        56\n",
      "          11       1.00      0.50      0.66       139\n",
      "          12       1.00      0.64      0.78       330\n",
      "          13       1.00      0.65      0.79        77\n",
      "          14       1.00      0.48      0.65        27\n",
      "          15       1.00      0.71      0.83        52\n",
      "          16       1.00      0.71      0.83        59\n",
      "          17       0.96      0.71      0.81      1194\n",
      "          18       1.00      0.65      0.79        63\n",
      "          19       1.00      0.60      0.75       282\n",
      "          20       0.92      0.73      0.82      1680\n",
      "          21       0.99      0.62      0.76       116\n",
      "          22       1.00      0.61      0.76       257\n",
      "          23       1.00      0.62      0.76       125\n",
      "          24       1.00      0.66      0.79       254\n",
      "          25       0.99      0.73      0.84       450\n",
      "          26       0.99      0.81      0.89       351\n",
      "          27       1.00      0.69      0.81        48\n",
      "          28       0.89      0.72      0.79      1849\n",
      "          29       0.99      0.75      0.85       548\n",
      "          30       1.00      0.73      0.84       285\n",
      "          31       1.00      0.43      0.60        44\n",
      "          32       1.00      0.70      0.82        99\n",
      "          33       1.00      0.80      0.89       171\n",
      "          34       0.99      0.75      0.85       102\n",
      "          35       1.00      0.51      0.67        97\n",
      "          36       1.00      0.70      0.82       304\n",
      "          37       1.00      0.67      0.80       527\n",
      "          38       0.99      0.67      0.80       413\n",
      "          39       1.00      0.74      0.85        68\n",
      "          40       1.00      0.62      0.77       106\n",
      "          41       1.00      0.66      0.79        35\n",
      "          42       0.99      0.65      0.78       298\n",
      "          43       0.88      0.88      0.88      3697\n",
      "          44       1.00      0.67      0.80       322\n",
      "          45       0.98      0.66      0.79       464\n",
      "          46       1.00      0.64      0.78        25\n",
      "          47       1.00      0.71      0.83       330\n",
      "          48       1.00      0.74      0.85       154\n",
      "          49       1.00      0.51      0.68       123\n",
      "          50       1.00      0.61      0.75       104\n",
      "          51       1.00      0.66      0.80       169\n",
      "          52       0.97      0.77      0.86      1184\n",
      "          53       1.00      0.58      0.73       176\n",
      "          54       1.00      0.64      0.78        64\n",
      "          55       0.99      0.66      0.79       140\n",
      "          56       0.91      0.73      0.81      1556\n",
      "          57       0.91      0.78      0.84      1850\n",
      "          58       1.00      0.57      0.73       392\n",
      "          59       0.99      0.73      0.84       532\n",
      "          60       0.99      0.81      0.89       197\n",
      "          61       0.99      0.60      0.75       156\n",
      "          62       0.99      0.68      0.81       233\n",
      "          63       1.00      0.56      0.72       111\n",
      "          64       1.00      0.29      0.44        28\n",
      "          65       0.99      0.67      0.80       661\n",
      "          66       1.00      0.55      0.71        80\n",
      "          67       0.99      0.68      0.81       370\n",
      "          68       0.89      0.84      0.86      2813\n",
      "          69       1.00      0.55      0.71        49\n",
      "          70       1.00      0.64      0.78        55\n",
      "\n",
      "   micro avg       0.94      0.74      0.83     28022\n",
      "   macro avg       0.99      0.67      0.79     28022\n",
      "weighted avg       0.95      0.74      0.82     28022\n",
      " samples avg       0.85      0.76      0.78     28022\n",
      "\n",
      "t1_10000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.940    |             |\n",
      "|     Precision      |    0.983    |    0.981    |\n",
      "|       Recall       |    0.968    |    0.973    |\n",
      "|     F1-measure     |    0.975    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.961 |\n",
      "|   Literature Precision   | 0.972 |\n",
      "|     LiteratureRecall     | 0.972 |\n",
      "|   LiteratureF1-measure   | 0.972 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       169\n",
      "           1       0.98      0.96      0.97       423\n",
      "           2       0.99      0.98      0.98        82\n",
      "           3       0.99      0.92      0.96        93\n",
      "           4       1.00      0.98      0.99        64\n",
      "           5       0.98      0.98      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.98      0.98      0.98       264\n",
      "           8       0.97      1.00      0.98        28\n",
      "           9       1.00      0.94      0.97       158\n",
      "          10       1.00      0.95      0.97        56\n",
      "          11       0.99      0.97      0.98       139\n",
      "          12       1.00      0.96      0.98       330\n",
      "          13       0.99      1.00      0.99        77\n",
      "          14       1.00      0.89      0.94        27\n",
      "          15       0.98      0.98      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.99      0.98      0.99      1194\n",
      "          18       1.00      0.98      0.99        63\n",
      "          19       0.95      0.99      0.97       282\n",
      "          20       0.97      0.99      0.98      1680\n",
      "          21       0.99      0.96      0.97       116\n",
      "          22       0.97      0.98      0.97       257\n",
      "          23       0.98      1.00      0.99       125\n",
      "          24       0.98      0.99      0.99       254\n",
      "          25       0.97      0.99      0.98       450\n",
      "          26       0.98      0.98      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.98      0.98      0.98      1849\n",
      "          29       0.98      0.97      0.97       548\n",
      "          30       1.00      0.95      0.97       285\n",
      "          31       0.98      1.00      0.99        44\n",
      "          32       1.00      0.92      0.96        99\n",
      "          33       0.98      0.97      0.98       171\n",
      "          34       0.98      0.96      0.97       102\n",
      "          35       0.99      0.97      0.98        97\n",
      "          36       0.99      0.98      0.98       304\n",
      "          37       0.99      0.99      0.99       527\n",
      "          38       0.99      0.94      0.97       413\n",
      "          39       1.00      0.97      0.99        68\n",
      "          40       0.98      0.91      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.96      0.98      0.97       298\n",
      "          43       0.97      0.99      0.98      3697\n",
      "          44       0.99      0.97      0.98       322\n",
      "          45       0.98      0.95      0.97       464\n",
      "          46       0.88      0.84      0.86        25\n",
      "          47       0.98      0.96      0.97       330\n",
      "          48       0.98      0.99      0.98       154\n",
      "          49       0.96      1.00      0.98       123\n",
      "          50       0.96      0.94      0.95       104\n",
      "          51       0.97      0.98      0.97       169\n",
      "          52       1.00      0.96      0.98      1184\n",
      "          53       1.00      0.97      0.99       176\n",
      "          54       0.98      0.98      0.98        64\n",
      "          55       0.98      0.98      0.98       140\n",
      "          56       0.98      0.96      0.97      1556\n",
      "          57       0.99      0.97      0.98      1850\n",
      "          58       0.95      0.99      0.97       392\n",
      "          59       0.99      0.96      0.98       532\n",
      "          60       0.97      0.97      0.97       197\n",
      "          61       0.99      0.96      0.98       156\n",
      "          62       0.98      0.95      0.97       233\n",
      "          63       0.99      0.98      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       1.00      0.96      0.98       661\n",
      "          66       1.00      0.94      0.97        80\n",
      "          67       0.96      0.96      0.96       370\n",
      "          68       1.00      0.97      0.98      2813\n",
      "          69       0.96      0.96      0.96        49\n",
      "          70       0.98      0.98      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.97      0.98     28022\n",
      "   macro avg       0.98      0.97      0.98     28022\n",
      "weighted avg       0.98      0.97      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n",
      "t1_10000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.941    |             |\n",
      "|     Precision      |    0.992    |    0.986    |\n",
      "|       Recall       |    0.959    |    0.968    |\n",
      "|     F1-measure     |    0.975    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.959 |\n",
      "|   Literature Precision   | 0.973 |\n",
      "|     LiteratureRecall     | 0.965 |\n",
      "|   LiteratureF1-measure   | 0.969 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       169\n",
      "           1       0.99      0.95      0.97       423\n",
      "           2       1.00      0.96      0.98        82\n",
      "           3       1.00      0.91      0.96        93\n",
      "           4       0.98      1.00      0.99        64\n",
      "           5       0.96      1.00      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.99      0.97      0.98       264\n",
      "           8       1.00      0.96      0.98        28\n",
      "           9       1.00      0.94      0.97       158\n",
      "          10       1.00      0.95      0.97        56\n",
      "          11       1.00      0.96      0.98       139\n",
      "          12       0.99      0.97      0.98       330\n",
      "          13       1.00      0.99      0.99        77\n",
      "          14       1.00      0.89      0.94        27\n",
      "          15       1.00      0.96      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.98      0.99      0.99      1194\n",
      "          18       1.00      0.98      0.99        63\n",
      "          19       0.98      0.95      0.97       282\n",
      "          20       0.98      0.97      0.98      1680\n",
      "          21       0.96      0.99      0.97       116\n",
      "          22       1.00      0.95      0.97       257\n",
      "          23       0.99      0.99      0.99       125\n",
      "          24       0.99      0.98      0.99       254\n",
      "          25       0.99      0.97      0.98       450\n",
      "          26       0.97      0.99      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.99      0.96      0.98      1849\n",
      "          29       0.97      0.98      0.97       548\n",
      "          30       1.00      0.95      0.97       285\n",
      "          31       1.00      0.98      0.99        44\n",
      "          32       1.00      0.92      0.96        99\n",
      "          33       1.00      0.95      0.98       171\n",
      "          34       0.99      0.95      0.97       102\n",
      "          35       0.99      0.97      0.98        97\n",
      "          36       0.99      0.97      0.98       304\n",
      "          37       1.00      0.98      0.99       527\n",
      "          38       0.99      0.94      0.97       413\n",
      "          39       1.00      0.97      0.99        68\n",
      "          40       1.00      0.89      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.99      0.95      0.97       298\n",
      "          43       0.97      0.99      0.98      3697\n",
      "          44       1.00      0.96      0.98       322\n",
      "          45       1.00      0.93      0.96       464\n",
      "          46       1.00      0.72      0.84        25\n",
      "          47       1.00      0.95      0.97       330\n",
      "          48       1.00      0.97      0.98       154\n",
      "          49       1.00      0.96      0.98       123\n",
      "          50       1.00      0.90      0.95       104\n",
      "          51       1.00      0.95      0.97       169\n",
      "          52       0.98      0.97      0.98      1184\n",
      "          53       1.00      0.97      0.99       176\n",
      "          54       1.00      0.97      0.98        64\n",
      "          55       1.00      0.96      0.98       140\n",
      "          56       0.99      0.94      0.97      1556\n",
      "          57       0.97      0.98      0.98      1850\n",
      "          58       0.98      0.96      0.97       392\n",
      "          59       1.00      0.96      0.98       532\n",
      "          60       0.99      0.95      0.97       197\n",
      "          61       1.00      0.96      0.98       156\n",
      "          62       0.99      0.94      0.96       233\n",
      "          63       0.99      0.98      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.98      0.98      0.98       661\n",
      "          66       0.99      0.95      0.97        80\n",
      "          67       0.98      0.95      0.96       370\n",
      "          68       1.00      0.97      0.98      2813\n",
      "          69       1.00      0.92      0.96        49\n",
      "          70       0.98      0.98      0.98        55\n",
      "\n",
      "   micro avg       0.99      0.97      0.98     28022\n",
      "   macro avg       0.99      0.96      0.97     28022\n",
      "weighted avg       0.99      0.97      0.98     28022\n",
      " samples avg       0.97      0.96      0.97     28022\n",
      "\n",
      "t1_10000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.940    |             |\n",
      "|     Precision      |    0.976    |    0.977    |\n",
      "|       Recall       |    0.976    |    0.977    |\n",
      "|     F1-measure     |    0.976    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.962 |\n",
      "|   Literature Precision   | 0.972 |\n",
      "|     LiteratureRecall     | 0.975 |\n",
      "|   LiteratureF1-measure   | 0.973 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       169\n",
      "           1       0.96      0.98      0.97       423\n",
      "           2       0.98      0.99      0.98        82\n",
      "           3       0.96      0.96      0.96        93\n",
      "           4       1.00      0.98      0.99        64\n",
      "           5       0.99      0.97      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       1.00      0.96      0.98       264\n",
      "           8       0.97      1.00      0.98        28\n",
      "           9       0.98      0.96      0.97       158\n",
      "          10       0.96      0.98      0.97        56\n",
      "          11       0.97      1.00      0.98       139\n",
      "          12       0.97      0.99      0.98       330\n",
      "          13       0.99      1.00      0.99        77\n",
      "          14       0.90      1.00      0.95        27\n",
      "          15       1.00      0.96      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.98      0.99      0.99      1194\n",
      "          18       1.00      0.98      0.99        63\n",
      "          19       0.96      0.98      0.97       282\n",
      "          20       0.97      0.99      0.98      1680\n",
      "          21       0.97      0.98      0.97       116\n",
      "          22       1.00      0.95      0.97       257\n",
      "          23       1.00      0.98      0.99       125\n",
      "          24       1.00      0.97      0.99       254\n",
      "          25       0.97      0.99      0.98       450\n",
      "          26       0.96      1.00      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.97      0.99      0.98      1849\n",
      "          29       0.99      0.96      0.97       548\n",
      "          30       0.98      0.98      0.98       285\n",
      "          31       0.98      1.00      0.99        44\n",
      "          32       0.94      0.98      0.96        99\n",
      "          33       0.99      0.96      0.98       171\n",
      "          34       0.99      0.95      0.97       102\n",
      "          35       0.99      0.97      0.98        97\n",
      "          36       0.98      0.99      0.98       304\n",
      "          37       1.00      0.98      0.99       527\n",
      "          38       0.99      0.94      0.97       413\n",
      "          39       0.97      1.00      0.99        68\n",
      "          40       0.96      0.92      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.98      0.96      0.97       298\n",
      "          43       0.97      0.99      0.98      3697\n",
      "          44       0.97      0.99      0.98       322\n",
      "          45       0.95      0.98      0.97       464\n",
      "          46       0.88      0.84      0.86        25\n",
      "          47       1.00      0.95      0.97       330\n",
      "          48       0.98      0.99      0.98       154\n",
      "          49       0.96      1.00      0.98       123\n",
      "          50       0.97      0.93      0.95       104\n",
      "          51       0.97      0.98      0.97       169\n",
      "          52       0.98      0.98      0.98      1184\n",
      "          53       1.00      0.97      0.99       176\n",
      "          54       0.97      1.00      0.98        64\n",
      "          55       1.00      0.96      0.98       140\n",
      "          56       0.99      0.94      0.97      1556\n",
      "          57       0.98      0.98      0.98      1850\n",
      "          58       0.95      0.98      0.97       392\n",
      "          59       0.97      0.98      0.98       532\n",
      "          60       0.97      0.97      0.97       197\n",
      "          61       0.96      0.99      0.98       156\n",
      "          62       0.96      0.97      0.97       233\n",
      "          63       0.98      0.99      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.97      0.99      0.98       661\n",
      "          66       0.96      0.97      0.97        80\n",
      "          67       0.96      0.96      0.96       370\n",
      "          68       0.99      0.97      0.98      2813\n",
      "          69       0.94      0.98      0.96        49\n",
      "          70       0.98      0.98      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     28022\n",
      "   macro avg       0.98      0.98      0.98     28022\n",
      "weighted avg       0.98      0.98      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n",
      "t2_10000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.941    |             |\n",
      "|     Precision      |    0.969    |    0.978    |\n",
      "|       Recall       |    0.984    |    0.977    |\n",
      "|     F1-measure     |    0.976    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.963 |\n",
      "|   Literature Precision   | 0.973 |\n",
      "|     LiteratureRecall     | 0.975 |\n",
      "|   LiteratureF1-measure   | 0.974 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       169\n",
      "           1       0.95      0.99      0.97       423\n",
      "           2       0.98      0.99      0.98        82\n",
      "           3       0.94      0.98      0.96        93\n",
      "           4       0.98      1.00      0.99        64\n",
      "           5       0.97      0.99      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.98      0.98      0.98       264\n",
      "           8       0.97      1.00      0.98        28\n",
      "           9       0.96      0.97      0.97       158\n",
      "          10       0.96      0.98      0.97        56\n",
      "          11       0.97      1.00      0.98       139\n",
      "          12       0.97      0.99      0.98       330\n",
      "          13       0.99      1.00      0.99        77\n",
      "          14       0.90      1.00      0.95        27\n",
      "          15       0.96      1.00      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       1.00      0.97      0.99      1194\n",
      "          18       0.98      1.00      0.99        63\n",
      "          19       0.99      0.95      0.97       282\n",
      "          20       0.98      0.97      0.98      1680\n",
      "          21       0.97      0.98      0.97       116\n",
      "          22       0.99      0.95      0.97       257\n",
      "          23       0.98      1.00      0.99       125\n",
      "          24       0.98      0.99      0.99       254\n",
      "          25       0.97      1.00      0.98       450\n",
      "          26       0.96      1.00      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.99      0.96      0.98      1849\n",
      "          29       0.97      0.98      0.97       548\n",
      "          30       0.99      0.96      0.98       285\n",
      "          31       0.98      1.00      0.99        44\n",
      "          32       0.94      0.98      0.96        99\n",
      "          33       0.98      0.98      0.98       171\n",
      "          34       0.98      0.96      0.97       102\n",
      "          35       0.96      1.00      0.98        97\n",
      "          36       0.97      0.99      0.98       304\n",
      "          37       0.98      0.99      0.99       527\n",
      "          38       0.98      0.96      0.97       413\n",
      "          39       0.97      1.00      0.99        68\n",
      "          40       0.94      0.94      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.95      0.99      0.97       298\n",
      "          43       0.99      0.97      0.98      3697\n",
      "          44       0.97      0.99      0.98       322\n",
      "          45       0.95      0.99      0.97       464\n",
      "          46       0.78      1.00      0.88        25\n",
      "          47       0.97      0.98      0.97       330\n",
      "          48       0.97      0.99      0.98       154\n",
      "          49       0.96      1.00      0.98       123\n",
      "          50       0.94      0.96      0.95       104\n",
      "          51       0.95      0.99      0.97       169\n",
      "          52       0.97      0.99      0.98      1184\n",
      "          53       0.98      0.99      0.99       176\n",
      "          54       0.97      1.00      0.98        64\n",
      "          55       0.97      0.99      0.98       140\n",
      "          56       0.99      0.95      0.97      1556\n",
      "          57       0.98      0.98      0.98      1850\n",
      "          58       0.95      0.98      0.97       392\n",
      "          59       0.98      0.97      0.98       532\n",
      "          60       0.96      0.98      0.97       197\n",
      "          61       0.97      0.99      0.98       156\n",
      "          62       0.95      0.98      0.97       233\n",
      "          63       0.98      0.99      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.97      0.99      0.98       661\n",
      "          66       0.94      1.00      0.97        80\n",
      "          67       0.97      0.96      0.96       370\n",
      "          68       0.98      0.98      0.98      2813\n",
      "          69       0.94      0.98      0.96        49\n",
      "          70       0.98      0.98      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     28022\n",
      "   macro avg       0.97      0.98      0.98     28022\n",
      "weighted avg       0.98      0.98      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n",
      "t2_10000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.941    |             |\n",
      "|     Precision      |    0.994    |    0.988    |\n",
      "|       Recall       |    0.957    |    0.967    |\n",
      "|     F1-measure     |    0.975    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.958 |\n",
      "|   Literature Precision   | 0.972 |\n",
      "|     LiteratureRecall     | 0.964 |\n",
      "|   LiteratureF1-measure   | 0.968 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98       169\n",
      "           1       0.99      0.94      0.97       423\n",
      "           2       0.99      0.98      0.98        82\n",
      "           3       0.98      0.94      0.96        93\n",
      "           4       1.00      0.98      0.99        64\n",
      "           5       1.00      0.96      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.99      0.97      0.98       264\n",
      "           8       1.00      0.96      0.98        28\n",
      "           9       1.00      0.94      0.97       158\n",
      "          10       1.00      0.95      0.97        56\n",
      "          11       1.00      0.96      0.98       139\n",
      "          12       1.00      0.96      0.98       330\n",
      "          13       0.99      1.00      0.99        77\n",
      "          14       1.00      0.89      0.94        27\n",
      "          15       1.00      0.96      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       1.00      0.97      0.99      1194\n",
      "          18       1.00      0.98      0.99        63\n",
      "          19       1.00      0.94      0.97       282\n",
      "          20       0.99      0.96      0.98      1680\n",
      "          21       0.97      0.98      0.97       116\n",
      "          22       1.00      0.95      0.97       257\n",
      "          23       1.00      0.98      0.99       125\n",
      "          24       1.00      0.97      0.99       254\n",
      "          25       1.00      0.96      0.98       450\n",
      "          26       0.96      1.00      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.98      0.98      0.98      1849\n",
      "          29       1.00      0.95      0.97       548\n",
      "          30       1.00      0.95      0.97       285\n",
      "          31       1.00      0.98      0.99        44\n",
      "          32       1.00      0.92      0.96        99\n",
      "          33       1.00      0.95      0.98       171\n",
      "          34       0.99      0.95      0.97       102\n",
      "          35       1.00      0.96      0.98        97\n",
      "          36       0.98      0.98      0.98       304\n",
      "          37       1.00      0.98      0.99       527\n",
      "          38       0.96      0.98      0.97       413\n",
      "          39       1.00      0.97      0.99        68\n",
      "          40       1.00      0.89      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.99      0.95      0.97       298\n",
      "          43       0.98      0.98      0.98      3697\n",
      "          44       0.99      0.97      0.98       322\n",
      "          45       0.98      0.95      0.96       464\n",
      "          46       1.00      0.72      0.84        25\n",
      "          47       0.99      0.96      0.97       330\n",
      "          48       1.00      0.97      0.98       154\n",
      "          49       1.00      0.96      0.98       123\n",
      "          50       0.95      0.95      0.95       104\n",
      "          51       1.00      0.95      0.97       169\n",
      "          52       0.99      0.96      0.98      1184\n",
      "          53       1.00      0.97      0.99       176\n",
      "          54       1.00      0.97      0.98        64\n",
      "          55       1.00      0.96      0.98       140\n",
      "          56       1.00      0.94      0.97      1556\n",
      "          57       0.98      0.98      0.98      1850\n",
      "          58       1.00      0.94      0.97       392\n",
      "          59       1.00      0.95      0.98       532\n",
      "          60       0.99      0.95      0.97       197\n",
      "          61       1.00      0.96      0.98       156\n",
      "          62       1.00      0.94      0.96       233\n",
      "          63       1.00      0.97      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       1.00      0.96      0.98       661\n",
      "          66       1.00      0.94      0.97        80\n",
      "          67       0.99      0.94      0.96       370\n",
      "          68       0.97      0.99      0.98      2813\n",
      "          69       1.00      0.92      0.96        49\n",
      "          70       1.00      0.96      0.98        55\n",
      "\n",
      "   micro avg       0.99      0.97      0.98     28022\n",
      "   macro avg       0.99      0.96      0.97     28022\n",
      "weighted avg       0.99      0.97      0.98     28022\n",
      " samples avg       0.97      0.96      0.96     28022\n",
      "\n",
      "t2_10000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.940    |             |\n",
      "|     Precision      |    0.979    |    0.980    |\n",
      "|       Recall       |    0.973    |    0.974    |\n",
      "|     F1-measure     |    0.976    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.961 |\n",
      "|   Literature Precision   | 0.971 |\n",
      "|     LiteratureRecall     | 0.972 |\n",
      "|   LiteratureF1-measure   | 0.972 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       169\n",
      "           1       0.95      0.98      0.97       423\n",
      "           2       0.99      0.98      0.98        82\n",
      "           3       0.97      0.95      0.96        93\n",
      "           4       1.00      0.98      0.99        64\n",
      "           5       1.00      0.96      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       1.00      0.96      0.98       264\n",
      "           8       0.97      1.00      0.98        28\n",
      "           9       0.99      0.94      0.97       158\n",
      "          10       1.00      0.95      0.97        56\n",
      "          11       0.99      0.98      0.98       139\n",
      "          12       0.99      0.97      0.98       330\n",
      "          13       0.99      1.00      0.99        77\n",
      "          14       0.93      0.96      0.95        27\n",
      "          15       0.98      0.98      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.98      0.99      0.99      1194\n",
      "          18       0.98      1.00      0.99        63\n",
      "          19       0.96      0.98      0.97       282\n",
      "          20       0.99      0.96      0.97      1680\n",
      "          21       0.98      0.97      0.97       116\n",
      "          22       1.00      0.95      0.97       257\n",
      "          23       0.98      1.00      0.99       125\n",
      "          24       1.00      0.97      0.99       254\n",
      "          25       1.00      0.97      0.98       450\n",
      "          26       0.96      1.00      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.97      0.99      0.98      1849\n",
      "          29       0.97      0.98      0.97       548\n",
      "          30       0.97      0.98      0.98       285\n",
      "          31       0.98      1.00      0.99        44\n",
      "          32       1.00      0.92      0.96        99\n",
      "          33       0.99      0.96      0.98       171\n",
      "          34       0.98      0.96      0.97       102\n",
      "          35       0.98      0.98      0.98        97\n",
      "          36       0.97      0.99      0.98       304\n",
      "          37       0.98      0.99      0.99       527\n",
      "          38       0.95      0.98      0.97       413\n",
      "          39       0.99      0.99      0.99        68\n",
      "          40       0.99      0.90      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.98      0.96      0.97       298\n",
      "          43       0.99      0.97      0.98      3697\n",
      "          44       1.00      0.97      0.98       322\n",
      "          45       0.98      0.95      0.96       464\n",
      "          46       0.78      1.00      0.88        25\n",
      "          47       1.00      0.95      0.97       330\n",
      "          48       0.99      0.98      0.98       154\n",
      "          49       1.00      0.96      0.98       123\n",
      "          50       0.95      0.95      0.95       104\n",
      "          51       0.95      0.99      0.97       169\n",
      "          52       1.00      0.96      0.98      1184\n",
      "          53       1.00      0.97      0.99       176\n",
      "          54       1.00      0.97      0.98        64\n",
      "          55       0.97      0.99      0.98       140\n",
      "          56       0.95      0.99      0.97      1556\n",
      "          57       0.99      0.96      0.98      1850\n",
      "          58       1.00      0.94      0.97       392\n",
      "          59       0.98      0.97      0.98       532\n",
      "          60       0.96      0.98      0.97       197\n",
      "          61       0.99      0.96      0.98       156\n",
      "          62       0.95      0.98      0.97       233\n",
      "          63       1.00      0.97      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.97      0.98      0.98       661\n",
      "          66       0.95      0.99      0.97        80\n",
      "          67       0.99      0.94      0.96       370\n",
      "          68       0.97      0.99      0.98      2813\n",
      "          69       0.94      0.98      0.96        49\n",
      "          70       0.96      1.00      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.97      0.98     28022\n",
      "   macro avg       0.98      0.97      0.98     28022\n",
      "weighted avg       0.98      0.97      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n",
      "t1_15000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.940    |             |\n",
      "|     Precision      |    0.985    |    0.984    |\n",
      "|       Recall       |    0.966    |    0.970    |\n",
      "|     F1-measure     |    0.975    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.959 |\n",
      "|   Literature Precision   | 0.971 |\n",
      "|     LiteratureRecall     | 0.966 |\n",
      "|   LiteratureF1-measure   | 0.969 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       169\n",
      "           1       0.97      0.96      0.97       423\n",
      "           2       0.99      0.98      0.98        82\n",
      "           3       0.97      0.95      0.96        93\n",
      "           4       1.00      0.98      0.99        64\n",
      "           5       0.98      0.98      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.97      0.98      0.98       264\n",
      "           8       1.00      0.96      0.98        28\n",
      "           9       0.96      0.97      0.97       158\n",
      "          10       1.00      0.95      0.97        56\n",
      "          11       0.99      0.97      0.98       139\n",
      "          12       0.98      0.98      0.98       330\n",
      "          13       1.00      0.99      0.99        77\n",
      "          14       1.00      0.89      0.94        27\n",
      "          15       0.98      0.98      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.99      0.98      0.99      1194\n",
      "          18       1.00      0.98      0.99        63\n",
      "          19       0.96      0.98      0.97       282\n",
      "          20       0.98      0.97      0.98      1680\n",
      "          21       0.98      0.97      0.97       116\n",
      "          22       0.97      0.97      0.97       257\n",
      "          23       0.99      0.99      0.99       125\n",
      "          24       0.99      0.98      0.99       254\n",
      "          25       0.98      0.98      0.98       450\n",
      "          26       0.98      0.98      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.99      0.97      0.98      1849\n",
      "          29       0.99      0.96      0.97       548\n",
      "          30       0.99      0.96      0.98       285\n",
      "          31       1.00      0.98      0.99        44\n",
      "          32       0.96      0.96      0.96        99\n",
      "          33       0.99      0.96      0.98       171\n",
      "          34       0.99      0.95      0.97       102\n",
      "          35       0.97      0.99      0.98        97\n",
      "          36       0.98      0.98      0.98       304\n",
      "          37       0.99      0.98      0.99       527\n",
      "          38       0.96      0.98      0.97       413\n",
      "          39       1.00      0.97      0.99        68\n",
      "          40       0.97      0.92      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.98      0.96      0.97       298\n",
      "          43       0.98      0.98      0.98      3697\n",
      "          44       0.98      0.98      0.98       322\n",
      "          45       0.99      0.94      0.96       464\n",
      "          46       0.95      0.76      0.84        25\n",
      "          47       0.98      0.97      0.97       330\n",
      "          48       0.99      0.97      0.98       154\n",
      "          49       0.98      0.98      0.98       123\n",
      "          50       0.99      0.91      0.95       104\n",
      "          51       0.99      0.96      0.97       169\n",
      "          52       0.98      0.98      0.98      1184\n",
      "          53       0.99      0.98      0.99       176\n",
      "          54       1.00      0.97      0.98        64\n",
      "          55       0.97      0.99      0.98       140\n",
      "          56       0.98      0.95      0.97      1556\n",
      "          57       0.98      0.97      0.98      1850\n",
      "          58       0.99      0.94      0.97       392\n",
      "          59       0.98      0.97      0.98       532\n",
      "          60       0.99      0.95      0.97       197\n",
      "          61       0.99      0.96      0.98       156\n",
      "          62       0.96      0.97      0.97       233\n",
      "          63       0.98      0.99      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.99      0.97      0.98       661\n",
      "          66       0.96      0.97      0.97        80\n",
      "          67       0.97      0.95      0.96       370\n",
      "          68       0.99      0.97      0.98      2813\n",
      "          69       1.00      0.92      0.96        49\n",
      "          70       0.98      0.98      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.97      0.98     28022\n",
      "   macro avg       0.99      0.97      0.98     28022\n",
      "weighted avg       0.98      0.97      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n",
      "t1_15000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.941    |             |\n",
      "|     Precision      |    0.970    |    0.975    |\n",
      "|       Recall       |    0.982    |    0.980    |\n",
      "|     F1-measure     |    0.976    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.963 |\n",
      "|   Literature Precision   | 0.972 |\n",
      "|     LiteratureRecall     | 0.977 |\n",
      "|   LiteratureF1-measure   | 0.974 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       169\n",
      "           1       0.96      0.98      0.97       423\n",
      "           2       0.99      0.98      0.98        82\n",
      "           3       0.95      0.97      0.96        93\n",
      "           4       0.98      1.00      0.99        64\n",
      "           5       0.97      0.99      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.97      0.98      0.98       264\n",
      "           8       0.97      1.00      0.98        28\n",
      "           9       0.98      0.96      0.97       158\n",
      "          10       0.96      0.98      0.97        56\n",
      "          11       0.99      0.98      0.98       139\n",
      "          12       0.98      0.98      0.98       330\n",
      "          13       0.99      1.00      0.99        77\n",
      "          14       0.90      1.00      0.95        27\n",
      "          15       0.96      1.00      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.98      0.99      0.99      1194\n",
      "          18       0.98      1.00      0.99        63\n",
      "          19       0.95      0.99      0.97       282\n",
      "          20       0.98      0.97      0.98      1680\n",
      "          21       0.96      0.99      0.97       116\n",
      "          22       0.96      0.98      0.97       257\n",
      "          23       0.98      1.00      0.99       125\n",
      "          24       0.98      0.99      0.99       254\n",
      "          25       0.97      0.99      0.98       450\n",
      "          26       0.97      0.99      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.98      0.98      0.98      1849\n",
      "          29       0.97      0.98      0.97       548\n",
      "          30       0.99      0.96      0.98       285\n",
      "          31       0.98      1.00      0.99        44\n",
      "          32       0.98      0.94      0.96        99\n",
      "          33       0.99      0.96      0.98       171\n",
      "          34       0.99      0.95      0.97       102\n",
      "          35       0.96      1.00      0.98        97\n",
      "          36       0.98      0.99      0.98       304\n",
      "          37       0.99      0.99      0.99       527\n",
      "          38       0.95      0.99      0.97       413\n",
      "          39       0.97      1.00      0.99        68\n",
      "          40       0.97      0.92      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.97      0.97      0.97       298\n",
      "          43       0.99      0.98      0.98      3697\n",
      "          44       0.98      0.98      0.98       322\n",
      "          45       0.98      0.95      0.96       464\n",
      "          46       0.80      0.96      0.87        25\n",
      "          47       0.98      0.96      0.97       330\n",
      "          48       0.97      0.99      0.98       154\n",
      "          49       0.98      0.98      0.98       123\n",
      "          50       0.93      0.98      0.95       104\n",
      "          51       0.97      0.98      0.97       169\n",
      "          52       0.97      0.98      0.98      1184\n",
      "          53       0.98      0.99      0.99       176\n",
      "          54       0.97      1.00      0.98        64\n",
      "          55       0.98      0.98      0.98       140\n",
      "          56       0.96      0.97      0.97      1556\n",
      "          57       0.97      0.98      0.98      1850\n",
      "          58       0.95      0.99      0.97       392\n",
      "          59       0.96      0.99      0.98       532\n",
      "          60       0.96      0.98      0.97       197\n",
      "          61       0.97      0.98      0.98       156\n",
      "          62       0.97      0.96      0.97       233\n",
      "          63       0.98      0.99      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.97      0.99      0.98       661\n",
      "          66       0.94      1.00      0.97        80\n",
      "          67       0.98      0.95      0.96       370\n",
      "          68       0.98      0.98      0.98      2813\n",
      "          69       0.94      0.98      0.96        49\n",
      "          70       0.96      1.00      0.98        55\n",
      "\n",
      "   micro avg       0.97      0.98      0.98     28022\n",
      "   macro avg       0.97      0.98      0.98     28022\n",
      "weighted avg       0.98      0.98      0.98     28022\n",
      " samples avg       0.97      0.98      0.97     28022\n",
      "\n",
      "t1_15000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.940    |             |\n",
      "|     Precision      |    0.974    |    0.980    |\n",
      "|       Recall       |    0.979    |    0.974    |\n",
      "|     F1-measure     |    0.976    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.961 |\n",
      "|   Literature Precision   | 0.971 |\n",
      "|     LiteratureRecall     | 0.972 |\n",
      "|   LiteratureF1-measure   | 0.971 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       169\n",
      "           1       0.95      0.98      0.97       423\n",
      "           2       0.98      0.99      0.98        82\n",
      "           3       0.96      0.96      0.96        93\n",
      "           4       0.98      1.00      0.99        64\n",
      "           5       0.97      0.99      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.97      0.99      0.98       264\n",
      "           8       0.97      1.00      0.98        28\n",
      "           9       0.98      0.96      0.97       158\n",
      "          10       0.96      0.98      0.97        56\n",
      "          11       0.98      0.99      0.98       139\n",
      "          12       0.97      0.99      0.98       330\n",
      "          13       0.99      1.00      0.99        77\n",
      "          14       0.96      0.93      0.94        27\n",
      "          15       0.96      1.00      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.99      0.99      0.99      1194\n",
      "          18       0.98      1.00      0.99        63\n",
      "          19       0.99      0.95      0.97       282\n",
      "          20       0.99      0.96      0.97      1680\n",
      "          21       0.97      0.98      0.97       116\n",
      "          22       0.98      0.96      0.97       257\n",
      "          23       0.98      1.00      0.99       125\n",
      "          24       0.99      0.98      0.99       254\n",
      "          25       0.97      1.00      0.98       450\n",
      "          26       0.97      0.99      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.99      0.96      0.98      1849\n",
      "          29       0.98      0.97      0.97       548\n",
      "          30       0.98      0.97      0.98       285\n",
      "          31       0.98      1.00      0.99        44\n",
      "          32       0.95      0.97      0.96        99\n",
      "          33       0.98      0.97      0.98       171\n",
      "          34       0.98      0.96      0.97       102\n",
      "          35       1.00      0.96      0.98        97\n",
      "          36       0.97      0.99      0.98       304\n",
      "          37       0.98      0.99      0.99       527\n",
      "          38       0.96      0.98      0.97       413\n",
      "          39       0.97      1.00      0.99        68\n",
      "          40       0.95      0.93      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.95      0.99      0.97       298\n",
      "          43       0.99      0.97      0.98      3697\n",
      "          44       0.97      0.99      0.98       322\n",
      "          45       0.97      0.96      0.97       464\n",
      "          46       0.78      1.00      0.88        25\n",
      "          47       0.98      0.96      0.97       330\n",
      "          48       0.98      0.99      0.98       154\n",
      "          49       0.98      0.98      0.98       123\n",
      "          50       0.93      0.98      0.95       104\n",
      "          51       0.98      0.96      0.97       169\n",
      "          52       0.99      0.96      0.98      1184\n",
      "          53       0.98      0.99      0.99       176\n",
      "          54       1.00      0.97      0.98        64\n",
      "          55       0.98      0.98      0.98       140\n",
      "          56       0.96      0.97      0.97      1556\n",
      "          57       0.97      0.99      0.98      1850\n",
      "          58       0.95      0.98      0.97       392\n",
      "          59       0.97      0.98      0.98       532\n",
      "          60       0.96      0.99      0.97       197\n",
      "          61       0.97      0.99      0.98       156\n",
      "          62       0.99      0.94      0.96       233\n",
      "          63       0.99      0.98      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.98      0.98      0.98       661\n",
      "          66       0.96      0.97      0.97        80\n",
      "          67       0.98      0.95      0.96       370\n",
      "          68       1.00      0.97      0.98      2813\n",
      "          69       0.94      0.98      0.96        49\n",
      "          70       0.98      0.98      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.97      0.98     28022\n",
      "   macro avg       0.97      0.98      0.98     28022\n",
      "weighted avg       0.98      0.97      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n",
      "t2_15000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.940    |             |\n",
      "|     Precision      |    0.980    |    0.981    |\n",
      "|       Recall       |    0.971    |    0.973    |\n",
      "|     F1-measure     |    0.975    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.960 |\n",
      "|   Literature Precision   | 0.971 |\n",
      "|     LiteratureRecall     | 0.970 |\n",
      "|   LiteratureF1-measure   | 0.971 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       169\n",
      "           1       0.95      0.99      0.97       423\n",
      "           2       0.99      0.98      0.98        82\n",
      "           3       0.99      0.92      0.96        93\n",
      "           4       1.00      0.98      0.99        64\n",
      "           5       0.98      0.98      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.98      0.98      0.98       264\n",
      "           8       1.00      0.96      0.98        28\n",
      "           9       0.97      0.96      0.97       158\n",
      "          10       0.96      0.98      0.97        56\n",
      "          11       0.98      0.99      0.98       139\n",
      "          12       0.97      0.99      0.98       330\n",
      "          13       1.00      0.99      0.99        77\n",
      "          14       0.96      0.93      0.94        27\n",
      "          15       1.00      0.96      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.99      0.98      0.99      1194\n",
      "          18       1.00      0.98      0.99        63\n",
      "          19       0.99      0.95      0.97       282\n",
      "          20       0.97      0.98      0.98      1680\n",
      "          21       0.97      0.98      0.97       116\n",
      "          22       0.99      0.95      0.97       257\n",
      "          23       0.98      1.00      0.99       125\n",
      "          24       0.98      0.99      0.99       254\n",
      "          25       0.98      0.98      0.98       450\n",
      "          26       0.98      0.98      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       1.00      0.96      0.98      1849\n",
      "          29       0.99      0.96      0.97       548\n",
      "          30       0.97      0.98      0.98       285\n",
      "          31       0.98      1.00      0.99        44\n",
      "          32       0.99      0.93      0.96        99\n",
      "          33       0.99      0.96      0.98       171\n",
      "          34       0.99      0.95      0.97       102\n",
      "          35       0.97      0.99      0.98        97\n",
      "          36       0.98      0.98      0.98       304\n",
      "          37       0.99      0.99      0.99       527\n",
      "          38       0.97      0.97      0.97       413\n",
      "          39       1.00      0.97      0.99        68\n",
      "          40       0.98      0.91      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.98      0.96      0.97       298\n",
      "          43       0.98      0.99      0.98      3697\n",
      "          44       0.98      0.99      0.98       322\n",
      "          45       0.98      0.95      0.97       464\n",
      "          46       0.82      0.92      0.87        25\n",
      "          47       0.98      0.97      0.97       330\n",
      "          48       0.99      0.97      0.98       154\n",
      "          49       0.97      0.99      0.98       123\n",
      "          50       0.98      0.92      0.95       104\n",
      "          51       0.98      0.97      0.97       169\n",
      "          52       0.98      0.98      0.98      1184\n",
      "          53       0.99      0.98      0.99       176\n",
      "          54       1.00      0.97      0.98        64\n",
      "          55       0.99      0.97      0.98       140\n",
      "          56       0.98      0.95      0.97      1556\n",
      "          57       0.99      0.97      0.98      1850\n",
      "          58       0.97      0.96      0.97       392\n",
      "          59       0.99      0.96      0.98       532\n",
      "          60       0.97      0.97      0.97       197\n",
      "          61       0.97      0.99      0.98       156\n",
      "          62       0.96      0.97      0.97       233\n",
      "          63       0.98      0.99      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.98      0.98      0.98       661\n",
      "          66       0.96      0.97      0.97        80\n",
      "          67       0.98      0.95      0.96       370\n",
      "          68       0.98      0.98      0.98      2813\n",
      "          69       0.98      0.94      0.96        49\n",
      "          70       0.98      0.98      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.97      0.98     28022\n",
      "   macro avg       0.98      0.97      0.98     28022\n",
      "weighted avg       0.98      0.97      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n",
      "t2_15000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.941    |             |\n",
      "|     Precision      |    0.987    |    0.985    |\n",
      "|       Recall       |    0.963    |    0.969    |\n",
      "|     F1-measure     |    0.975    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.959 |\n",
      "|   Literature Precision   | 0.972 |\n",
      "|     LiteratureRecall     | 0.966 |\n",
      "|   LiteratureF1-measure   | 0.969 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       169\n",
      "           1       0.99      0.95      0.97       423\n",
      "           2       1.00      0.96      0.98        82\n",
      "           3       0.98      0.94      0.96        93\n",
      "           4       1.00      0.98      0.99        64\n",
      "           5       1.00      0.96      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.98      0.98      0.98       264\n",
      "           8       0.97      1.00      0.98        28\n",
      "           9       0.99      0.95      0.97       158\n",
      "          10       1.00      0.95      0.97        56\n",
      "          11       0.97      0.99      0.98       139\n",
      "          12       0.98      0.98      0.98       330\n",
      "          13       1.00      0.99      0.99        77\n",
      "          14       1.00      0.89      0.94        27\n",
      "          15       1.00      0.96      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.98      0.99      0.99      1194\n",
      "          18       1.00      0.98      0.99        63\n",
      "          19       0.99      0.94      0.97       282\n",
      "          20       0.99      0.96      0.97      1680\n",
      "          21       0.99      0.96      0.97       116\n",
      "          22       1.00      0.95      0.97       257\n",
      "          23       0.99      0.99      0.99       125\n",
      "          24       0.99      0.98      0.99       254\n",
      "          25       0.99      0.97      0.98       450\n",
      "          26       0.99      0.97      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.99      0.97      0.98      1849\n",
      "          29       0.99      0.96      0.97       548\n",
      "          30       1.00      0.95      0.97       285\n",
      "          31       1.00      0.98      0.99        44\n",
      "          32       0.99      0.93      0.96        99\n",
      "          33       0.99      0.96      0.98       171\n",
      "          34       0.99      0.95      0.97       102\n",
      "          35       1.00      0.96      0.98        97\n",
      "          36       0.99      0.98      0.98       304\n",
      "          37       1.00      0.98      0.99       527\n",
      "          38       0.96      0.97      0.97       413\n",
      "          39       0.99      0.99      0.99        68\n",
      "          40       0.97      0.92      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.99      0.95      0.97       298\n",
      "          43       0.98      0.98      0.98      3697\n",
      "          44       0.97      0.99      0.98       322\n",
      "          45       0.98      0.95      0.97       464\n",
      "          46       0.95      0.76      0.84        25\n",
      "          47       0.99      0.95      0.97       330\n",
      "          48       1.00      0.97      0.98       154\n",
      "          49       0.99      0.97      0.98       123\n",
      "          50       0.99      0.91      0.95       104\n",
      "          51       0.99      0.95      0.97       169\n",
      "          52       0.98      0.97      0.98      1184\n",
      "          53       0.98      0.99      0.99       176\n",
      "          54       0.98      0.98      0.98        64\n",
      "          55       0.99      0.96      0.98       140\n",
      "          56       0.97      0.97      0.97      1556\n",
      "          57       0.99      0.96      0.98      1850\n",
      "          58       0.99      0.95      0.97       392\n",
      "          59       0.97      0.98      0.98       532\n",
      "          60       0.96      0.98      0.97       197\n",
      "          61       0.99      0.97      0.98       156\n",
      "          62       0.99      0.94      0.96       233\n",
      "          63       0.99      0.98      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.98      0.98      0.98       661\n",
      "          66       0.95      0.99      0.97        80\n",
      "          67       0.99      0.94      0.96       370\n",
      "          68       0.99      0.98      0.98      2813\n",
      "          69       0.96      0.96      0.96        49\n",
      "          70       1.00      0.96      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.97      0.98     28022\n",
      "   macro avg       0.99      0.96      0.97     28022\n",
      "weighted avg       0.98      0.97      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n",
      "t2_15000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.941    |             |\n",
      "|     Precision      |    0.969    |    0.972    |\n",
      "|       Recall       |    0.983    |    0.982    |\n",
      "|     F1-measure     |    0.976    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.965 |\n",
      "|   Literature Precision   | 0.972 |\n",
      "|     LiteratureRecall     | 0.981 |\n",
      "|   LiteratureF1-measure   | 0.977 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       169\n",
      "           1       0.95      0.98      0.97       423\n",
      "           2       0.98      0.99      0.98        82\n",
      "           3       0.95      0.97      0.96        93\n",
      "           4       0.98      1.00      0.99        64\n",
      "           5       0.96      1.00      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.97      0.98      0.98       264\n",
      "           8       0.97      1.00      0.98        28\n",
      "           9       0.96      0.97      0.97       158\n",
      "          10       0.96      0.98      0.97        56\n",
      "          11       0.98      0.99      0.98       139\n",
      "          12       0.97      0.99      0.98       330\n",
      "          13       0.99      1.00      0.99        77\n",
      "          14       0.93      0.96      0.95        27\n",
      "          15       0.98      0.98      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.99      0.98      0.99      1194\n",
      "          18       0.98      1.00      0.99        63\n",
      "          19       0.96      0.97      0.97       282\n",
      "          20       0.97      0.98      0.98      1680\n",
      "          21       0.97      0.97      0.97       116\n",
      "          22       0.97      0.98      0.97       257\n",
      "          23       0.98      1.00      0.99       125\n",
      "          24       0.98      0.99      0.99       254\n",
      "          25       0.97      1.00      0.98       450\n",
      "          26       0.96      1.00      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.99      0.96      0.98      1849\n",
      "          29       0.97      0.97      0.97       548\n",
      "          30       0.98      0.98      0.98       285\n",
      "          31       1.00      0.98      0.99        44\n",
      "          32       0.94      0.98      0.96        99\n",
      "          33       0.98      0.98      0.98       171\n",
      "          34       0.98      0.96      0.97       102\n",
      "          35       0.97      0.99      0.98        97\n",
      "          36       0.97      0.99      0.98       304\n",
      "          37       0.99      0.99      0.99       527\n",
      "          38       0.95      0.99      0.97       413\n",
      "          39       0.97      1.00      0.99        68\n",
      "          40       0.94      0.94      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.96      0.98      0.97       298\n",
      "          43       0.97      0.99      0.98      3697\n",
      "          44       0.98      0.98      0.98       322\n",
      "          45       0.96      0.97      0.97       464\n",
      "          46       0.80      0.96      0.87        25\n",
      "          47       0.97      0.98      0.97       330\n",
      "          48       0.99      0.98      0.98       154\n",
      "          49       0.96      1.00      0.98       123\n",
      "          50       0.93      0.98      0.95       104\n",
      "          51       0.97      0.98      0.97       169\n",
      "          52       0.99      0.97      0.98      1184\n",
      "          53       0.98      0.99      0.99       176\n",
      "          54       0.97      1.00      0.98        64\n",
      "          55       0.97      0.99      0.98       140\n",
      "          56       0.96      0.98      0.97      1556\n",
      "          57       0.98      0.97      0.98      1850\n",
      "          58       0.96      0.98      0.97       392\n",
      "          59       0.97      0.98      0.98       532\n",
      "          60       0.96      0.98      0.97       197\n",
      "          61       0.96      0.99      0.98       156\n",
      "          62       0.95      0.98      0.97       233\n",
      "          63       0.98      0.99      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.98      0.98      0.98       661\n",
      "          66       0.94      1.00      0.97        80\n",
      "          67       0.97      0.96      0.96       370\n",
      "          68       0.97      0.99      0.98      2813\n",
      "          69       0.96      0.96      0.96        49\n",
      "          70       0.96      1.00      0.98        55\n",
      "\n",
      "   micro avg       0.97      0.98      0.98     28022\n",
      "   macro avg       0.97      0.98      0.98     28022\n",
      "weighted avg       0.97      0.98      0.98     28022\n",
      " samples avg       0.97      0.98      0.97     28022\n",
      "\n",
      "t1_20000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.940    |             |\n",
      "|     Precision      |    0.975    |    0.979    |\n",
      "|       Recall       |    0.977    |    0.975    |\n",
      "|     F1-measure     |    0.976    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.961 |\n",
      "|   Literature Precision   | 0.972 |\n",
      "|     LiteratureRecall     | 0.973 |\n",
      "|   LiteratureF1-measure   | 0.972 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       169\n",
      "           1       0.95      0.98      0.97       423\n",
      "           2       0.98      0.99      0.98        82\n",
      "           3       0.96      0.96      0.96        93\n",
      "           4       0.98      1.00      0.99        64\n",
      "           5       0.97      0.99      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.97      0.98      0.98       264\n",
      "           8       0.97      1.00      0.98        28\n",
      "           9       0.96      0.97      0.97       158\n",
      "          10       1.00      0.95      0.97        56\n",
      "          11       0.97      1.00      0.98       139\n",
      "          12       0.98      0.98      0.98       330\n",
      "          13       1.00      0.99      0.99        77\n",
      "          14       0.90      1.00      0.95        27\n",
      "          15       0.96      1.00      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.99      0.98      0.99      1194\n",
      "          18       0.98      1.00      0.99        63\n",
      "          19       0.97      0.97      0.97       282\n",
      "          20       0.97      0.98      0.98      1680\n",
      "          21       0.97      0.97      0.97       116\n",
      "          22       0.97      0.98      0.97       257\n",
      "          23       0.98      1.00      0.99       125\n",
      "          24       0.99      0.98      0.99       254\n",
      "          25       0.99      0.97      0.98       450\n",
      "          26       0.97      0.99      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.97      0.98      0.98      1849\n",
      "          29       0.97      0.97      0.97       548\n",
      "          30       0.99      0.96      0.98       285\n",
      "          31       0.98      1.00      0.99        44\n",
      "          32       0.95      0.97      0.96        99\n",
      "          33       0.98      0.97      0.98       171\n",
      "          34       0.98      0.96      0.97       102\n",
      "          35       0.96      1.00      0.98        97\n",
      "          36       0.98      0.98      0.98       304\n",
      "          37       0.99      0.98      0.99       527\n",
      "          38       0.97      0.97      0.97       413\n",
      "          39       0.99      0.99      0.99        68\n",
      "          40       0.96      0.92      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.96      0.98      0.97       298\n",
      "          43       0.99      0.97      0.98      3697\n",
      "          44       0.99      0.97      0.98       322\n",
      "          45       0.96      0.97      0.97       464\n",
      "          46       0.85      0.88      0.86        25\n",
      "          47       0.98      0.97      0.97       330\n",
      "          48       0.99      0.97      0.98       154\n",
      "          49       1.00      0.96      0.98       123\n",
      "          50       0.97      0.93      0.95       104\n",
      "          51       0.96      0.99      0.97       169\n",
      "          52       0.98      0.97      0.98      1184\n",
      "          53       0.98      0.99      0.99       176\n",
      "          54       0.98      0.98      0.98        64\n",
      "          55       0.98      0.98      0.98       140\n",
      "          56       0.98      0.95      0.97      1556\n",
      "          57       0.98      0.97      0.98      1850\n",
      "          58       0.98      0.96      0.97       392\n",
      "          59       0.98      0.98      0.98       532\n",
      "          60       0.96      0.98      0.97       197\n",
      "          61       0.97      0.99      0.98       156\n",
      "          62       0.98      0.95      0.97       233\n",
      "          63       0.99      0.98      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.97      0.99      0.98       661\n",
      "          66       0.96      0.97      0.97        80\n",
      "          67       0.96      0.96      0.96       370\n",
      "          68       0.99      0.98      0.98      2813\n",
      "          69       0.96      0.96      0.96        49\n",
      "          70       0.98      0.98      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.97      0.98     28022\n",
      "   macro avg       0.97      0.98      0.98     28022\n",
      "weighted avg       0.98      0.97      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n",
      "t1_20000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.941    |             |\n",
      "|     Precision      |    0.969    |    0.977    |\n",
      "|       Recall       |    0.983    |    0.977    |\n",
      "|     F1-measure     |    0.976    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.962 |\n",
      "|   Literature Precision   | 0.972 |\n",
      "|     LiteratureRecall     | 0.974 |\n",
      "|   LiteratureF1-measure   | 0.973 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       169\n",
      "           1       0.96      0.97      0.97       423\n",
      "           2       0.98      0.99      0.98        82\n",
      "           3       0.96      0.96      0.96        93\n",
      "           4       0.98      1.00      0.99        64\n",
      "           5       0.96      1.00      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.98      0.98      0.98       264\n",
      "           8       0.97      1.00      0.98        28\n",
      "           9       0.97      0.97      0.97       158\n",
      "          10       0.96      0.98      0.97        56\n",
      "          11       0.97      0.99      0.98       139\n",
      "          12       0.97      0.99      0.98       330\n",
      "          13       0.99      1.00      0.99        77\n",
      "          14       0.90      1.00      0.95        27\n",
      "          15       0.96      1.00      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.98      0.99      0.99      1194\n",
      "          18       0.98      1.00      0.99        63\n",
      "          19       0.95      0.99      0.97       282\n",
      "          20       0.98      0.97      0.98      1680\n",
      "          21       0.98      0.97      0.97       116\n",
      "          22       0.97      0.98      0.97       257\n",
      "          23       0.98      1.00      0.99       125\n",
      "          24       0.98      0.99      0.99       254\n",
      "          25       0.97      1.00      0.98       450\n",
      "          26       0.96      1.00      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.97      0.98      0.98      1849\n",
      "          29       0.98      0.97      0.97       548\n",
      "          30       0.98      0.98      0.98       285\n",
      "          31       0.98      1.00      0.99        44\n",
      "          32       0.94      0.98      0.96        99\n",
      "          33       0.98      0.98      0.98       171\n",
      "          34       0.98      0.96      0.97       102\n",
      "          35       0.97      0.99      0.98        97\n",
      "          36       0.97      0.99      0.98       304\n",
      "          37       0.98      0.99      0.99       527\n",
      "          38       0.97      0.97      0.97       413\n",
      "          39       0.97      1.00      0.99        68\n",
      "          40       0.94      0.94      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.96      0.98      0.97       298\n",
      "          43       0.99      0.97      0.98      3697\n",
      "          44       0.97      0.99      0.98       322\n",
      "          45       0.97      0.96      0.97       464\n",
      "          46       0.80      0.96      0.87        25\n",
      "          47       0.97      0.97      0.97       330\n",
      "          48       0.97      0.99      0.98       154\n",
      "          49       0.97      0.99      0.98       123\n",
      "          50       0.95      0.95      0.95       104\n",
      "          51       0.97      0.98      0.97       169\n",
      "          52       0.99      0.96      0.98      1184\n",
      "          53       0.98      0.99      0.99       176\n",
      "          54       0.97      1.00      0.98        64\n",
      "          55       0.97      0.99      0.98       140\n",
      "          56       0.97      0.97      0.97      1556\n",
      "          57       0.98      0.97      0.98      1850\n",
      "          58       0.95      0.99      0.97       392\n",
      "          59       0.96      0.99      0.98       532\n",
      "          60       0.96      0.98      0.97       197\n",
      "          61       0.96      0.99      0.98       156\n",
      "          62       0.95      0.98      0.97       233\n",
      "          63       0.99      0.98      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.98      0.98      0.98       661\n",
      "          66       0.94      1.00      0.97        80\n",
      "          67       0.97      0.96      0.96       370\n",
      "          68       0.99      0.98      0.98      2813\n",
      "          69       0.94      0.98      0.96        49\n",
      "          70       0.96      1.00      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     28022\n",
      "   macro avg       0.97      0.98      0.98     28022\n",
      "weighted avg       0.98      0.98      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n",
      "t1_20000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.942    |             |\n",
      "|     Precision      |    0.991    |    0.987    |\n",
      "|       Recall       |    0.960    |    0.967    |\n",
      "|     F1-measure     |    0.975    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.958 |\n",
      "|   Literature Precision   | 0.972 |\n",
      "|     LiteratureRecall     | 0.963 |\n",
      "|   LiteratureF1-measure   | 0.967 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       169\n",
      "           1       0.99      0.94      0.97       423\n",
      "           2       1.00      0.96      0.98        82\n",
      "           3       1.00      0.91      0.96        93\n",
      "           4       1.00      0.98      0.99        64\n",
      "           5       0.98      0.98      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.99      0.97      0.98       264\n",
      "           8       1.00      0.96      0.98        28\n",
      "           9       0.99      0.94      0.97       158\n",
      "          10       1.00      0.95      0.97        56\n",
      "          11       1.00      0.96      0.98       139\n",
      "          12       0.99      0.97      0.98       330\n",
      "          13       1.00      0.99      0.99        77\n",
      "          14       1.00      0.89      0.94        27\n",
      "          15       1.00      0.96      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.99      0.98      0.99      1194\n",
      "          18       1.00      0.98      0.99        63\n",
      "          19       1.00      0.94      0.97       282\n",
      "          20       0.98      0.97      0.98      1680\n",
      "          21       0.99      0.96      0.97       116\n",
      "          22       0.98      0.96      0.97       257\n",
      "          23       1.00      0.98      0.99       125\n",
      "          24       1.00      0.97      0.99       254\n",
      "          25       0.99      0.98      0.98       450\n",
      "          26       0.99      0.97      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       1.00      0.96      0.98      1849\n",
      "          29       0.99      0.96      0.97       548\n",
      "          30       1.00      0.95      0.97       285\n",
      "          31       1.00      0.98      0.99        44\n",
      "          32       0.98      0.94      0.96        99\n",
      "          33       0.99      0.96      0.98       171\n",
      "          34       0.99      0.95      0.97       102\n",
      "          35       1.00      0.96      0.98        97\n",
      "          36       0.99      0.97      0.98       304\n",
      "          37       0.99      0.98      0.99       527\n",
      "          38       0.98      0.96      0.97       413\n",
      "          39       0.99      0.99      0.99        68\n",
      "          40       1.00      0.89      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.99      0.95      0.97       298\n",
      "          43       0.99      0.98      0.98      3697\n",
      "          44       0.99      0.97      0.98       322\n",
      "          45       0.97      0.97      0.97       464\n",
      "          46       0.95      0.76      0.84        25\n",
      "          47       1.00      0.95      0.97       330\n",
      "          48       0.99      0.98      0.98       154\n",
      "          49       0.99      0.97      0.98       123\n",
      "          50       0.99      0.91      0.95       104\n",
      "          51       0.98      0.97      0.97       169\n",
      "          52       0.98      0.98      0.98      1184\n",
      "          53       0.99      0.98      0.99       176\n",
      "          54       0.98      0.98      0.98        64\n",
      "          55       1.00      0.96      0.98       140\n",
      "          56       0.99      0.95      0.97      1556\n",
      "          57       0.99      0.97      0.98      1850\n",
      "          58       0.99      0.94      0.97       392\n",
      "          59       0.99      0.96      0.98       532\n",
      "          60       1.00      0.94      0.97       197\n",
      "          61       0.98      0.97      0.98       156\n",
      "          62       0.97      0.96      0.97       233\n",
      "          63       1.00      0.97      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.98      0.97      0.98       661\n",
      "          66       0.99      0.95      0.97        80\n",
      "          67       0.99      0.94      0.96       370\n",
      "          68       0.98      0.99      0.98      2813\n",
      "          69       0.98      0.94      0.96        49\n",
      "          70       1.00      0.96      0.98        55\n",
      "\n",
      "   micro avg       0.99      0.97      0.98     28022\n",
      "   macro avg       0.99      0.96      0.97     28022\n",
      "weighted avg       0.99      0.97      0.98     28022\n",
      " samples avg       0.97      0.96      0.96     28022\n",
      "\n",
      "t2_20000_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.941    |             |\n",
      "|     Precision      |    0.986    |    0.981    |\n",
      "|       Recall       |    0.965    |    0.973    |\n",
      "|     F1-measure     |    0.975    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.961 |\n",
      "|   Literature Precision   | 0.972 |\n",
      "|     LiteratureRecall     | 0.971 |\n",
      "|   LiteratureF1-measure   | 0.971 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       169\n",
      "           1       0.95      0.98      0.97       423\n",
      "           2       1.00      0.96      0.98        82\n",
      "           3       0.98      0.94      0.96        93\n",
      "           4       1.00      0.98      0.99        64\n",
      "           5       0.98      0.98      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.99      0.97      0.98       264\n",
      "           8       1.00      0.96      0.98        28\n",
      "           9       0.99      0.94      0.97       158\n",
      "          10       1.00      0.95      0.97        56\n",
      "          11       0.99      0.98      0.98       139\n",
      "          12       0.99      0.97      0.98       330\n",
      "          13       1.00      0.99      0.99        77\n",
      "          14       1.00      0.89      0.94        27\n",
      "          15       0.98      0.98      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.99      0.99      0.99      1194\n",
      "          18       1.00      0.98      0.99        63\n",
      "          19       0.98      0.95      0.97       282\n",
      "          20       0.97      0.98      0.98      1680\n",
      "          21       0.99      0.96      0.97       116\n",
      "          22       1.00      0.95      0.97       257\n",
      "          23       1.00      0.98      0.99       125\n",
      "          24       1.00      0.97      0.99       254\n",
      "          25       0.99      0.97      0.98       450\n",
      "          26       0.98      0.98      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.98      0.97      0.98      1849\n",
      "          29       0.98      0.97      0.97       548\n",
      "          30       0.99      0.96      0.98       285\n",
      "          31       0.98      1.00      0.99        44\n",
      "          32       0.99      0.93      0.96        99\n",
      "          33       1.00      0.95      0.98       171\n",
      "          34       0.99      0.95      0.97       102\n",
      "          35       1.00      0.96      0.98        97\n",
      "          36       0.99      0.97      0.98       304\n",
      "          37       0.99      0.99      0.99       527\n",
      "          38       0.99      0.94      0.97       413\n",
      "          39       0.99      0.99      0.99        68\n",
      "          40       0.99      0.90      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.98      0.96      0.97       298\n",
      "          43       0.97      0.99      0.98      3697\n",
      "          44       0.99      0.98      0.98       322\n",
      "          45       0.97      0.96      0.97       464\n",
      "          46       0.88      0.84      0.86        25\n",
      "          47       0.99      0.95      0.97       330\n",
      "          48       0.98      0.99      0.98       154\n",
      "          49       0.98      0.98      0.98       123\n",
      "          50       1.00      0.90      0.95       104\n",
      "          51       0.97      0.98      0.97       169\n",
      "          52       0.99      0.97      0.98      1184\n",
      "          53       0.99      0.98      0.99       176\n",
      "          54       0.98      0.98      0.98        64\n",
      "          55       0.99      0.96      0.98       140\n",
      "          56       0.97      0.97      0.97      1556\n",
      "          57       0.98      0.97      0.98      1850\n",
      "          58       0.98      0.96      0.97       392\n",
      "          59       0.99      0.96      0.98       532\n",
      "          60       0.97      0.97      0.97       197\n",
      "          61       0.99      0.97      0.98       156\n",
      "          62       0.97      0.96      0.97       233\n",
      "          63       0.99      0.98      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.97      0.98      0.98       661\n",
      "          66       0.99      0.95      0.97        80\n",
      "          67       0.98      0.95      0.96       370\n",
      "          68       0.98      0.98      0.98      2813\n",
      "          69       1.00      0.92      0.96        49\n",
      "          70       1.00      0.96      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.97      0.98     28022\n",
      "   macro avg       0.99      0.96      0.98     28022\n",
      "weighted avg       0.98      0.97      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n",
      "t2_20000_relu_leaky_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.941    |             |\n",
      "|     Precision      |    0.979    |    0.980    |\n",
      "|       Recall       |    0.972    |    0.974    |\n",
      "|     F1-measure     |    0.975    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.961 |\n",
      "|   Literature Precision   | 0.972 |\n",
      "|     LiteratureRecall     | 0.972 |\n",
      "|   LiteratureF1-measure   | 0.972 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       169\n",
      "           1       0.96      0.97      0.97       423\n",
      "           2       0.98      0.99      0.98        82\n",
      "           3       0.98      0.94      0.96        93\n",
      "           4       0.98      1.00      0.99        64\n",
      "           5       0.97      0.99      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.99      0.97      0.98       264\n",
      "           8       1.00      0.96      0.98        28\n",
      "           9       0.99      0.94      0.97       158\n",
      "          10       0.96      0.98      0.97        56\n",
      "          11       0.99      0.97      0.98       139\n",
      "          12       0.99      0.97      0.98       330\n",
      "          13       1.00      0.99      0.99        77\n",
      "          14       0.96      0.93      0.94        27\n",
      "          15       0.96      1.00      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.99      0.98      0.99      1194\n",
      "          18       0.98      1.00      0.99        63\n",
      "          19       0.99      0.95      0.97       282\n",
      "          20       0.99      0.96      0.97      1680\n",
      "          21       0.98      0.97      0.97       116\n",
      "          22       0.97      0.97      0.97       257\n",
      "          23       0.99      0.99      0.99       125\n",
      "          24       0.99      0.98      0.99       254\n",
      "          25       0.97      0.99      0.98       450\n",
      "          26       0.97      0.99      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.98      0.98      0.98      1849\n",
      "          29       0.98      0.97      0.97       548\n",
      "          30       0.98      0.98      0.98       285\n",
      "          31       0.98      1.00      0.99        44\n",
      "          32       0.94      0.98      0.96        99\n",
      "          33       0.99      0.96      0.98       171\n",
      "          34       0.98      0.96      0.97       102\n",
      "          35       0.98      0.98      0.98        97\n",
      "          36       0.98      0.98      0.98       304\n",
      "          37       0.98      0.99      0.99       527\n",
      "          38       0.97      0.96      0.97       413\n",
      "          39       0.99      0.99      0.99        68\n",
      "          40       0.99      0.90      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.98      0.96      0.97       298\n",
      "          43       0.97      0.99      0.98      3697\n",
      "          44       1.00      0.97      0.98       322\n",
      "          45       0.97      0.96      0.97       464\n",
      "          46       0.82      0.92      0.87        25\n",
      "          47       0.97      0.98      0.97       330\n",
      "          48       0.99      0.98      0.98       154\n",
      "          49       0.98      0.98      0.98       123\n",
      "          50       0.98      0.92      0.95       104\n",
      "          51       0.99      0.95      0.97       169\n",
      "          52       0.98      0.98      0.98      1184\n",
      "          53       1.00      0.97      0.99       176\n",
      "          54       0.97      1.00      0.98        64\n",
      "          55       0.97      0.99      0.98       140\n",
      "          56       0.97      0.96      0.97      1556\n",
      "          57       0.99      0.97      0.98      1850\n",
      "          58       0.98      0.95      0.97       392\n",
      "          59       0.98      0.98      0.98       532\n",
      "          60       0.96      0.99      0.97       197\n",
      "          61       0.99      0.97      0.98       156\n",
      "          62       0.98      0.95      0.97       233\n",
      "          63       0.99      0.98      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       0.99      0.97      0.98       661\n",
      "          66       0.99      0.95      0.97        80\n",
      "          67       0.98      0.95      0.96       370\n",
      "          68       0.98      0.98      0.98      2813\n",
      "          69       0.96      0.96      0.96        49\n",
      "          70       0.98      0.98      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.97      0.98     28022\n",
      "   macro avg       0.98      0.97      0.98     28022\n",
      "weighted avg       0.98      0.97      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n",
      "t2_20000_biploar_step_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.002    |             |\n",
      "| Term Wise Accuracy |    0.998    |             |\n",
      "|      Accuracy      |    0.941    |             |\n",
      "|     Precision      |    0.984    |    0.983    |\n",
      "|       Recall       |    0.967    |    0.971    |\n",
      "|     F1-measure     |    0.975    |    0.977    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.002 |\n",
      "|   Literature Accuracy    | 0.960 |\n",
      "|   Literature Precision   | 0.972 |\n",
      "|     LiteratureRecall     | 0.969 |\n",
      "|   LiteratureF1-measure   | 0.970 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       169\n",
      "           1       0.99      0.94      0.97       423\n",
      "           2       1.00      0.96      0.98        82\n",
      "           3       0.97      0.95      0.96        93\n",
      "           4       1.00      0.98      0.99        64\n",
      "           5       0.98      0.98      0.98       129\n",
      "           6       1.00      1.00      1.00        71\n",
      "           7       0.97      0.99      0.98       264\n",
      "           8       0.97      1.00      0.98        28\n",
      "           9       1.00      0.94      0.97       158\n",
      "          10       0.98      0.96      0.97        56\n",
      "          11       0.98      0.99      0.98       139\n",
      "          12       0.99      0.97      0.98       330\n",
      "          13       0.99      1.00      0.99        77\n",
      "          14       1.00      0.89      0.94        27\n",
      "          15       1.00      0.96      0.98        52\n",
      "          16       1.00      0.98      0.99        59\n",
      "          17       0.99      0.98      0.99      1194\n",
      "          18       1.00      0.98      0.99        63\n",
      "          19       0.98      0.95      0.97       282\n",
      "          20       0.98      0.97      0.98      1680\n",
      "          21       1.00      0.95      0.97       116\n",
      "          22       0.99      0.95      0.97       257\n",
      "          23       0.99      0.99      0.99       125\n",
      "          24       0.98      0.99      0.99       254\n",
      "          25       1.00      0.96      0.98       450\n",
      "          26       0.98      0.98      0.98       351\n",
      "          27       1.00      1.00      1.00        48\n",
      "          28       0.99      0.96      0.98      1849\n",
      "          29       0.98      0.97      0.97       548\n",
      "          30       0.99      0.96      0.98       285\n",
      "          31       0.98      1.00      0.99        44\n",
      "          32       0.99      0.93      0.96        99\n",
      "          33       0.99      0.96      0.98       171\n",
      "          34       0.99      0.95      0.97       102\n",
      "          35       1.00      0.96      0.98        97\n",
      "          36       0.97      0.99      0.98       304\n",
      "          37       1.00      0.98      0.99       527\n",
      "          38       0.97      0.97      0.97       413\n",
      "          39       0.97      1.00      0.99        68\n",
      "          40       0.97      0.92      0.94       106\n",
      "          41       1.00      0.97      0.99        35\n",
      "          42       0.97      0.97      0.97       298\n",
      "          43       0.98      0.98      0.98      3697\n",
      "          44       1.00      0.96      0.98       322\n",
      "          45       0.96      0.97      0.97       464\n",
      "          46       0.85      0.88      0.86        25\n",
      "          47       0.98      0.97      0.97       330\n",
      "          48       0.99      0.98      0.98       154\n",
      "          49       0.98      0.98      0.98       123\n",
      "          50       0.99      0.91      0.95       104\n",
      "          51       0.98      0.97      0.97       169\n",
      "          52       0.99      0.96      0.98      1184\n",
      "          53       0.99      0.98      0.99       176\n",
      "          54       0.98      0.98      0.98        64\n",
      "          55       0.97      0.99      0.98       140\n",
      "          56       0.96      0.98      0.97      1556\n",
      "          57       0.98      0.97      0.98      1850\n",
      "          58       0.99      0.94      0.97       392\n",
      "          59       0.99      0.96      0.98       532\n",
      "          60       0.98      0.96      0.97       197\n",
      "          61       0.97      0.99      0.98       156\n",
      "          62       0.98      0.95      0.97       233\n",
      "          63       0.99      0.98      0.99       111\n",
      "          64       1.00      1.00      1.00        28\n",
      "          65       1.00      0.96      0.98       661\n",
      "          66       0.97      0.96      0.97        80\n",
      "          67       0.97      0.95      0.96       370\n",
      "          68       0.99      0.98      0.98      2813\n",
      "          69       0.98      0.94      0.96        49\n",
      "          70       1.00      0.96      0.98        55\n",
      "\n",
      "   micro avg       0.98      0.97      0.98     28022\n",
      "   macro avg       0.98      0.97      0.98     28022\n",
      "weighted avg       0.98      0.97      0.98     28022\n",
      " samples avg       0.97      0.97      0.97     28022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in list_of_models_hidden_nodes:\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_bipolar_sigmoid_train_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_start\"]=time.time()\n",
    "    t1_models[\"t1_\"+str(i)+\"_bipolar_sigmoid\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_bipolar_sigmoid\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_start\"], \"train\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_end\"])\n",
    "\n",
    "    \n",
    "    print( \"t1_\"   +str(i)+\"_relu_leaky_train_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_train_start\"]=time.time()\n",
    "    t1_models[\"t1_\"+str(i)+\"_relu_leaky\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_relu_leaky)\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_relu_leaky\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(i)+\"_relu_leaky_train_start\"], \"train\", time_log[\"t1_\" +str(i)+\"_relu_leaky_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_biploar_step_train_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_train_start\"]=time.time()\n",
    "    t1_models[\"t1_\"+str(i)+\"_biploar_step\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_biploar_step)\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_biploar_step\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(i)+\"_biploar_step_train_start\"], \"train\", time_log[\"t1_\" +str(i)+\"_biploar_step_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_bipolar_sigmoid_train_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_start\"]=time.time()\n",
    "    t2_models[\"t2_\"+str(i)+\"_bipolar_sigmoid\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_bipolar_sigmoid\"].fit(type2_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type2_BERT_Embeddings_Train\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_start\"], \"train\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_relu_leaky_train_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_train_start\"]=time.time()\n",
    "    t2_models[\"t2_\"+str(i)+\"_relu_leaky\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_relu_leaky)\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_relu_leaky\"].fit(type2_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type2_BERT_Embeddings_Train\", time_log[\"t2_\" +str(i)+\"_relu_leaky_train_start\"], \"train\", time_log[\"t2_\" +str(i)+\"_relu_leaky_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_biploar_step_train_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_train_start\"]=time.time()\n",
    "    t2_models[\"t2_\"+str(i)+\"_biploar_step\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_biploar_step)\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_biploar_step\"].fit(type2_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type2_BERT_Embeddings_Train\", time_log[\"t2_\" +str(i)+\"_biploar_step_train_start\"], \"train\", time_log[\"t2_\" +str(i)+\"_biploar_step_train_end\"])\n",
    "\n"
   ]
  },
  {
   "source": [
    "TESTING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t1_100_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.032    |             |\n",
      "|     Precision      |    0.055    |    0.573    |\n",
      "|       Recall       |    0.009    |    0.077    |\n",
      "|     F1-measure     |    0.012    |    0.136    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.084 |\n",
      "|   Literature Precision   | 0.182 |\n",
      "|     LiteratureRecall     | 0.094 |\n",
      "|   LiteratureF1-measure   | 0.124 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       1.00      0.00      0.01       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.60      0.40      0.48      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.80      0.01      0.02       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       1.00      0.01      0.02       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.51      0.24      0.33       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.57      0.08      0.14      9022\n",
      "   macro avg       0.06      0.01      0.01      9022\n",
      "weighted avg       0.29      0.08      0.10      9022\n",
      " samples avg       0.18      0.09      0.11      9022\n",
      "\n",
      "t1_100_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.039    |             |\n",
      "|     Precision      |    0.037    |    0.584    |\n",
      "|       Recall       |    0.010    |    0.082    |\n",
      "|     F1-measure     |    0.013    |    0.143    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.093 |\n",
      "|   Literature Precision   | 0.196 |\n",
      "|     LiteratureRecall     | 0.104 |\n",
      "|   LiteratureF1-measure   | 0.136 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.60      0.42      0.49      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.65      0.03      0.05       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.83      0.03      0.06       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.54      0.24      0.34       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.58      0.08      0.14      9022\n",
      "   macro avg       0.04      0.01      0.01      9022\n",
      "weighted avg       0.21      0.08      0.10      9022\n",
      " samples avg       0.20      0.10      0.12      9022\n",
      "\n",
      "t1_100_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.025    |             |\n",
      "|     Precision      |    0.078    |    0.536    |\n",
      "|       Recall       |    0.008    |    0.066    |\n",
      "|     F1-measure     |    0.012    |    0.117    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.070 |\n",
      "|   Literature Precision   | 0.158 |\n",
      "|     LiteratureRecall     | 0.078 |\n",
      "|   LiteratureF1-measure   | 0.105 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       1.00      0.02      0.05        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.57      0.37      0.45      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       1.00      0.01      0.01       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.47      0.01      0.03       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       1.00      0.01      0.02       119\n",
      "          68       0.48      0.17      0.25       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.54      0.07      0.12      9022\n",
      "   macro avg       0.08      0.01      0.01      9022\n",
      "weighted avg       0.22      0.07      0.09      9022\n",
      " samples avg       0.16      0.08      0.09      9022\n",
      "\n",
      "t2_100_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.034    |             |\n",
      "|     Precision      |    0.133    |    0.554    |\n",
      "|       Recall       |    0.011    |    0.081    |\n",
      "|     F1-measure     |    0.016    |    0.141    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.088 |\n",
      "|   Literature Precision   | 0.187 |\n",
      "|     LiteratureRecall     | 0.099 |\n",
      "|   LiteratureF1-measure   | 0.130 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       1.00      0.00      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       1.00      0.02      0.05        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       1.00      0.01      0.01       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.58      0.41      0.48      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.92      0.03      0.05       395\n",
      "          53       1.00      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.43      0.01      0.02       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       1.00      0.04      0.07        27\n",
      "          67       1.00      0.01      0.02       119\n",
      "          68       0.54      0.25      0.34       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.55      0.08      0.14      9022\n",
      "   macro avg       0.13      0.01      0.02      9022\n",
      "weighted avg       0.31      0.08      0.10      9022\n",
      " samples avg       0.19      0.10      0.11      9022\n",
      "\n",
      "t2_100_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.040    |             |\n",
      "|     Precision      |    0.060    |    0.600    |\n",
      "|       Recall       |    0.011    |    0.084    |\n",
      "|     F1-measure     |    0.014    |    0.148    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.095 |\n",
      "|   Literature Precision   | 0.197 |\n",
      "|     LiteratureRecall     | 0.106 |\n",
      "|   LiteratureF1-measure   | 0.138 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       1.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.50      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.61      0.43      0.50      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.89      0.04      0.08       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.71      0.02      0.03       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.58      0.26      0.36       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.60      0.08      0.15      9022\n",
      "   macro avg       0.06      0.01      0.01      9022\n",
      "weighted avg       0.32      0.08      0.11      9022\n",
      " samples avg       0.20      0.11      0.12      9022\n",
      "\n",
      "t2_100_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.029    |             |\n",
      "|     Precision      |    0.135    |    0.538    |\n",
      "|       Recall       |    0.010    |    0.068    |\n",
      "|     F1-measure     |    0.014    |    0.120    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.073 |\n",
      "|   Literature Precision   | 0.163 |\n",
      "|     LiteratureRecall     | 0.081 |\n",
      "|   LiteratureF1-measure   | 0.108 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       1.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       1.00      0.02      0.05        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       1.00      0.01      0.01       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.59      0.35      0.44      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.71      0.01      0.02       395\n",
      "          53       1.00      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.75      0.02      0.04       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       1.00      0.04      0.07        27\n",
      "          67       1.00      0.01      0.02       119\n",
      "          68       0.52      0.20      0.29       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.54      0.07      0.12      9022\n",
      "   macro avg       0.13      0.01      0.01      9022\n",
      "weighted avg       0.32      0.07      0.09      9022\n",
      " samples avg       0.16      0.08      0.09      9022\n",
      "\n",
      "t1_200_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.038    |             |\n",
      "|     Precision      |    0.038    |    0.582    |\n",
      "|       Recall       |    0.011    |    0.086    |\n",
      "|     F1-measure     |    0.014    |    0.150    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.096 |\n",
      "|   Literature Precision   | 0.200 |\n",
      "|     LiteratureRecall     | 0.107 |\n",
      "|   LiteratureF1-measure   | 0.140 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.25      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.61      0.43      0.50      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.69      0.02      0.04       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.65      0.04      0.08       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.53      0.27      0.36       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.58      0.09      0.15      9022\n",
      "   macro avg       0.04      0.01      0.01      9022\n",
      "weighted avg       0.22      0.09      0.11      9022\n",
      " samples avg       0.20      0.11      0.12      9022\n",
      "\n",
      "t1_200_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.044    |             |\n",
      "|     Precision      |    0.044    |    0.591    |\n",
      "|       Recall       |    0.012    |    0.093    |\n",
      "|     F1-measure     |    0.016    |    0.161    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.105 |\n",
      "|   Literature Precision   | 0.219 |\n",
      "|     LiteratureRecall     | 0.117 |\n",
      "|   LiteratureF1-measure   | 0.153 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.50      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.60      0.44      0.50      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.81      0.03      0.06       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.61      0.07      0.13       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.58      0.30      0.40       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.59      0.09      0.16      9022\n",
      "   macro avg       0.04      0.01      0.02      9022\n",
      "weighted avg       0.22      0.09      0.12      9022\n",
      " samples avg       0.22      0.12      0.13      9022\n",
      "\n",
      "t1_200_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.029    |             |\n",
      "|     Precision      |    0.081    |    0.574    |\n",
      "|       Recall       |    0.011    |    0.084    |\n",
      "|     F1-measure     |    0.015    |    0.146    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.087 |\n",
      "|   Literature Precision   | 0.191 |\n",
      "|     LiteratureRecall     | 0.099 |\n",
      "|   LiteratureF1-measure   | 0.130 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.50      0.00      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       1.00      0.01      0.02       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.33      0.01      0.01       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.60      0.42      0.50      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       1.00      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.67      0.02      0.03       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.20      0.00      0.00       507\n",
      "          57       0.43      0.02      0.03       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.50      0.02      0.03        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.54      0.27      0.36       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.57      0.08      0.15      9022\n",
      "   macro avg       0.08      0.01      0.01      9022\n",
      "weighted avg       0.26      0.08      0.10      9022\n",
      " samples avg       0.19      0.10      0.11      9022\n",
      "\n",
      "t2_200_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.042    |             |\n",
      "|     Precision      |    0.036    |    0.591    |\n",
      "|       Recall       |    0.012    |    0.094    |\n",
      "|     F1-measure     |    0.016    |    0.163    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.105 |\n",
      "|   Literature Precision   | 0.224 |\n",
      "|     LiteratureRecall     | 0.117 |\n",
      "|   LiteratureF1-measure   | 0.153 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.61      0.45      0.52      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.83      0.05      0.09       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.58      0.08      0.14       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.55      0.29      0.38       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.59      0.09      0.16      9022\n",
      "   macro avg       0.04      0.01      0.02      9022\n",
      "weighted avg       0.21      0.09      0.12      9022\n",
      " samples avg       0.22      0.12      0.13      9022\n",
      "\n",
      "t2_200_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.051    |             |\n",
      "|     Precision      |    0.159    |    0.582    |\n",
      "|       Recall       |    0.015    |    0.104    |\n",
      "|     F1-measure     |    0.021    |    0.177    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.119 |\n",
      "|   Literature Precision   | 0.246 |\n",
      "|     LiteratureRecall     | 0.132 |\n",
      "|   LiteratureF1-measure   | 0.171 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       1.00      0.00      0.01       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.50      0.00      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       1.00      0.02      0.05        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.25      0.01      0.02       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.17      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       1.00      0.01      0.01       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.63      0.49      0.55      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.70      0.05      0.09       395\n",
      "          53       1.00      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.43      0.01      0.01       507\n",
      "          57       0.56      0.09      0.15       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.50      0.01      0.01       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       1.00      0.04      0.07        27\n",
      "          67       1.00      0.01      0.02       119\n",
      "          68       0.57      0.32      0.41       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.58      0.10      0.18      9022\n",
      "   macro avg       0.16      0.02      0.02      9022\n",
      "weighted avg       0.38      0.10      0.13      9022\n",
      " samples avg       0.25      0.13      0.15      9022\n",
      "\n",
      "t2_200_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.040    |             |\n",
      "|     Precision      |    0.099    |    0.575    |\n",
      "|       Recall       |    0.012    |    0.085    |\n",
      "|     F1-measure     |    0.016    |    0.148    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.097 |\n",
      "|   Literature Precision   | 0.207 |\n",
      "|     LiteratureRecall     | 0.106 |\n",
      "|   LiteratureF1-measure   | 0.140 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       1.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       1.00      0.02      0.05        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.59      0.43      0.50      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.79      0.03      0.05       395\n",
      "          53       1.00      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.33      0.00      0.01       507\n",
      "          57       0.72      0.05      0.09       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       1.00      0.04      0.07        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.56      0.24      0.34       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.58      0.09      0.15      9022\n",
      "   macro avg       0.10      0.01      0.02      9022\n",
      "weighted avg       0.31      0.09      0.11      9022\n",
      " samples avg       0.21      0.11      0.12      9022\n",
      "\n",
      "t1_300_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.041    |             |\n",
      "|     Precision      |    0.060    |    0.568    |\n",
      "|       Recall       |    0.012    |    0.092    |\n",
      "|     F1-measure     |    0.015    |    0.158    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.102 |\n",
      "|   Literature Precision   | 0.209 |\n",
      "|     LiteratureRecall     | 0.115 |\n",
      "|   LiteratureF1-measure   | 0.148 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.25      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.29      0.00      0.01       596\n",
      "          29       1.00      0.01      0.01       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.59      0.44      0.50      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.71      0.03      0.06       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.33      0.00      0.01       507\n",
      "          57       0.58      0.05      0.09       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.54      0.30      0.39       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.57      0.09      0.16      9022\n",
      "   macro avg       0.06      0.01      0.01      9022\n",
      "weighted avg       0.27      0.09      0.11      9022\n",
      " samples avg       0.21      0.11      0.13      9022\n",
      "\n",
      "t1_300_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.051    |             |\n",
      "|     Precision      |    0.159    |    0.582    |\n",
      "|       Recall       |    0.015    |    0.098    |\n",
      "|     F1-measure     |    0.021    |    0.168    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.113 |\n",
      "|   Literature Precision   | 0.235 |\n",
      "|     LiteratureRecall     | 0.125 |\n",
      "|   LiteratureF1-measure   | 0.163 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       1.00      0.01      0.02       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.50      0.01      0.02       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.43      0.01      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.67      0.02      0.03       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.38      0.01      0.01       596\n",
      "          29       0.33      0.01      0.01       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.60      0.45      0.52      1155\n",
      "          44       1.00      0.01      0.02       117\n",
      "          45       1.00      0.01      0.01       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       1.00      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.83      0.07      0.13       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.40      0.00      0.01       507\n",
      "          57       0.62      0.08      0.14       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       1.00      0.04      0.07        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.55      0.30      0.39       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.58      0.10      0.17      9022\n",
      "   macro avg       0.16      0.01      0.02      9022\n",
      "weighted avg       0.36      0.10      0.12      9022\n",
      " samples avg       0.23      0.12      0.14      9022\n",
      "\n",
      "t1_300_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.039    |             |\n",
      "|     Precision      |    0.072    |    0.552    |\n",
      "|       Recall       |    0.011    |    0.086    |\n",
      "|     F1-measure     |    0.015    |    0.149    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.097 |\n",
      "|   Literature Precision   | 0.203 |\n",
      "|     LiteratureRecall     | 0.108 |\n",
      "|   LiteratureF1-measure   | 0.141 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.57      0.01      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.57      0.42      0.48      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.70      0.04      0.08       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.20      0.00      0.00       507\n",
      "          57       0.54      0.05      0.08       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       1.00      0.01      0.02       119\n",
      "          68       0.52      0.27      0.36       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.55      0.09      0.15      9022\n",
      "   macro avg       0.07      0.01      0.01      9022\n",
      "weighted avg       0.26      0.09      0.11      9022\n",
      " samples avg       0.20      0.11      0.12      9022\n",
      "\n",
      "t2_300_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.047    |             |\n",
      "|     Precision      |    0.072    |    0.579    |\n",
      "|       Recall       |    0.013    |    0.098    |\n",
      "|     F1-measure     |    0.017    |    0.168    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.113 |\n",
      "|   Literature Precision   | 0.235 |\n",
      "|     LiteratureRecall     | 0.125 |\n",
      "|   LiteratureF1-measure   | 0.163 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.60      0.01      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       1.00      0.01      0.01       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.60      0.47      0.53      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.77      0.06      0.11       395\n",
      "          53       1.00      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.59      0.06      0.11       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.54      0.30      0.39       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.58      0.10      0.17      9022\n",
      "   macro avg       0.07      0.01      0.02      9022\n",
      "weighted avg       0.27      0.10      0.12      9022\n",
      " samples avg       0.24      0.12      0.14      9022\n",
      "\n",
      "t2_300_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.041    |             |\n",
      "| Term Wise Accuracy |    0.959    |             |\n",
      "|      Accuracy      |    0.053    |             |\n",
      "|     Precision      |    0.066    |    0.592    |\n",
      "|       Recall       |    0.014    |    0.103    |\n",
      "|     F1-measure     |    0.019    |    0.175    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.041 |\n",
      "|   Literature Accuracy    | 0.120 |\n",
      "|   Literature Precision   | 0.243 |\n",
      "|     LiteratureRecall     | 0.132 |\n",
      "|   LiteratureF1-measure   | 0.171 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.38      0.01      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.67      0.02      0.03       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.14      0.00      0.00       596\n",
      "          29       0.67      0.01      0.02       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.61      0.46      0.53      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.75      0.08      0.14       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.25      0.00      0.01       507\n",
      "          57       0.67      0.10      0.17       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.56      0.33      0.41       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.59      0.10      0.18      9022\n",
      "   macro avg       0.07      0.01      0.02      9022\n",
      "weighted avg       0.28      0.10      0.13      9022\n",
      " samples avg       0.24      0.13      0.15      9022\n",
      "\n",
      "t2_300_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.045    |             |\n",
      "|     Precision      |    0.046    |    0.573    |\n",
      "|       Recall       |    0.012    |    0.092    |\n",
      "|     F1-measure     |    0.015    |    0.159    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.107 |\n",
      "|   Literature Precision   | 0.219 |\n",
      "|     LiteratureRecall     | 0.118 |\n",
      "|   LiteratureF1-measure   | 0.154 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.50      0.00      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.59      0.46      0.52      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.80      0.04      0.08       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.20      0.00      0.00       507\n",
      "          57       0.67      0.08      0.14       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.52      0.27      0.35       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.57      0.09      0.16      9022\n",
      "   macro avg       0.05      0.01      0.02      9022\n",
      "weighted avg       0.25      0.09      0.11      9022\n",
      " samples avg       0.22      0.12      0.14      9022\n",
      "\n",
      "t1_400_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.047    |             |\n",
      "|     Precision      |    0.063    |    0.577    |\n",
      "|       Recall       |    0.013    |    0.097    |\n",
      "|     F1-measure     |    0.017    |    0.167    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.111 |\n",
      "|   Literature Precision   | 0.226 |\n",
      "|     LiteratureRecall     | 0.124 |\n",
      "|   LiteratureF1-measure   | 0.160 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.25      0.00      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       1.00      0.01      0.02       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.41      0.01      0.02       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.60      0.45      0.51      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.68      0.04      0.08       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.36      0.01      0.02       507\n",
      "          57       0.59      0.08      0.14       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.56      0.32      0.40       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.58      0.10      0.17      9022\n",
      "   macro avg       0.06      0.01      0.02      9022\n",
      "weighted avg       0.28      0.10      0.12      9022\n",
      " samples avg       0.23      0.12      0.14      9022\n",
      "\n",
      "t1_400_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.052    |             |\n",
      "|     Precision      |    0.074    |    0.586    |\n",
      "|       Recall       |    0.015    |    0.105    |\n",
      "|     F1-measure     |    0.019    |    0.179    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.121 |\n",
      "|   Literature Precision   | 0.248 |\n",
      "|     LiteratureRecall     | 0.134 |\n",
      "|   LiteratureF1-measure   | 0.174 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.31      0.01      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.29      0.02      0.03       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.25      0.01      0.02       596\n",
      "          29       0.67      0.01      0.02       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.61      0.46      0.53      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.72      0.07      0.12       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.33      0.00      0.00       507\n",
      "          57       0.52      0.09      0.15       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.58      0.35      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.59      0.11      0.18      9022\n",
      "   macro avg       0.07      0.01      0.02      9022\n",
      "weighted avg       0.28      0.11      0.13      9022\n",
      " samples avg       0.25      0.13      0.15      9022\n",
      "\n",
      "t1_400_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.041    |             |\n",
      "|     Precision      |    0.043    |    0.538    |\n",
      "|       Recall       |    0.011    |    0.086    |\n",
      "|     F1-measure     |    0.015    |    0.148    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.097 |\n",
      "|   Literature Precision   | 0.203 |\n",
      "|     LiteratureRecall     | 0.108 |\n",
      "|   LiteratureF1-measure   | 0.141 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.17      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.33      0.01      0.02       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.29      0.01      0.01       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.57      0.42      0.48      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.65      0.04      0.07       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.54      0.06      0.11       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.50      0.26      0.34       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.54      0.09      0.15      9022\n",
      "   macro avg       0.04      0.01      0.01      9022\n",
      "weighted avg       0.22      0.09      0.11      9022\n",
      " samples avg       0.20      0.11      0.12      9022\n",
      "\n",
      "t2_400_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.047    |             |\n",
      "|     Precision      |    0.069    |    0.571    |\n",
      "|       Recall       |    0.015    |    0.103    |\n",
      "|     F1-measure     |    0.019    |    0.175    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.116 |\n",
      "|   Literature Precision   | 0.244 |\n",
      "|     LiteratureRecall     | 0.129 |\n",
      "|   LiteratureF1-measure   | 0.169 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.50      0.01      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.67      0.02      0.03       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.25      0.01      0.02       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.60      0.47      0.53      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.76      0.07      0.12       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.21      0.01      0.01       507\n",
      "          57       0.61      0.09      0.15       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.25      0.04      0.06        27\n",
      "          67       0.50      0.01      0.02       119\n",
      "          68       0.57      0.32      0.41       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.57      0.10      0.17      9022\n",
      "   macro avg       0.07      0.01      0.02      9022\n",
      "weighted avg       0.28      0.10      0.13      9022\n",
      " samples avg       0.24      0.13      0.15      9022\n",
      "\n",
      "t2_400_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.054    |             |\n",
      "|     Precision      |    0.075    |    0.576    |\n",
      "|       Recall       |    0.016    |    0.113    |\n",
      "|     F1-measure     |    0.020    |    0.189    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.129 |\n",
      "|   Literature Precision   | 0.264 |\n",
      "|     LiteratureRecall     | 0.144 |\n",
      "|   LiteratureF1-measure   | 0.186 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.50      0.01      0.02       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.67      0.02      0.03       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.19      0.01      0.02       596\n",
      "          29       0.50      0.01      0.01       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.60      0.50      0.54      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.68      0.08      0.14       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.38      0.01      0.02       507\n",
      "          57       0.57      0.11      0.19       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.67      0.02      0.03       119\n",
      "          68       0.57      0.36      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.58      0.11      0.19      9022\n",
      "   macro avg       0.07      0.02      0.02      9022\n",
      "weighted avg       0.29      0.11      0.14      9022\n",
      " samples avg       0.26      0.14      0.16      9022\n",
      "\n",
      "t2_400_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.052    |             |\n",
      "|     Precision      |    0.115    |    0.567    |\n",
      "|       Recall       |    0.014    |    0.101    |\n",
      "|     F1-measure     |    0.019    |    0.172    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.117 |\n",
      "|   Literature Precision   | 0.238 |\n",
      "|     LiteratureRecall     | 0.131 |\n",
      "|   LiteratureF1-measure   | 0.169 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.33      0.01      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.40      0.01      0.01       596\n",
      "          29       0.67      0.01      0.02       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       1.00      0.01      0.01       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.61      0.47      0.53      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.74      0.06      0.12       395\n",
      "          53       1.00      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.54      0.06      0.12       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.33      0.00      0.01       228\n",
      "          66       1.00      0.04      0.07        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.54      0.32      0.40       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.57      0.10      0.17      9022\n",
      "   macro avg       0.12      0.01      0.02      9022\n",
      "weighted avg       0.30      0.10      0.12      9022\n",
      " samples avg       0.24      0.13      0.15      9022\n",
      "\n",
      "t1_500_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.049    |             |\n",
      "|     Precision      |    0.102    |    0.560    |\n",
      "|       Recall       |    0.015    |    0.102    |\n",
      "|     F1-measure     |    0.019    |    0.173    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.117 |\n",
      "|   Literature Precision   | 0.237 |\n",
      "|     LiteratureRecall     | 0.130 |\n",
      "|   LiteratureF1-measure   | 0.168 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.33      0.01      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       1.00      0.01      0.02       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.25      0.01      0.02       596\n",
      "          29       1.00      0.01      0.01       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.59      0.46      0.51      1155\n",
      "          44       1.00      0.01      0.02       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       1.00      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.69      0.08      0.14       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.28      0.01      0.02       507\n",
      "          57       0.55      0.10      0.17       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.54      0.32      0.40       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.56      0.10      0.17      9022\n",
      "   macro avg       0.10      0.01      0.02      9022\n",
      "weighted avg       0.30      0.10      0.13      9022\n",
      " samples avg       0.24      0.13      0.15      9022\n",
      "\n",
      "t1_500_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.055    |             |\n",
      "|     Precision      |    0.092    |    0.558    |\n",
      "|       Recall       |    0.016    |    0.109    |\n",
      "|     F1-measure     |    0.021    |    0.183    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.125 |\n",
      "|   Literature Precision   | 0.249 |\n",
      "|     LiteratureRecall     | 0.140 |\n",
      "|   LiteratureF1-measure   | 0.179 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.50      0.01      0.02       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.39      0.02      0.03       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.83      0.04      0.08       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.33      0.02      0.04       596\n",
      "          29       0.67      0.01      0.02       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.58      0.47      0.52      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.69      0.07      0.13       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.40      0.02      0.03       507\n",
      "          57       0.57      0.11      0.18       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.54      0.34      0.41       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.56      0.11      0.18      9022\n",
      "   macro avg       0.09      0.02      0.02      9022\n",
      "weighted avg       0.30      0.11      0.13      9022\n",
      " samples avg       0.25      0.14      0.16      9022\n",
      "\n",
      "t1_500_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.044    |             |\n",
      "|     Precision      |    0.117    |    0.542    |\n",
      "|       Recall       |    0.014    |    0.098    |\n",
      "|     F1-measure     |    0.019    |    0.166    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.108 |\n",
      "|   Literature Precision   | 0.225 |\n",
      "|     LiteratureRecall     | 0.122 |\n",
      "|   LiteratureF1-measure   | 0.158 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.40      0.01      0.03       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       1.00      0.02      0.05        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.67      0.02      0.03       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.21      0.01      0.01       596\n",
      "          29       0.50      0.01      0.01       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       1.00      0.01      0.01       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.57      0.44      0.50      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.67      0.06      0.10       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.22      0.01      0.02       507\n",
      "          57       0.53      0.07      0.12       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       1.00      0.04      0.07        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.55      0.32      0.40       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.54      0.10      0.17      9022\n",
      "   macro avg       0.12      0.01      0.02      9022\n",
      "weighted avg       0.29      0.10      0.12      9022\n",
      " samples avg       0.22      0.12      0.14      9022\n",
      "\n",
      "t2_500_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.041    |             |\n",
      "| Term Wise Accuracy |    0.959    |             |\n",
      "|      Accuracy      |    0.057    |             |\n",
      "|     Precision      |    0.079    |    0.587    |\n",
      "|       Recall       |    0.016    |    0.112    |\n",
      "|     F1-measure     |    0.021    |    0.188    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.041 |\n",
      "|   Literature Accuracy    | 0.130 |\n",
      "|   Literature Precision   | 0.260 |\n",
      "|     LiteratureRecall     | 0.144 |\n",
      "|   LiteratureF1-measure   | 0.185 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.44      0.01      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       1.00      0.02      0.03       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.56      0.02      0.05       596\n",
      "          29       0.43      0.02      0.03       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.60      0.48      0.53      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.72      0.07      0.13       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.67      0.02      0.04       507\n",
      "          57       0.61      0.11      0.19       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.57      0.36      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.59      0.11      0.19      9022\n",
      "   macro avg       0.08      0.02      0.02      9022\n",
      "weighted avg       0.33      0.11      0.14      9022\n",
      " samples avg       0.26      0.14      0.16      9022\n",
      "\n",
      "t2_500_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.058    |             |\n",
      "|     Precision      |    0.126    |    0.575    |\n",
      "|       Recall       |    0.017    |    0.116    |\n",
      "|     F1-measure     |    0.023    |    0.194    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.134 |\n",
      "|   Literature Precision   | 0.271 |\n",
      "|     LiteratureRecall     | 0.148 |\n",
      "|   LiteratureF1-measure   | 0.192 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.17      0.00      0.01       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.48      0.03      0.05       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.50      0.02      0.05        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.33      0.01      0.02       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.50      0.02      0.04       596\n",
      "          29       1.00      0.02      0.03       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       1.00      0.01      0.01       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.60      0.51      0.55      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.81      0.09      0.16       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.42      0.02      0.04       507\n",
      "          57       0.60      0.13      0.21       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       1.00      0.01      0.01       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       1.00      0.04      0.07        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.56      0.34      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.57      0.12      0.19      9022\n",
      "   macro avg       0.13      0.02      0.02      9022\n",
      "weighted avg       0.37      0.12      0.14      9022\n",
      " samples avg       0.27      0.15      0.17      9022\n",
      "\n",
      "t2_500_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.047    |             |\n",
      "|     Precision      |    0.058    |    0.577    |\n",
      "|       Recall       |    0.014    |    0.104    |\n",
      "|     F1-measure     |    0.018    |    0.176    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.117 |\n",
      "|   Literature Precision   | 0.245 |\n",
      "|     LiteratureRecall     | 0.131 |\n",
      "|   LiteratureF1-measure   | 0.171 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.24      0.01      0.01       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.67      0.02      0.03       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.41      0.01      0.02       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.60      0.47      0.53      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.73      0.03      0.05       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.27      0.01      0.02       507\n",
      "          57       0.61      0.11      0.19       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.56      0.33      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.58      0.10      0.18      9022\n",
      "   macro avg       0.06      0.01      0.02      9022\n",
      "weighted avg       0.27      0.10      0.13      9022\n",
      " samples avg       0.24      0.13      0.15      9022\n",
      "\n",
      "t1_1000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.043    |             |\n",
      "| Term Wise Accuracy |    0.957    |             |\n",
      "|      Accuracy      |    0.058    |             |\n",
      "|     Precision      |    0.101    |    0.514    |\n",
      "|       Recall       |    0.019    |    0.119    |\n",
      "|     F1-measure     |    0.025    |    0.193    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.043 |\n",
      "|   Literature Accuracy    | 0.134 |\n",
      "|   Literature Precision   | 0.269 |\n",
      "|     LiteratureRecall     | 0.151 |\n",
      "|   LiteratureF1-measure   | 0.194 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.50      0.01      0.02       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.67      0.03      0.05        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.40      0.01      0.02       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.36      0.05      0.09       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.44      0.04      0.07       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.30      0.05      0.08       596\n",
      "          29       0.33      0.02      0.03       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.25      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.50      0.01      0.01       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.58      0.50      0.54      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.63      0.09      0.15       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.19      0.02      0.03       507\n",
      "          57       0.49      0.15      0.23       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       1.00      0.02      0.03        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.51      0.33      0.40       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.51      0.12      0.19      9022\n",
      "   macro avg       0.10      0.02      0.03      9022\n",
      "weighted avg       0.29      0.12      0.15      9022\n",
      " samples avg       0.27      0.15      0.17      9022\n",
      "\n",
      "t1_1000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.061    |             |\n",
      "|     Precision      |    0.110    |    0.531    |\n",
      "|       Recall       |    0.021    |    0.128    |\n",
      "|     F1-measure     |    0.028    |    0.207    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.142 |\n",
      "|   Literature Precision   | 0.283 |\n",
      "|     LiteratureRecall     | 0.159 |\n",
      "|   LiteratureF1-measure   | 0.204 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.50      0.02      0.03       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.15      0.01      0.01       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.32      0.05      0.09       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.50      0.06      0.11       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.32      0.05      0.08       596\n",
      "          29       0.46      0.03      0.06       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.59      0.50      0.54      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       1.00      0.01      0.01       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       1.00      0.06      0.11        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.62      0.11      0.18       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.33      0.04      0.06       507\n",
      "          57       0.50      0.14      0.22       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       1.00      0.02      0.03       119\n",
      "          68       0.54      0.39      0.45       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.53      0.13      0.21      9022\n",
      "   macro avg       0.11      0.02      0.03      9022\n",
      "weighted avg       0.31      0.13      0.16      9022\n",
      " samples avg       0.28      0.16      0.18      9022\n",
      "\n",
      "t1_1000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.043    |             |\n",
      "| Term Wise Accuracy |    0.957    |             |\n",
      "|      Accuracy      |    0.052    |             |\n",
      "|     Precision      |    0.068    |    0.504    |\n",
      "|       Recall       |    0.016    |    0.114    |\n",
      "|     F1-measure     |    0.022    |    0.186    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.043 |\n",
      "|   Literature Accuracy    | 0.125 |\n",
      "|   Literature Precision   | 0.255 |\n",
      "|     LiteratureRecall     | 0.142 |\n",
      "|   LiteratureF1-measure   | 0.182 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.11      0.00      0.01       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.41      0.05      0.08       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.43      0.03      0.05       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.28      0.05      0.08       596\n",
      "          29       0.50      0.01      0.01       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.58      0.48      0.53      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.48      0.07      0.12       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.29      0.03      0.05       507\n",
      "          57       0.39      0.10      0.16       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.50      0.01      0.01       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.40      0.02      0.03       119\n",
      "          68       0.49      0.34      0.40       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.50      0.11      0.19      9022\n",
      "   macro avg       0.07      0.02      0.02      9022\n",
      "weighted avg       0.27      0.11      0.14      9022\n",
      " samples avg       0.25      0.14      0.16      9022\n",
      "\n",
      "t2_1000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.059    |             |\n",
      "|     Precision      |    0.105    |    0.531    |\n",
      "|       Recall       |    0.020    |    0.129    |\n",
      "|     F1-measure     |    0.028    |    0.208    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.146 |\n",
      "|   Literature Precision   | 0.293 |\n",
      "|     LiteratureRecall     | 0.167 |\n",
      "|   LiteratureF1-measure   | 0.213 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.50      0.01      0.02       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.33      0.01      0.02       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.25      0.03      0.05       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.78      0.06      0.11       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.31      0.04      0.07       596\n",
      "          29       0.50      0.04      0.07       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.58      0.51      0.54      1155\n",
      "          44       0.33      0.01      0.02       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.69      0.11      0.19       395\n",
      "          53       0.50      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.36      0.04      0.07       507\n",
      "          57       0.51      0.17      0.26       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.25      0.00      0.01       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       1.00      0.03      0.05       119\n",
      "          68       0.53      0.38      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.53      0.13      0.21      9022\n",
      "   macro avg       0.10      0.02      0.03      9022\n",
      "weighted avg       0.32      0.13      0.16      9022\n",
      " samples avg       0.29      0.17      0.19      9022\n",
      "\n",
      "t2_1000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.056    |             |\n",
      "|     Precision      |    0.115    |    0.525    |\n",
      "|       Recall       |    0.021    |    0.129    |\n",
      "|     F1-measure     |    0.029    |    0.207    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.141 |\n",
      "|   Literature Precision   | 0.287 |\n",
      "|     LiteratureRecall     | 0.160 |\n",
      "|   LiteratureF1-measure   | 0.206 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.27      0.02      0.03       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.33      0.04      0.08       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.59      0.09      0.15       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.29      0.05      0.08       596\n",
      "          29       0.56      0.03      0.05       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.59      0.51      0.55      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.54      0.09      0.16       395\n",
      "          53       1.00      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.34      0.04      0.08       507\n",
      "          57       0.54      0.16      0.25       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       1.00      0.03      0.06        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.60      0.03      0.05       119\n",
      "          68       0.53      0.37      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.53      0.13      0.21      9022\n",
      "   macro avg       0.12      0.02      0.03      9022\n",
      "weighted avg       0.31      0.13      0.16      9022\n",
      " samples avg       0.29      0.16      0.18      9022\n",
      "\n",
      "t2_1000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.061    |             |\n",
      "|     Precision      |    0.105    |    0.536    |\n",
      "|       Recall       |    0.019    |    0.123    |\n",
      "|     F1-measure     |    0.025    |    0.200    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.140 |\n",
      "|   Literature Precision   | 0.283 |\n",
      "|     LiteratureRecall     | 0.157 |\n",
      "|   LiteratureF1-measure   | 0.202 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.50      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.46      0.02      0.03       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.37      0.04      0.07       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.75      0.03      0.05       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.36      0.04      0.08       596\n",
      "          29       0.50      0.01      0.02       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.58      0.49      0.54      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.65      0.11      0.18       395\n",
      "          53       1.00      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.29      0.03      0.05       507\n",
      "          57       0.51      0.14      0.22       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.33      0.01      0.01       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.60      0.03      0.05       119\n",
      "          68       0.54      0.37      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.54      0.12      0.20      9022\n",
      "   macro avg       0.10      0.02      0.03      9022\n",
      "weighted avg       0.32      0.12      0.15      9022\n",
      " samples avg       0.28      0.16      0.18      9022\n",
      "\n",
      "t1_2000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.045    |             |\n",
      "| Term Wise Accuracy |    0.955    |             |\n",
      "|      Accuracy      |    0.049    |             |\n",
      "|     Precision      |    0.111    |    0.425    |\n",
      "|       Recall       |    0.028    |    0.146    |\n",
      "|     F1-measure     |    0.037    |    0.217    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.045 |\n",
      "|   Literature Accuracy    | 0.142 |\n",
      "|   Literature Precision   | 0.280 |\n",
      "|     LiteratureRecall     | 0.173 |\n",
      "|   LiteratureF1-measure   | 0.214 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.42      0.04      0.07       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.28      0.04      0.08       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.25      0.10      0.14       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.60      0.13      0.22       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.26      0.10      0.15       596\n",
      "          29       0.27      0.04      0.06       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       1.00      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.22      0.01      0.03       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.53      0.50      0.51      1155\n",
      "          44       0.25      0.01      0.02       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.45      0.14      0.21       395\n",
      "          53       0.33      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.32      0.10      0.15       507\n",
      "          57       0.32      0.16      0.21       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.23      0.02      0.03       173\n",
      "          60       0.38      0.05      0.08        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.18      0.01      0.02       228\n",
      "          66       0.50      0.04      0.07        27\n",
      "          67       0.57      0.03      0.06       119\n",
      "          68       0.50      0.40      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.42      0.15      0.22      9022\n",
      "   macro avg       0.11      0.03      0.04      9022\n",
      "weighted avg       0.27      0.15      0.17      9022\n",
      " samples avg       0.28      0.17      0.19      9022\n",
      "\n",
      "t1_2000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.045    |             |\n",
      "| Term Wise Accuracy |    0.955    |             |\n",
      "|      Accuracy      |    0.055    |             |\n",
      "|     Precision      |    0.139    |    0.426    |\n",
      "|       Recall       |    0.031    |    0.157    |\n",
      "|     F1-measure     |    0.043    |    0.230    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.045 |\n",
      "|   Literature Accuracy    | 0.155 |\n",
      "|   Literature Precision   | 0.294 |\n",
      "|     LiteratureRecall     | 0.192 |\n",
      "|   LiteratureF1-measure   | 0.232 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.22      0.02      0.03       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.20      0.01      0.02       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.10      0.02      0.04       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.25      0.10      0.14       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.33      0.01      0.02        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.17      0.01      0.01       159\n",
      "          26       0.61      0.12      0.21       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.30      0.14      0.19       596\n",
      "          29       0.30      0.05      0.08       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       1.00      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.67      0.08      0.14        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.55      0.52      0.54      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.80      0.03      0.05       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.33      0.01      0.02       129\n",
      "          48       0.17      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.40      0.13      0.20       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.28      0.11      0.15       507\n",
      "          57       0.37      0.20      0.26       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.27      0.02      0.04       173\n",
      "          60       0.43      0.05      0.08        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.12      0.02      0.03        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.17      0.00      0.01       228\n",
      "          66       1.00      0.07      0.14        27\n",
      "          67       0.38      0.03      0.05       119\n",
      "          68       0.50      0.43      0.46       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.43      0.16      0.23      9022\n",
      "   macro avg       0.14      0.03      0.04      9022\n",
      "weighted avg       0.29      0.16      0.18      9022\n",
      " samples avg       0.29      0.19      0.20      9022\n",
      "\n",
      "t1_2000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.045    |             |\n",
      "| Term Wise Accuracy |    0.955    |             |\n",
      "|      Accuracy      |    0.051    |             |\n",
      "|     Precision      |    0.167    |    0.425    |\n",
      "|       Recall       |    0.025    |    0.134    |\n",
      "|     F1-measure     |    0.035    |    0.204    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.045 |\n",
      "|   Literature Accuracy    | 0.137 |\n",
      "|   Literature Precision   | 0.266 |\n",
      "|     LiteratureRecall     | 0.167 |\n",
      "|   LiteratureF1-measure   | 0.205 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.15      0.02      0.04       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.25      0.09      0.13       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.50      0.01      0.02       159\n",
      "          26       0.57      0.07      0.13       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.24      0.09      0.13       596\n",
      "          29       0.44      0.04      0.08       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       1.00      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       1.00      0.04      0.08        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.40      0.01      0.03       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.54      0.48      0.51      1155\n",
      "          44       0.50      0.01      0.02       117\n",
      "          45       0.17      0.01      0.01       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.50      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.35      0.09      0.15       395\n",
      "          53       0.33      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.25      0.08      0.12       507\n",
      "          57       0.37      0.17      0.23       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.17      0.01      0.01       173\n",
      "          60       1.00      0.03      0.06        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.29      0.02      0.04       228\n",
      "          66       1.00      0.04      0.07        27\n",
      "          67       0.38      0.03      0.05       119\n",
      "          68       0.48      0.36      0.41       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.43      0.13      0.20      9022\n",
      "   macro avg       0.17      0.03      0.04      9022\n",
      "weighted avg       0.29      0.13      0.16      9022\n",
      " samples avg       0.27      0.17      0.18      9022\n",
      "\n",
      "t2_2000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.045    |             |\n",
      "| Term Wise Accuracy |    0.955    |             |\n",
      "|      Accuracy      |    0.051    |             |\n",
      "|     Precision      |    0.114    |    0.434    |\n",
      "|       Recall       |    0.028    |    0.152    |\n",
      "|     F1-measure     |    0.038    |    0.225    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.045 |\n",
      "|   Literature Accuracy    | 0.153 |\n",
      "|   Literature Precision   | 0.301 |\n",
      "|     LiteratureRecall     | 0.189 |\n",
      "|   LiteratureF1-measure   | 0.232 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.43      0.02      0.04       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       1.00      0.03      0.05        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.21      0.05      0.09       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.24      0.09      0.13       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.53      0.09      0.15       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.28      0.11      0.16       596\n",
      "          29       0.36      0.05      0.09       190\n",
      "          30       0.25      0.01      0.02        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.33      0.04      0.07        26\n",
      "          33       0.33      0.02      0.03        64\n",
      "          34       0.33      0.04      0.07        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.17      0.01      0.01       172\n",
      "          38       0.20      0.01      0.01       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.54      0.52      0.53      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.17      0.01      0.01       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.45      0.14      0.21       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.25      0.08      0.12       507\n",
      "          57       0.39      0.20      0.27       587\n",
      "          58       0.17      0.01      0.01       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.50      0.03      0.06        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.11      0.00      0.01       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.36      0.03      0.06       119\n",
      "          68       0.51      0.42      0.46       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.43      0.15      0.23      9022\n",
      "   macro avg       0.11      0.03      0.04      9022\n",
      "weighted avg       0.28      0.15      0.18      9022\n",
      " samples avg       0.30      0.19      0.20      9022\n",
      "\n",
      "t2_2000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.044    |             |\n",
      "| Term Wise Accuracy |    0.956    |             |\n",
      "|      Accuracy      |    0.061    |             |\n",
      "|     Precision      |    0.154    |    0.460    |\n",
      "|       Recall       |    0.032    |    0.163    |\n",
      "|     F1-measure     |    0.043    |    0.240    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.044 |\n",
      "|   Literature Accuracy    | 0.166 |\n",
      "|   Literature Precision   | 0.320 |\n",
      "|     LiteratureRecall     | 0.204 |\n",
      "|   LiteratureF1-measure   | 0.249 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.27      0.02      0.04       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       1.00      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.28      0.06      0.10       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.26      0.11      0.15       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       1.00      0.02      0.05        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.58      0.10      0.17       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.31      0.13      0.19       596\n",
      "          29       0.50      0.06      0.11       190\n",
      "          30       0.38      0.04      0.07        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.50      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.50      0.08      0.14        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.11      0.01      0.01       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.56      0.54      0.55      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.33      0.01      0.01       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.50      0.02      0.03       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.49      0.15      0.23       395\n",
      "          53       1.00      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.29      0.10      0.15       507\n",
      "          57       0.43      0.21      0.28       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.29      0.02      0.04       173\n",
      "          60       0.17      0.02      0.03        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.07      0.00      0.01       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.56      0.04      0.08       119\n",
      "          68       0.54      0.45      0.49       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.46      0.16      0.24      9022\n",
      "   macro avg       0.15      0.03      0.04      9022\n",
      "weighted avg       0.32      0.16      0.19      9022\n",
      " samples avg       0.32      0.20      0.21      9022\n",
      "\n",
      "t2_2000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.044    |             |\n",
      "| Term Wise Accuracy |    0.956    |             |\n",
      "|      Accuracy      |    0.048    |             |\n",
      "|     Precision      |    0.122    |    0.444    |\n",
      "|       Recall       |    0.027    |    0.147    |\n",
      "|     F1-measure     |    0.036    |    0.221    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.044 |\n",
      "|   Literature Accuracy    | 0.148 |\n",
      "|   Literature Precision   | 0.290 |\n",
      "|     LiteratureRecall     | 0.181 |\n",
      "|   LiteratureF1-measure   | 0.223 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.50      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.25      0.01      0.02       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.16      0.03      0.05       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.29      0.09      0.14       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.71      0.11      0.18       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.29      0.12      0.17       596\n",
      "          29       0.33      0.03      0.06       190\n",
      "          30       0.33      0.01      0.02        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.50      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       1.00      0.04      0.08        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.55      0.52      0.53      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.41      0.13      0.20       395\n",
      "          53       1.00      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.33      0.10      0.15       507\n",
      "          57       0.36      0.18      0.24       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.14      0.01      0.01       173\n",
      "          60       0.33      0.03      0.06        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.20      0.01      0.02       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.50      0.04      0.08       119\n",
      "          68       0.49      0.40      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.44      0.15      0.22      9022\n",
      "   macro avg       0.12      0.03      0.04      9022\n",
      "weighted avg       0.28      0.15      0.17      9022\n",
      " samples avg       0.29      0.18      0.19      9022\n",
      "\n",
      "t1_3000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.049    |             |\n",
      "| Term Wise Accuracy |    0.951    |             |\n",
      "|      Accuracy      |    0.043    |             |\n",
      "|     Precision      |    0.123    |    0.355    |\n",
      "|       Recall       |    0.038    |    0.175    |\n",
      "|     F1-measure     |    0.049    |    0.234    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.049 |\n",
      "|   Literature Accuracy    | 0.154 |\n",
      "|   Literature Precision   | 0.289 |\n",
      "|     LiteratureRecall     | 0.209 |\n",
      "|   LiteratureF1-measure   | 0.243 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.02      0.03        56\n",
      "           1       0.18      0.04      0.06       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.25      0.03      0.05        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.25      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.50      0.02      0.04        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.25      0.02      0.03       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.18      0.09      0.12       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.27      0.17      0.21       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.25      0.02      0.04        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.09      0.01      0.01       159\n",
      "          26       0.52      0.10      0.16       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.24      0.19      0.21       596\n",
      "          29       0.26      0.06      0.10       190\n",
      "          30       0.08      0.01      0.02        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.50      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.50      0.08      0.14        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.09      0.01      0.02       172\n",
      "          38       0.14      0.01      0.03       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.51      0.52      0.51      1155\n",
      "          44       0.12      0.01      0.02       117\n",
      "          45       0.30      0.04      0.07       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.33      0.02      0.03       129\n",
      "          48       0.25      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.31      0.17      0.22       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.22      0.15      0.18       507\n",
      "          57       0.30      0.22      0.25       587\n",
      "          58       0.17      0.01      0.01       148\n",
      "          59       0.12      0.02      0.04       173\n",
      "          60       0.33      0.05      0.08        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.04      0.01      0.01       228\n",
      "          66       0.20      0.04      0.06        27\n",
      "          67       0.38      0.05      0.09       119\n",
      "          68       0.47      0.44      0.46       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.36      0.17      0.23      9022\n",
      "   macro avg       0.12      0.04      0.05      9022\n",
      "weighted avg       0.26      0.17      0.19      9022\n",
      " samples avg       0.29      0.21      0.21      9022\n",
      "\n",
      "t1_3000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.048    |             |\n",
      "| Term Wise Accuracy |    0.952    |             |\n",
      "|      Accuracy      |    0.050    |             |\n",
      "|     Precision      |    0.136    |    0.366    |\n",
      "|       Recall       |    0.040    |    0.177    |\n",
      "|     F1-measure     |    0.052    |    0.239    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.048 |\n",
      "|   Literature Accuracy    | 0.159 |\n",
      "|   Literature Precision   | 0.298 |\n",
      "|     LiteratureRecall     | 0.211 |\n",
      "|   LiteratureF1-measure   | 0.247 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.12      0.02      0.04       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.25      0.03      0.05        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.22      0.03      0.05        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.06      0.01      0.02       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.12      0.06      0.08       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.23      0.17      0.19       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       1.00      0.02      0.05        42\n",
      "          24       0.25      0.01      0.02        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.50      0.13      0.21       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.30      0.22      0.25       596\n",
      "          29       0.26      0.08      0.12       190\n",
      "          30       0.42      0.06      0.11        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.20      0.04      0.06        26\n",
      "          33       0.50      0.02      0.03        64\n",
      "          34       0.67      0.08      0.14        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.05      0.01      0.01       172\n",
      "          38       0.20      0.02      0.04       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.53      0.52      0.52      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.06      0.01      0.01       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.29      0.02      0.03       129\n",
      "          48       0.20      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.33      0.17      0.22       395\n",
      "          53       0.20      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.31      0.18      0.22       507\n",
      "          57       0.29      0.21      0.24       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.21      0.04      0.07       173\n",
      "          60       0.62      0.12      0.20        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.16      0.03      0.05       228\n",
      "          66       0.33      0.04      0.07        27\n",
      "          67       0.30      0.05      0.09       119\n",
      "          68       0.46      0.42      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.37      0.18      0.24      9022\n",
      "   macro avg       0.14      0.04      0.05      9022\n",
      "weighted avg       0.27      0.18      0.20      9022\n",
      " samples avg       0.30      0.21      0.21      9022\n",
      "\n",
      "t1_3000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.048    |             |\n",
      "| Term Wise Accuracy |    0.952    |             |\n",
      "|      Accuracy      |    0.049    |             |\n",
      "|     Precision      |    0.132    |    0.362    |\n",
      "|       Recall       |    0.035    |    0.166    |\n",
      "|     F1-measure     |    0.045    |    0.228    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.048 |\n",
      "|   Literature Accuracy    | 0.154 |\n",
      "|   Literature Precision   | 0.287 |\n",
      "|     LiteratureRecall     | 0.201 |\n",
      "|   LiteratureF1-measure   | 0.237 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.33      0.06      0.10       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.17      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.50      0.02      0.04        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.14      0.01      0.02       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.15      0.07      0.10       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.25      0.15      0.19       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.14      0.01      0.02        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.09      0.01      0.01       159\n",
      "          26       0.56      0.09      0.15       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.26      0.18      0.21       596\n",
      "          29       0.19      0.04      0.06       190\n",
      "          30       0.17      0.01      0.02        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       1.00      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       1.00      0.08      0.15        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.11      0.01      0.02       172\n",
      "          38       0.14      0.01      0.01       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.50      0.50      0.50      1155\n",
      "          44       0.17      0.01      0.02       117\n",
      "          45       0.20      0.01      0.03       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.25      0.02      0.03       129\n",
      "          48       0.50      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.27      0.15      0.19       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.22      0.12      0.16       507\n",
      "          57       0.39      0.28      0.33       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.10      0.02      0.03       173\n",
      "          60       0.57      0.06      0.11        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.17      0.03      0.05       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.38      0.04      0.08       119\n",
      "          68       0.44      0.40      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.36      0.17      0.23      9022\n",
      "   macro avg       0.13      0.03      0.04      9022\n",
      "weighted avg       0.26      0.17      0.19      9022\n",
      " samples avg       0.29      0.20      0.20      9022\n",
      "\n",
      "t2_3000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.049    |             |\n",
      "| Term Wise Accuracy |    0.951    |             |\n",
      "|      Accuracy      |    0.046    |             |\n",
      "|     Precision      |    0.110    |    0.357    |\n",
      "|       Recall       |    0.036    |    0.167    |\n",
      "|     F1-measure     |    0.046    |    0.227    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.049 |\n",
      "|   Literature Accuracy    | 0.153 |\n",
      "|   Literature Precision   | 0.294 |\n",
      "|     LiteratureRecall     | 0.205 |\n",
      "|   LiteratureF1-measure   | 0.242 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.02      0.03        56\n",
      "           1       0.15      0.03      0.05       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.14      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.50      0.02      0.04        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.22      0.02      0.03       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.21      0.10      0.14       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.21      0.13      0.16       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.17      0.01      0.02        85\n",
      "          23       0.50      0.02      0.05        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.05      0.01      0.01       159\n",
      "          26       0.37      0.10      0.15       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.23      0.15      0.18       596\n",
      "          29       0.33      0.09      0.15       190\n",
      "          30       0.18      0.04      0.06        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.17      0.04      0.06        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.50      0.08      0.14        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.04      0.01      0.01       172\n",
      "          38       0.22      0.01      0.03       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.52      0.52      0.52      1155\n",
      "          44       0.12      0.01      0.02       117\n",
      "          45       0.06      0.01      0.01       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.33      0.02      0.03       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.35      0.17      0.23       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.20      0.12      0.15       507\n",
      "          57       0.33      0.24      0.28       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.21      0.04      0.07       173\n",
      "          60       0.33      0.06      0.10        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.12      0.03      0.04       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.33      0.04      0.07       119\n",
      "          68       0.45      0.40      0.43       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.36      0.17      0.23      9022\n",
      "   macro avg       0.11      0.04      0.05      9022\n",
      "weighted avg       0.25      0.17      0.19      9022\n",
      " samples avg       0.29      0.21      0.20      9022\n",
      "\n",
      "t2_3000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.049    |             |\n",
      "| Term Wise Accuracy |    0.951    |             |\n",
      "|      Accuracy      |    0.049    |             |\n",
      "|     Precision      |    0.129    |    0.362    |\n",
      "|       Recall       |    0.039    |    0.177    |\n",
      "|     F1-measure     |    0.051    |    0.238    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.049 |\n",
      "|   Literature Accuracy    | 0.163 |\n",
      "|   Literature Precision   | 0.307 |\n",
      "|     LiteratureRecall     | 0.217 |\n",
      "|   LiteratureF1-measure   | 0.254 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.02      0.03        56\n",
      "           1       0.24      0.07      0.11       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       1.00      0.03      0.06        30\n",
      "           7       0.22      0.03      0.05        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.06      0.01      0.02       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.21      0.11      0.14       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.26      0.17      0.21       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.14      0.01      0.02       159\n",
      "          26       0.35      0.12      0.17       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.21      0.14      0.17       596\n",
      "          29       0.31      0.09      0.15       190\n",
      "          30       0.20      0.04      0.06        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       1.00      0.04      0.07        26\n",
      "          33       0.25      0.02      0.03        64\n",
      "          34       0.40      0.08      0.13        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.14      0.01      0.02        92\n",
      "          37       0.06      0.01      0.02       172\n",
      "          38       0.17      0.01      0.03       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.52      0.52      0.52      1155\n",
      "          44       0.17      0.01      0.02       117\n",
      "          45       0.20      0.02      0.04       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.08      0.01      0.01       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.32      0.16      0.22       395\n",
      "          53       0.20      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.24      0.15      0.18       507\n",
      "          57       0.37      0.27      0.31       587\n",
      "          58       0.08      0.01      0.01       148\n",
      "          59       0.17      0.04      0.07       173\n",
      "          60       0.44      0.06      0.11        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.15      0.03      0.05       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.32      0.05      0.09       119\n",
      "          68       0.47      0.44      0.45       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.36      0.18      0.24      9022\n",
      "   macro avg       0.13      0.04      0.05      9022\n",
      "weighted avg       0.26      0.18      0.20      9022\n",
      " samples avg       0.31      0.22      0.22      9022\n",
      "\n",
      "t2_3000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.048    |             |\n",
      "| Term Wise Accuracy |    0.952    |             |\n",
      "|      Accuracy      |    0.044    |             |\n",
      "|     Precision      |    0.121    |    0.363    |\n",
      "|       Recall       |    0.033    |    0.163    |\n",
      "|     F1-measure     |    0.043    |    0.225    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.048 |\n",
      "|   Literature Accuracy    | 0.151 |\n",
      "|   Literature Precision   | 0.287 |\n",
      "|     LiteratureRecall     | 0.200 |\n",
      "|   LiteratureF1-measure   | 0.236 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.12      0.02      0.03       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.33      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.19      0.09      0.12       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.26      0.17      0.20       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.09      0.01      0.01       159\n",
      "          26       0.46      0.10      0.16       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.21      0.14      0.17       596\n",
      "          29       0.27      0.06      0.10       190\n",
      "          30       0.25      0.02      0.04        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       1.00      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.67      0.08      0.14        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.08      0.01      0.02       172\n",
      "          38       0.12      0.01      0.01       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.50      0.48      0.49      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.18      0.01      0.03       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.08      0.01      0.01       129\n",
      "          48       1.00      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.35      0.17      0.23       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.25      0.13      0.17       507\n",
      "          57       0.36      0.24      0.29       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.20      0.02      0.04       173\n",
      "          60       0.67      0.03      0.06        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.17      0.04      0.06       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.36      0.04      0.08       119\n",
      "          68       0.44      0.40      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.36      0.16      0.22      9022\n",
      "   macro avg       0.12      0.03      0.04      9022\n",
      "weighted avg       0.25      0.16      0.18      9022\n",
      " samples avg       0.29      0.20      0.20      9022\n",
      "\n",
      "t1_4000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.053    |             |\n",
      "| Term Wise Accuracy |    0.947    |             |\n",
      "|      Accuracy      |    0.034    |             |\n",
      "|     Precision      |    0.126    |    0.307    |\n",
      "|       Recall       |    0.047    |    0.190    |\n",
      "|     F1-measure     |    0.058    |    0.235    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.053 |\n",
      "|   Literature Accuracy    | 0.150 |\n",
      "|   Literature Precision   | 0.275 |\n",
      "|     LiteratureRecall     | 0.223 |\n",
      "|   LiteratureF1-measure   | 0.246 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.11      0.04      0.06       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.20      0.02      0.04        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.17      0.03      0.06       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.21      0.18      0.19       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.25      0.22      0.24       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.31      0.05      0.08        85\n",
      "          23       0.33      0.02      0.04        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.08      0.03      0.04       159\n",
      "          26       0.42      0.12      0.19       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.25      0.23      0.24       596\n",
      "          29       0.23      0.11      0.14       190\n",
      "          30       0.20      0.04      0.06        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       1.00      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.29      0.08      0.12        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.09      0.02      0.04        92\n",
      "          37       0.05      0.02      0.03       172\n",
      "          38       0.17      0.04      0.06       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.47      0.48      0.48      1155\n",
      "          44       0.10      0.02      0.03       117\n",
      "          45       0.14      0.04      0.06       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.12      0.02      0.04       129\n",
      "          48       0.50      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.30      0.20      0.24       395\n",
      "          53       0.25      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.19      0.16      0.18       507\n",
      "          57       0.29      0.26      0.27       587\n",
      "          58       0.09      0.01      0.02       148\n",
      "          59       0.21      0.09      0.12       173\n",
      "          60       0.43      0.09      0.15        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.07      0.02      0.03        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.15      0.07      0.09       228\n",
      "          66       0.67      0.07      0.13        27\n",
      "          67       0.16      0.04      0.07       119\n",
      "          68       0.43      0.42      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.31      0.19      0.23      9022\n",
      "   macro avg       0.13      0.05      0.06      9022\n",
      "weighted avg       0.24      0.19      0.20      9022\n",
      " samples avg       0.28      0.22      0.21      9022\n",
      "\n",
      "t1_4000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.053    |             |\n",
      "| Term Wise Accuracy |    0.947    |             |\n",
      "|      Accuracy      |    0.040    |             |\n",
      "|     Precision      |    0.106    |    0.314    |\n",
      "|       Recall       |    0.050    |    0.201    |\n",
      "|     F1-measure     |    0.061    |    0.245    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.053 |\n",
      "|   Literature Accuracy    | 0.161 |\n",
      "|   Literature Precision   | 0.287 |\n",
      "|     LiteratureRecall     | 0.235 |\n",
      "|   LiteratureF1-measure   | 0.259 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.20      0.06      0.09       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.50      0.03      0.06        30\n",
      "           7       0.06      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.11      0.02      0.04        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.09      0.02      0.03       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.20      0.19      0.19       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.23      0.19      0.21       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.18      0.02      0.04        85\n",
      "          23       0.33      0.02      0.04        42\n",
      "          24       0.17      0.02      0.04        83\n",
      "          25       0.09      0.03      0.04       159\n",
      "          26       0.38      0.14      0.21       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.27      0.27      0.27       596\n",
      "          29       0.24      0.11      0.15       190\n",
      "          30       0.20      0.06      0.09        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.20      0.04      0.06        26\n",
      "          33       0.17      0.03      0.05        64\n",
      "          34       0.20      0.08      0.11        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.09      0.05      0.06       172\n",
      "          38       0.10      0.02      0.04       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.05      0.01      0.02        93\n",
      "          43       0.50      0.51      0.50      1155\n",
      "          44       0.06      0.01      0.01       117\n",
      "          45       0.20      0.06      0.10       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.12      0.02      0.04       129\n",
      "          48       0.33      0.08      0.13        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.29      0.23      0.26       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.22      0.20      0.21       507\n",
      "          57       0.33      0.29      0.30       587\n",
      "          58       0.09      0.02      0.03       148\n",
      "          59       0.10      0.04      0.06       173\n",
      "          60       0.53      0.15      0.24        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.10      0.04      0.06       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.18      0.05      0.08       119\n",
      "          68       0.43      0.44      0.43       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.31      0.20      0.25      9022\n",
      "   macro avg       0.11      0.05      0.06      9022\n",
      "weighted avg       0.24      0.20      0.21      9022\n",
      " samples avg       0.29      0.24      0.22      9022\n",
      "\n",
      "t1_4000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.053    |             |\n",
      "| Term Wise Accuracy |    0.947    |             |\n",
      "|      Accuracy      |    0.035    |             |\n",
      "|     Precision      |    0.106    |    0.308    |\n",
      "|       Recall       |    0.043    |    0.185    |\n",
      "|     F1-measure     |    0.052    |    0.231    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.053 |\n",
      "|   Literature Accuracy    | 0.147 |\n",
      "|   Literature Precision   | 0.273 |\n",
      "|     LiteratureRecall     | 0.214 |\n",
      "|   LiteratureF1-measure   | 0.240 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.15      0.05      0.07       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.20      0.03      0.05        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.33      0.03      0.05        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.17      0.02      0.04        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.11      0.02      0.03       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.16      0.12      0.14       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.23      0.21      0.22       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.33      0.02      0.04        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.20      0.01      0.02        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.36      0.11      0.16       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.23      0.22      0.22       596\n",
      "          29       0.11      0.05      0.07       190\n",
      "          30       0.12      0.02      0.04        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.50      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.40      0.08      0.13        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.08      0.01      0.02        92\n",
      "          37       0.07      0.02      0.03       172\n",
      "          38       0.17      0.03      0.05       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.50      0.50      0.50      1155\n",
      "          44       0.07      0.01      0.02       117\n",
      "          45       0.17      0.03      0.05       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.11      0.02      0.03       129\n",
      "          48       0.17      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.22      0.14      0.17       395\n",
      "          53       0.20      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.20      0.03      0.05        37\n",
      "          56       0.19      0.17      0.18       507\n",
      "          57       0.28      0.25      0.26       587\n",
      "          58       0.06      0.01      0.02       148\n",
      "          59       0.21      0.10      0.13       173\n",
      "          60       0.35      0.09      0.14        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.19      0.08      0.11       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.26      0.05      0.08       119\n",
      "          68       0.42      0.45      0.43       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.31      0.19      0.23      9022\n",
      "   macro avg       0.11      0.04      0.05      9022\n",
      "weighted avg       0.23      0.19      0.19      9022\n",
      " samples avg       0.27      0.21      0.20      9022\n",
      "\n",
      "t2_4000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.054    |             |\n",
      "| Term Wise Accuracy |    0.946    |             |\n",
      "|      Accuracy      |    0.035    |             |\n",
      "|     Precision      |    0.107    |    0.302    |\n",
      "|       Recall       |    0.047    |    0.194    |\n",
      "|     F1-measure     |    0.056    |    0.236    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.054 |\n",
      "|   Literature Accuracy    | 0.156 |\n",
      "|   Literature Precision   | 0.284 |\n",
      "|     LiteratureRecall     | 0.231 |\n",
      "|   LiteratureF1-measure   | 0.255 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.22      0.09      0.12       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.67      0.04      0.08        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.17      0.14      0.15       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.08      0.01      0.02        97\n",
      "          20       0.23      0.20      0.21       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.12      0.02      0.04        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.04      0.01      0.02       159\n",
      "          26       0.35      0.16      0.22       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.23      0.23      0.23       596\n",
      "          29       0.26      0.12      0.16       190\n",
      "          30       0.21      0.06      0.09        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       1.00      0.04      0.07        26\n",
      "          33       0.15      0.03      0.05        64\n",
      "          34       0.40      0.08      0.13        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.05      0.01      0.02        92\n",
      "          37       0.13      0.05      0.07       172\n",
      "          38       0.11      0.03      0.05       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.49      0.50      0.49      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.20      0.06      0.09       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.14      0.02      0.04       129\n",
      "          48       0.09      0.03      0.04        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.25      0.20      0.22       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.24      0.22      0.23       507\n",
      "          57       0.28      0.27      0.28       587\n",
      "          58       0.05      0.01      0.01       148\n",
      "          59       0.11      0.05      0.06       173\n",
      "          60       0.27      0.06      0.10        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.12      0.05      0.07       228\n",
      "          66       0.33      0.04      0.07        27\n",
      "          67       0.19      0.04      0.07       119\n",
      "          68       0.42      0.44      0.43       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.30      0.19      0.24      9022\n",
      "   macro avg       0.11      0.05      0.06      9022\n",
      "weighted avg       0.23      0.19      0.20      9022\n",
      " samples avg       0.28      0.23      0.21      9022\n",
      "\n",
      "t2_4000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.054    |             |\n",
      "| Term Wise Accuracy |    0.946    |             |\n",
      "|      Accuracy      |    0.036    |             |\n",
      "|     Precision      |    0.087    |    0.307    |\n",
      "|       Recall       |    0.050    |    0.203    |\n",
      "|     F1-measure     |    0.059    |    0.244    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.054 |\n",
      "|   Literature Accuracy    | 0.160 |\n",
      "|   Literature Precision   | 0.289 |\n",
      "|     LiteratureRecall     | 0.241 |\n",
      "|   LiteratureF1-measure   | 0.263 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.02      0.03        56\n",
      "           1       0.21      0.11      0.14       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.05      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.10      0.02      0.04        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.07      0.02      0.03       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.18      0.17      0.17       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.04      0.01      0.02        97\n",
      "          20       0.24      0.22      0.23       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.05      0.01      0.02        85\n",
      "          23       0.25      0.02      0.04        42\n",
      "          24       0.06      0.01      0.02        83\n",
      "          25       0.07      0.03      0.04       159\n",
      "          26       0.36      0.19      0.25       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.24      0.24      0.24       596\n",
      "          29       0.25      0.14      0.18       190\n",
      "          30       0.23      0.07      0.11        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.25      0.04      0.07        26\n",
      "          33       0.13      0.03      0.05        64\n",
      "          34       0.29      0.08      0.12        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.09      0.03      0.05       172\n",
      "          38       0.12      0.04      0.06       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.49      0.52      0.51      1155\n",
      "          44       0.04      0.01      0.01       117\n",
      "          45       0.10      0.03      0.05       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.08      0.02      0.03       129\n",
      "          48       0.10      0.03      0.04        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.32      0.25      0.28       395\n",
      "          53       0.07      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.23      0.19      0.21       507\n",
      "          57       0.30      0.26      0.28       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.21      0.10      0.14       173\n",
      "          60       0.22      0.06      0.10        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.12      0.05      0.07       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.09      0.03      0.05       119\n",
      "          68       0.44      0.45      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.31      0.20      0.24      9022\n",
      "   macro avg       0.09      0.05      0.06      9022\n",
      "weighted avg       0.23      0.20      0.21      9022\n",
      " samples avg       0.29      0.24      0.22      9022\n",
      "\n",
      "t2_4000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.053    |             |\n",
      "| Term Wise Accuracy |    0.947    |             |\n",
      "|      Accuracy      |    0.032    |             |\n",
      "|     Precision      |    0.098    |    0.314    |\n",
      "|       Recall       |    0.045    |    0.193    |\n",
      "|     F1-measure     |    0.054    |    0.239    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.053 |\n",
      "|   Literature Accuracy    | 0.155 |\n",
      "|   Literature Precision   | 0.284 |\n",
      "|     LiteratureRecall     | 0.230 |\n",
      "|   LiteratureF1-measure   | 0.254 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.14      0.04      0.06       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.20      0.02      0.04        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.12      0.02      0.03       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.21      0.17      0.19       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.23      0.21      0.22       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.13      0.02      0.04        85\n",
      "          23       0.50      0.02      0.05        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.07      0.02      0.03       159\n",
      "          26       0.36      0.13      0.19       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.21      0.20      0.20       596\n",
      "          29       0.25      0.10      0.14       190\n",
      "          30       0.27      0.05      0.08        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.50      0.08      0.13        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.40      0.08      0.13        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.07      0.01      0.02        92\n",
      "          37       0.07      0.02      0.03       172\n",
      "          38       0.07      0.01      0.02       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.48      0.51      0.50      1155\n",
      "          44       0.24      0.03      0.06       117\n",
      "          45       0.20      0.05      0.08       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.05      0.01      0.01       129\n",
      "          48       0.20      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.29      0.19      0.23       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.23      0.20      0.21       507\n",
      "          57       0.31      0.28      0.29       587\n",
      "          58       0.04      0.01      0.01       148\n",
      "          59       0.15      0.06      0.09       173\n",
      "          60       0.20      0.05      0.07        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.16      0.07      0.09       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.21      0.06      0.09       119\n",
      "          68       0.42      0.44      0.43       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.31      0.19      0.24      9022\n",
      "   macro avg       0.10      0.04      0.05      9022\n",
      "weighted avg       0.23      0.19      0.20      9022\n",
      " samples avg       0.28      0.23      0.21      9022\n",
      "\n",
      "t1_5000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.060    |             |\n",
      "| Term Wise Accuracy |    0.940    |             |\n",
      "|      Accuracy      |    0.020    |             |\n",
      "|     Precision      |    0.091    |    0.256    |\n",
      "|       Recall       |    0.057    |    0.212    |\n",
      "|     F1-measure     |    0.063    |    0.232    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.060 |\n",
      "|   Literature Accuracy    | 0.143 |\n",
      "|   Literature Precision   | 0.253 |\n",
      "|     LiteratureRecall     | 0.245 |\n",
      "|   LiteratureF1-measure   | 0.249 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.17      0.13      0.15       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.09      0.03      0.04        35\n",
      "           6       0.25      0.07      0.11        30\n",
      "           7       0.06      0.03      0.04        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.07      0.02      0.03        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.06      0.03      0.04       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.16      0.20      0.18       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.02      0.01      0.01        97\n",
      "          20       0.21      0.26      0.24       551\n",
      "          21       0.14      0.02      0.04        41\n",
      "          22       0.12      0.05      0.07        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.04      0.01      0.02        83\n",
      "          25       0.06      0.04      0.05       159\n",
      "          26       0.30      0.14      0.19       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.25      0.29      0.27       596\n",
      "          29       0.16      0.12      0.13       190\n",
      "          30       0.12      0.05      0.07        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.17      0.04      0.06        26\n",
      "          33       0.16      0.05      0.07        64\n",
      "          34       0.17      0.08      0.11        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.05      0.02      0.03        92\n",
      "          37       0.08      0.06      0.07       172\n",
      "          38       0.13      0.07      0.09       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.33      0.03      0.05        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.48      0.49      0.48      1155\n",
      "          44       0.02      0.01      0.01       117\n",
      "          45       0.10      0.06      0.08       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.10      0.04      0.06       129\n",
      "          48       0.12      0.03      0.05        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.21      0.20      0.20       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.22      0.25      0.24       507\n",
      "          57       0.25      0.30      0.27       587\n",
      "          58       0.06      0.03      0.04       148\n",
      "          59       0.07      0.05      0.06       173\n",
      "          60       0.24      0.09      0.13        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.05      0.02      0.02        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.12      0.09      0.11       228\n",
      "          66       0.50      0.04      0.07        27\n",
      "          67       0.17      0.08      0.11       119\n",
      "          68       0.41      0.43      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.26      0.21      0.23      9022\n",
      "   macro avg       0.09      0.06      0.06      9022\n",
      "weighted avg       0.22      0.21      0.21      9022\n",
      " samples avg       0.25      0.25      0.20      9022\n",
      "\n",
      "t1_5000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.061    |             |\n",
      "| Term Wise Accuracy |    0.939    |             |\n",
      "|      Accuracy      |    0.029    |             |\n",
      "|     Precision      |    0.088    |    0.258    |\n",
      "|       Recall       |    0.061    |    0.223    |\n",
      "|     F1-measure     |    0.067    |    0.239    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.061 |\n",
      "|   Literature Accuracy    | 0.153 |\n",
      "|   Literature Precision   | 0.264 |\n",
      "|     LiteratureRecall     | 0.256 |\n",
      "|   LiteratureF1-measure   | 0.260 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.02      0.03        56\n",
      "           1       0.14      0.10      0.12       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.12      0.06      0.08        35\n",
      "           6       0.09      0.03      0.05        30\n",
      "           7       0.02      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.05      0.02      0.03        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.06      0.03      0.03        40\n",
      "          12       0.06      0.03      0.05       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.17      0.20      0.18       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.02      0.01      0.01        97\n",
      "          20       0.24      0.28      0.26       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.17      0.07      0.10        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.07      0.04      0.05        83\n",
      "          25       0.10      0.07      0.08       159\n",
      "          26       0.29      0.17      0.21       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.23      0.27      0.24       596\n",
      "          29       0.11      0.10      0.11       190\n",
      "          30       0.06      0.04      0.05        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.17      0.04      0.06        26\n",
      "          33       0.06      0.02      0.02        64\n",
      "          34       0.15      0.08      0.11        25\n",
      "          35       0.25      0.03      0.06        29\n",
      "          36       0.09      0.07      0.08        92\n",
      "          37       0.06      0.05      0.05       172\n",
      "          38       0.06      0.04      0.05       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.04      0.02      0.03        93\n",
      "          43       0.49      0.53      0.51      1155\n",
      "          44       0.05      0.03      0.03       117\n",
      "          45       0.21      0.14      0.17       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.08      0.03      0.05       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.23      0.23      0.23       395\n",
      "          53       0.14      0.05      0.07        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.22      0.26      0.24       507\n",
      "          57       0.29      0.33      0.31       587\n",
      "          58       0.05      0.03      0.03       148\n",
      "          59       0.17      0.13      0.15       173\n",
      "          60       0.17      0.08      0.10        66\n",
      "          61       0.06      0.02      0.03        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.08      0.07      0.07       228\n",
      "          66       0.50      0.04      0.07        27\n",
      "          67       0.15      0.08      0.11       119\n",
      "          68       0.41      0.44      0.43       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.26      0.22      0.24      9022\n",
      "   macro avg       0.09      0.06      0.07      9022\n",
      "weighted avg       0.22      0.22      0.22      9022\n",
      " samples avg       0.26      0.26      0.22      9022\n",
      "\n",
      "t1_5000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.060    |             |\n",
      "| Term Wise Accuracy |    0.940    |             |\n",
      "|      Accuracy      |    0.024    |             |\n",
      "|     Precision      |    0.103    |    0.255    |\n",
      "|       Recall       |    0.057    |    0.211    |\n",
      "|     F1-measure     |    0.064    |    0.231    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.060 |\n",
      "|   Literature Accuracy    | 0.144 |\n",
      "|   Literature Precision   | 0.255 |\n",
      "|     LiteratureRecall     | 0.243 |\n",
      "|   LiteratureF1-measure   | 0.249 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.13      0.10      0.11       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.25      0.03      0.06        30\n",
      "           7       0.12      0.04      0.06        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.23      0.07      0.10        45\n",
      "          10       1.00      0.09      0.17        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.08      0.03      0.04       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.16      0.20      0.18       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.05      0.02      0.03        97\n",
      "          20       0.22      0.25      0.24       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.14      0.04      0.06        85\n",
      "          23       0.11      0.02      0.04        42\n",
      "          24       0.12      0.04      0.06        83\n",
      "          25       0.03      0.02      0.02       159\n",
      "          26       0.29      0.13      0.18       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.22      0.27      0.25       596\n",
      "          29       0.21      0.15      0.17       190\n",
      "          30       0.06      0.02      0.04        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.33      0.04      0.07        26\n",
      "          33       0.11      0.02      0.03        64\n",
      "          34       0.25      0.08      0.12        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.08      0.04      0.06        92\n",
      "          37       0.07      0.05      0.05       172\n",
      "          38       0.10      0.06      0.07       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.03      0.01      0.02        93\n",
      "          43       0.46      0.50      0.48      1155\n",
      "          44       0.12      0.04      0.06       117\n",
      "          45       0.14      0.09      0.11       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.11      0.05      0.06       129\n",
      "          48       0.12      0.03      0.05        36\n",
      "          49       0.25      0.02      0.04        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.19      0.19      0.19       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.19      0.21      0.20       507\n",
      "          57       0.26      0.32      0.28       587\n",
      "          58       0.04      0.01      0.02       148\n",
      "          59       0.10      0.09      0.09       173\n",
      "          60       0.30      0.09      0.14        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.09      0.07      0.08       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.12      0.07      0.09       119\n",
      "          68       0.39      0.44      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.26      0.21      0.23      9022\n",
      "   macro avg       0.10      0.06      0.06      9022\n",
      "weighted avg       0.21      0.21      0.21      9022\n",
      " samples avg       0.26      0.24      0.21      9022\n",
      "\n",
      "t2_5000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.061    |             |\n",
      "| Term Wise Accuracy |    0.939    |             |\n",
      "|      Accuracy      |    0.023    |             |\n",
      "|     Precision      |    0.087    |    0.260    |\n",
      "|       Recall       |    0.061    |    0.223    |\n",
      "|     F1-measure     |    0.066    |    0.240    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.061 |\n",
      "|   Literature Accuracy    | 0.150 |\n",
      "|   Literature Precision   | 0.265 |\n",
      "|     LiteratureRecall     | 0.254 |\n",
      "|   LiteratureF1-measure   | 0.260 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.11      0.09      0.10       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.17      0.03      0.06        30\n",
      "           7       0.03      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.09      0.04      0.06        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.04      0.02      0.03       115\n",
      "          13       0.33      0.06      0.10        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.16      0.20      0.18       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.05      0.02      0.03        97\n",
      "          20       0.24      0.26      0.25       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.11      0.04      0.05        85\n",
      "          23       0.12      0.02      0.04        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.03      0.03      0.03       159\n",
      "          26       0.29      0.19      0.23       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.24      0.29      0.26       596\n",
      "          29       0.21      0.17      0.19       190\n",
      "          30       0.13      0.05      0.07        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.33      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.20      0.08      0.11        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.09      0.04      0.06        92\n",
      "          37       0.10      0.08      0.09       172\n",
      "          38       0.09      0.05      0.06       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.04      0.02      0.03        93\n",
      "          43       0.47      0.51      0.49      1155\n",
      "          44       0.06      0.03      0.04       117\n",
      "          45       0.09      0.08      0.08       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.12      0.05      0.08       129\n",
      "          48       0.06      0.03      0.04        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.06      0.02      0.03        51\n",
      "          52       0.25      0.25      0.25       395\n",
      "          53       0.06      0.02      0.02        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.22      0.28      0.25       507\n",
      "          57       0.27      0.33      0.30       587\n",
      "          58       0.08      0.05      0.06       148\n",
      "          59       0.09      0.07      0.08       173\n",
      "          60       0.33      0.14      0.19        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.11      0.10      0.10       228\n",
      "          66       0.20      0.04      0.06        27\n",
      "          67       0.11      0.07      0.08       119\n",
      "          68       0.40      0.45      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.26      0.22      0.24      9022\n",
      "   macro avg       0.09      0.06      0.07      9022\n",
      "weighted avg       0.22      0.22      0.22      9022\n",
      " samples avg       0.27      0.25      0.21      9022\n",
      "\n",
      "t2_5000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.061    |             |\n",
      "| Term Wise Accuracy |    0.939    |             |\n",
      "|      Accuracy      |    0.025    |             |\n",
      "|     Precision      |    0.090    |    0.258    |\n",
      "|       Recall       |    0.064    |    0.226    |\n",
      "|     F1-measure     |    0.069    |    0.241    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.061 |\n",
      "|   Literature Accuracy    | 0.156 |\n",
      "|   Literature Precision   | 0.273 |\n",
      "|     LiteratureRecall     | 0.262 |\n",
      "|   LiteratureF1-measure   | 0.267 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.02      0.03        56\n",
      "           1       0.14      0.12      0.13       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.27      0.09      0.13        35\n",
      "           6       0.12      0.03      0.05        30\n",
      "           7       0.02      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.17      0.04      0.07        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.06      0.03      0.05       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.17      0.21      0.19       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.04      0.02      0.03        97\n",
      "          20       0.22      0.25      0.23       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.03      0.01      0.02        85\n",
      "          23       0.17      0.02      0.04        42\n",
      "          24       0.07      0.02      0.04        83\n",
      "          25       0.10      0.08      0.09       159\n",
      "          26       0.22      0.12      0.16       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.25      0.29      0.27       596\n",
      "          29       0.26      0.19      0.22       190\n",
      "          30       0.15      0.10      0.12        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.25      0.04      0.07        26\n",
      "          33       0.05      0.02      0.02        64\n",
      "          34       0.17      0.08      0.11        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.06      0.02      0.03        92\n",
      "          37       0.09      0.08      0.08       172\n",
      "          38       0.07      0.05      0.06       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.09      0.03      0.04        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.06      0.03      0.04        93\n",
      "          43       0.48      0.54      0.51      1155\n",
      "          44       0.03      0.02      0.02       117\n",
      "          45       0.14      0.11      0.12       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.07      0.03      0.04       129\n",
      "          48       0.13      0.06      0.08        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.27      0.28      0.27       395\n",
      "          53       0.05      0.02      0.02        65\n",
      "          54       0.25      0.07      0.11        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.25      0.28      0.26       507\n",
      "          57       0.25      0.29      0.27       587\n",
      "          58       0.03      0.01      0.02       148\n",
      "          59       0.09      0.08      0.09       173\n",
      "          60       0.24      0.11      0.15        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.10      0.09      0.09       228\n",
      "          66       0.20      0.04      0.06        27\n",
      "          67       0.10      0.06      0.07       119\n",
      "          68       0.39      0.44      0.41       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.26      0.23      0.24      9022\n",
      "   macro avg       0.09      0.06      0.07      9022\n",
      "weighted avg       0.22      0.23      0.22      9022\n",
      " samples avg       0.27      0.26      0.22      9022\n",
      "\n",
      "t2_5000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.059    |             |\n",
      "| Term Wise Accuracy |    0.941    |             |\n",
      "|      Accuracy      |    0.021    |             |\n",
      "|     Precision      |    0.094    |    0.266    |\n",
      "|       Recall       |    0.059    |    0.217    |\n",
      "|     F1-measure     |    0.066    |    0.239    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.059 |\n",
      "|   Literature Accuracy    | 0.147 |\n",
      "|   Literature Precision   | 0.261 |\n",
      "|     LiteratureRecall     | 0.252 |\n",
      "|   LiteratureF1-measure   | 0.256 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.04      0.05        56\n",
      "           1       0.13      0.09      0.10       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.25      0.06      0.09        18\n",
      "           5       0.22      0.06      0.09        35\n",
      "           6       0.17      0.03      0.06        30\n",
      "           7       0.14      0.05      0.07        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.29      0.04      0.08        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.25      0.03      0.05        40\n",
      "          12       0.10      0.05      0.07       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.14      0.15      0.14       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.02      0.01      0.01        97\n",
      "          20       0.22      0.24      0.23       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.09      0.02      0.04        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.03      0.01      0.02        83\n",
      "          25       0.06      0.03      0.04       159\n",
      "          26       0.24      0.13      0.17       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.24      0.28      0.26       596\n",
      "          29       0.21      0.15      0.18       190\n",
      "          30       0.19      0.08      0.12        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.11      0.04      0.06        26\n",
      "          33       0.09      0.03      0.05        64\n",
      "          34       0.22      0.08      0.12        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.06      0.03      0.04        92\n",
      "          37       0.07      0.05      0.06       172\n",
      "          38       0.06      0.04      0.05       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.09      0.03      0.05        93\n",
      "          43       0.48      0.52      0.50      1155\n",
      "          44       0.05      0.02      0.03       117\n",
      "          45       0.13      0.10      0.11       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.11      0.05      0.06       129\n",
      "          48       0.09      0.03      0.04        36\n",
      "          49       0.17      0.02      0.04        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.21      0.21      0.21       395\n",
      "          53       0.12      0.03      0.05        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.21      0.25      0.23       507\n",
      "          57       0.27      0.31      0.29       587\n",
      "          58       0.08      0.03      0.05       148\n",
      "          59       0.12      0.09      0.10       173\n",
      "          60       0.17      0.06      0.09        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.13      0.11      0.12       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.10      0.05      0.07       119\n",
      "          68       0.43      0.46      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.27      0.22      0.24      9022\n",
      "   macro avg       0.09      0.06      0.07      9022\n",
      "weighted avg       0.22      0.22      0.21      9022\n",
      " samples avg       0.26      0.25      0.21      9022\n",
      "\n",
      "t1_10000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.191    |             |\n",
      "| Term Wise Accuracy |    0.809    |             |\n",
      "|      Accuracy      |    0.007    |             |\n",
      "|     Precision      |    0.048    |    0.087    |\n",
      "|       Recall       |    0.210    |    0.364    |\n",
      "|     F1-measure     |    0.072    |    0.140    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.191 |\n",
      "|   Literature Accuracy    | 0.085 |\n",
      "|   Literature Precision   | 0.108 |\n",
      "|     LiteratureRecall     | 0.380 |\n",
      "|   LiteratureF1-measure   | 0.169 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.12      0.03        56\n",
      "           1       0.06      0.35      0.10       129\n",
      "           2       0.00      0.04      0.01        28\n",
      "           3       0.01      0.14      0.02        22\n",
      "           4       0.01      0.11      0.02        18\n",
      "           5       0.02      0.17      0.03        35\n",
      "           6       0.01      0.07      0.02        30\n",
      "           7       0.03      0.25      0.05        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.03      0.33      0.06        45\n",
      "          10       0.01      0.18      0.02        11\n",
      "          11       0.01      0.10      0.02        40\n",
      "          12       0.04      0.28      0.08       115\n",
      "          13       0.00      0.06      0.01        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.01      0.07      0.01        15\n",
      "          16       0.01      0.08      0.01        13\n",
      "          17       0.14      0.42      0.21       368\n",
      "          18       0.02      0.11      0.03        27\n",
      "          19       0.03      0.23      0.05        97\n",
      "          20       0.19      0.38      0.25       551\n",
      "          21       0.02      0.20      0.04        41\n",
      "          22       0.04      0.26      0.07        85\n",
      "          23       0.02      0.19      0.03        42\n",
      "          24       0.02      0.16      0.04        83\n",
      "          25       0.05      0.28      0.09       159\n",
      "          26       0.07      0.42      0.12       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.22      0.44      0.29       596\n",
      "          29       0.08      0.38      0.14       190\n",
      "          30       0.05      0.35      0.08        83\n",
      "          31       0.01      0.08      0.01        12\n",
      "          32       0.01      0.15      0.02        26\n",
      "          33       0.03      0.20      0.05        64\n",
      "          34       0.03      0.36      0.06        25\n",
      "          35       0.00      0.03      0.01        29\n",
      "          36       0.03      0.27      0.06        92\n",
      "          37       0.06      0.30      0.10       172\n",
      "          38       0.05      0.31      0.09       136\n",
      "          39       0.02      0.12      0.03        32\n",
      "          40       0.02      0.15      0.04        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.03      0.25      0.06        93\n",
      "          43       0.43      0.54      0.48      1155\n",
      "          44       0.05      0.29      0.08       117\n",
      "          45       0.06      0.33      0.10       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.06      0.31      0.10       129\n",
      "          48       0.02      0.22      0.03        36\n",
      "          49       0.02      0.20      0.04        44\n",
      "          50       0.02      0.25      0.04        28\n",
      "          51       0.02      0.24      0.04        51\n",
      "          52       0.14      0.38      0.20       395\n",
      "          53       0.02      0.15      0.04        65\n",
      "          54       0.01      0.13      0.02        15\n",
      "          55       0.00      0.03      0.00        37\n",
      "          56       0.18      0.44      0.26       507\n",
      "          57       0.22      0.41      0.29       587\n",
      "          58       0.06      0.30      0.10       148\n",
      "          59       0.06      0.27      0.09       173\n",
      "          60       0.02      0.15      0.04        66\n",
      "          61       0.01      0.12      0.02        51\n",
      "          62       0.03      0.23      0.05        66\n",
      "          63       0.01      0.13      0.02        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.09      0.38      0.14       228\n",
      "          66       0.02      0.15      0.03        27\n",
      "          67       0.04      0.26      0.07       119\n",
      "          68       0.32      0.45      0.38       911\n",
      "          69       0.01      0.07      0.01        14\n",
      "          70       0.01      0.08      0.01        13\n",
      "\n",
      "   micro avg       0.09      0.36      0.14      9022\n",
      "   macro avg       0.05      0.21      0.07      9022\n",
      "weighted avg       0.17      0.36      0.22      9022\n",
      " samples avg       0.11      0.38      0.14      9022\n",
      "\n",
      "t1_10000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.194    |             |\n",
      "| Term Wise Accuracy |    0.806    |             |\n",
      "|      Accuracy      |    0.006    |             |\n",
      "|     Precision      |    0.050    |    0.088    |\n",
      "|       Recall       |    0.234    |    0.377    |\n",
      "|     F1-measure     |    0.076    |    0.142    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.194 |\n",
      "|   Literature Accuracy    | 0.087 |\n",
      "|   Literature Precision   | 0.111 |\n",
      "|     LiteratureRecall     | 0.398 |\n",
      "|   LiteratureF1-measure   | 0.174 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.27      0.05        56\n",
      "           1       0.05      0.29      0.09       129\n",
      "           2       0.01      0.07      0.01        28\n",
      "           3       0.01      0.09      0.01        22\n",
      "           4       0.01      0.06      0.01        18\n",
      "           5       0.02      0.17      0.03        35\n",
      "           6       0.03      0.20      0.05        30\n",
      "           7       0.03      0.28      0.06        79\n",
      "           8       0.01      0.12      0.03         8\n",
      "           9       0.03      0.24      0.05        45\n",
      "          10       0.01      0.18      0.02        11\n",
      "          11       0.01      0.15      0.02        40\n",
      "          12       0.04      0.23      0.06       115\n",
      "          13       0.01      0.22      0.03        18\n",
      "          14       0.01      0.11      0.03         9\n",
      "          15       0.01      0.07      0.01        15\n",
      "          16       0.01      0.08      0.01        13\n",
      "          17       0.12      0.35      0.18       368\n",
      "          18       0.00      0.04      0.01        27\n",
      "          19       0.03      0.25      0.06        97\n",
      "          20       0.21      0.43      0.28       551\n",
      "          21       0.01      0.12      0.02        41\n",
      "          22       0.04      0.28      0.06        85\n",
      "          23       0.02      0.26      0.04        42\n",
      "          24       0.03      0.25      0.06        83\n",
      "          25       0.06      0.35      0.11       159\n",
      "          26       0.07      0.41      0.12       113\n",
      "          27       0.01      0.09      0.01        11\n",
      "          28       0.21      0.46      0.29       596\n",
      "          29       0.08      0.35      0.12       190\n",
      "          30       0.03      0.27      0.06        83\n",
      "          31       0.01      0.08      0.01        12\n",
      "          32       0.01      0.08      0.01        26\n",
      "          33       0.03      0.19      0.05        64\n",
      "          34       0.03      0.32      0.06        25\n",
      "          35       0.02      0.28      0.04        29\n",
      "          36       0.03      0.26      0.06        92\n",
      "          37       0.05      0.27      0.09       172\n",
      "          38       0.05      0.28      0.08       136\n",
      "          39       0.02      0.16      0.04        32\n",
      "          40       0.02      0.17      0.04        40\n",
      "          41       0.01      0.20      0.03         5\n",
      "          42       0.04      0.31      0.07        93\n",
      "          43       0.42      0.53      0.47      1155\n",
      "          44       0.04      0.28      0.07       117\n",
      "          45       0.06      0.37      0.10       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.04      0.24      0.07       129\n",
      "          48       0.02      0.25      0.03        36\n",
      "          49       0.01      0.14      0.03        44\n",
      "          50       0.02      0.29      0.04        28\n",
      "          51       0.02      0.22      0.04        51\n",
      "          52       0.15      0.38      0.21       395\n",
      "          53       0.03      0.26      0.05        65\n",
      "          54       0.01      0.13      0.02        15\n",
      "          55       0.01      0.11      0.02        37\n",
      "          56       0.19      0.46      0.27       507\n",
      "          57       0.22      0.46      0.30       587\n",
      "          58       0.05      0.28      0.09       148\n",
      "          59       0.07      0.37      0.12       173\n",
      "          60       0.04      0.29      0.07        66\n",
      "          61       0.02      0.24      0.04        51\n",
      "          62       0.01      0.12      0.02        66\n",
      "          63       0.02      0.18      0.03        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.09      0.41      0.15       228\n",
      "          66       0.01      0.15      0.03        27\n",
      "          67       0.04      0.27      0.07       119\n",
      "          68       0.34      0.48      0.40       911\n",
      "          69       0.03      0.36      0.05        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.09      0.38      0.14      9022\n",
      "   macro avg       0.05      0.23      0.08      9022\n",
      "weighted avg       0.17      0.38      0.22      9022\n",
      " samples avg       0.11      0.40      0.14      9022\n",
      "\n",
      "t1_10000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.192    |             |\n",
      "| Term Wise Accuracy |    0.808    |             |\n",
      "|      Accuracy      |    0.007    |             |\n",
      "|     Precision      |    0.047    |    0.085    |\n",
      "|       Recall       |    0.208    |    0.357    |\n",
      "|     F1-measure     |    0.070    |    0.137    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.192 |\n",
      "|   Literature Accuracy    | 0.083 |\n",
      "|   Literature Precision   | 0.105 |\n",
      "|     LiteratureRecall     | 0.380 |\n",
      "|   LiteratureF1-measure   | 0.164 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.29      0.05        56\n",
      "           1       0.04      0.24      0.07       129\n",
      "           2       0.00      0.04      0.01        28\n",
      "           3       0.01      0.14      0.02        22\n",
      "           4       0.01      0.11      0.02        18\n",
      "           5       0.02      0.20      0.03        35\n",
      "           6       0.02      0.13      0.03        30\n",
      "           7       0.03      0.25      0.06        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.02      0.22      0.04        45\n",
      "          10       0.01      0.18      0.02        11\n",
      "          11       0.02      0.20      0.03        40\n",
      "          12       0.05      0.31      0.08       115\n",
      "          13       0.00      0.06      0.01        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.12      0.35      0.18       368\n",
      "          18       0.01      0.11      0.02        27\n",
      "          19       0.04      0.30      0.07        97\n",
      "          20       0.20      0.42      0.27       551\n",
      "          21       0.02      0.17      0.03        41\n",
      "          22       0.03      0.24      0.06        85\n",
      "          23       0.02      0.24      0.04        42\n",
      "          24       0.04      0.31      0.07        83\n",
      "          25       0.06      0.28      0.09       159\n",
      "          26       0.05      0.29      0.09       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.21      0.44      0.28       596\n",
      "          29       0.07      0.30      0.11       190\n",
      "          30       0.03      0.22      0.05        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.01      0.15      0.02        26\n",
      "          33       0.03      0.20      0.05        64\n",
      "          34       0.02      0.16      0.03        25\n",
      "          35       0.02      0.28      0.04        29\n",
      "          36       0.04      0.30      0.07        92\n",
      "          37       0.05      0.25      0.08       172\n",
      "          38       0.05      0.29      0.08       136\n",
      "          39       0.02      0.12      0.04        32\n",
      "          40       0.01      0.10      0.02        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.03      0.19      0.05        93\n",
      "          43       0.41      0.49      0.45      1155\n",
      "          44       0.05      0.31      0.08       117\n",
      "          45       0.05      0.28      0.08       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.06      0.31      0.10       129\n",
      "          48       0.01      0.17      0.02        36\n",
      "          49       0.03      0.27      0.05        44\n",
      "          50       0.01      0.14      0.02        28\n",
      "          51       0.02      0.25      0.04        51\n",
      "          52       0.15      0.41      0.22       395\n",
      "          53       0.01      0.09      0.02        65\n",
      "          54       0.01      0.07      0.01        15\n",
      "          55       0.01      0.14      0.02        37\n",
      "          56       0.19      0.42      0.26       507\n",
      "          57       0.21      0.43      0.28       587\n",
      "          58       0.05      0.28      0.09       148\n",
      "          59       0.06      0.27      0.09       173\n",
      "          60       0.04      0.33      0.08        66\n",
      "          61       0.01      0.10      0.02        51\n",
      "          62       0.03      0.23      0.05        66\n",
      "          63       0.01      0.05      0.01        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.07      0.32      0.12       228\n",
      "          66       0.01      0.11      0.02        27\n",
      "          67       0.05      0.32      0.09       119\n",
      "          68       0.33      0.48      0.39       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.03      0.38      0.06        13\n",
      "\n",
      "   micro avg       0.08      0.36      0.14      9022\n",
      "   macro avg       0.05      0.21      0.07      9022\n",
      "weighted avg       0.17      0.36      0.21      9022\n",
      " samples avg       0.10      0.38      0.14      9022\n",
      "\n",
      "t2_10000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.203    |             |\n",
      "| Term Wise Accuracy |    0.797    |             |\n",
      "|      Accuracy      |    0.007    |             |\n",
      "|     Precision      |    0.048    |    0.083    |\n",
      "|       Recall       |    0.226    |    0.369    |\n",
      "|     F1-measure     |    0.072    |    0.135    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.203 |\n",
      "|   Literature Accuracy    | 0.082 |\n",
      "|   Literature Precision   | 0.103 |\n",
      "|     LiteratureRecall     | 0.384 |\n",
      "|   LiteratureF1-measure   | 0.163 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.20      0.04        56\n",
      "           1       0.04      0.29      0.08       129\n",
      "           2       0.01      0.18      0.03        28\n",
      "           3       0.01      0.18      0.02        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.02      0.31      0.04        35\n",
      "           6       0.01      0.10      0.02        30\n",
      "           7       0.03      0.25      0.05        79\n",
      "           8       0.01      0.12      0.02         8\n",
      "           9       0.02      0.24      0.04        45\n",
      "          10       0.00      0.09      0.01        11\n",
      "          11       0.01      0.20      0.03        40\n",
      "          12       0.05      0.31      0.08       115\n",
      "          13       0.00      0.06      0.01        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.07      0.01        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.12      0.35      0.17       368\n",
      "          18       0.01      0.07      0.02        27\n",
      "          19       0.03      0.23      0.05        97\n",
      "          20       0.19      0.40      0.26       551\n",
      "          21       0.02      0.17      0.03        41\n",
      "          22       0.03      0.29      0.06        85\n",
      "          23       0.02      0.26      0.04        42\n",
      "          24       0.03      0.27      0.05        83\n",
      "          25       0.07      0.38      0.12       159\n",
      "          26       0.06      0.36      0.10       113\n",
      "          27       0.01      0.09      0.01        11\n",
      "          28       0.21      0.42      0.28       596\n",
      "          29       0.08      0.39      0.14       190\n",
      "          30       0.04      0.33      0.07        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.01      0.08      0.01        26\n",
      "          33       0.03      0.27      0.06        64\n",
      "          34       0.01      0.12      0.02        25\n",
      "          35       0.01      0.07      0.01        29\n",
      "          36       0.03      0.27      0.06        92\n",
      "          37       0.06      0.31      0.10       172\n",
      "          38       0.05      0.31      0.08       136\n",
      "          39       0.03      0.22      0.05        32\n",
      "          40       0.01      0.10      0.02        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.03      0.23      0.05        93\n",
      "          43       0.41      0.50      0.45      1155\n",
      "          44       0.04      0.29      0.08       117\n",
      "          45       0.05      0.32      0.09       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.05      0.33      0.09       129\n",
      "          48       0.01      0.14      0.02        36\n",
      "          49       0.02      0.23      0.04        44\n",
      "          50       0.01      0.21      0.03        28\n",
      "          51       0.03      0.27      0.05        51\n",
      "          52       0.15      0.41      0.22       395\n",
      "          53       0.02      0.14      0.03        65\n",
      "          54       0.01      0.13      0.02        15\n",
      "          55       0.02      0.30      0.04        37\n",
      "          56       0.18      0.40      0.25       507\n",
      "          57       0.22      0.45      0.30       587\n",
      "          58       0.06      0.36      0.11       148\n",
      "          59       0.07      0.38      0.12       173\n",
      "          60       0.04      0.29      0.06        66\n",
      "          61       0.03      0.31      0.06        51\n",
      "          62       0.03      0.26      0.05        66\n",
      "          63       0.02      0.26      0.04        39\n",
      "          64       0.01      0.12      0.02         8\n",
      "          65       0.08      0.37      0.14       228\n",
      "          66       0.01      0.11      0.02        27\n",
      "          67       0.05      0.34      0.09       119\n",
      "          68       0.32      0.45      0.38       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.08      0.01        13\n",
      "\n",
      "   micro avg       0.08      0.37      0.13      9022\n",
      "   macro avg       0.05      0.23      0.07      9022\n",
      "weighted avg       0.17      0.37      0.21      9022\n",
      " samples avg       0.10      0.38      0.13      9022\n",
      "\n",
      "t2_10000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.209    |             |\n",
      "| Term Wise Accuracy |    0.791    |             |\n",
      "|      Accuracy      |    0.006    |             |\n",
      "|     Precision      |    0.049    |    0.082    |\n",
      "|       Recall       |    0.236    |    0.382    |\n",
      "|     F1-measure     |    0.073    |    0.136    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.209 |\n",
      "|   Literature Accuracy    | 0.084 |\n",
      "|   Literature Precision   | 0.106 |\n",
      "|     LiteratureRecall     | 0.401 |\n",
      "|   LiteratureF1-measure   | 0.168 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.27      0.05        56\n",
      "           1       0.06      0.40      0.11       129\n",
      "           2       0.01      0.14      0.02        28\n",
      "           3       0.01      0.23      0.03        22\n",
      "           4       0.01      0.06      0.01        18\n",
      "           5       0.01      0.14      0.02        35\n",
      "           6       0.01      0.07      0.01        30\n",
      "           7       0.03      0.32      0.06        79\n",
      "           8       0.02      0.25      0.04         8\n",
      "           9       0.02      0.18      0.03        45\n",
      "          10       0.00      0.09      0.01        11\n",
      "          11       0.01      0.15      0.02        40\n",
      "          12       0.05      0.30      0.08       115\n",
      "          13       0.00      0.06      0.01        18\n",
      "          14       0.01      0.11      0.02         9\n",
      "          15       0.00      0.07      0.01        15\n",
      "          16       0.00      0.08      0.01        13\n",
      "          17       0.14      0.43      0.22       368\n",
      "          18       0.00      0.04      0.01        27\n",
      "          19       0.05      0.34      0.08        97\n",
      "          20       0.20      0.43      0.27       551\n",
      "          21       0.01      0.15      0.03        41\n",
      "          22       0.03      0.24      0.05        85\n",
      "          23       0.02      0.24      0.03        42\n",
      "          24       0.03      0.25      0.05        83\n",
      "          25       0.05      0.25      0.08       159\n",
      "          26       0.05      0.32      0.09       113\n",
      "          27       0.01      0.18      0.02        11\n",
      "          28       0.22      0.47      0.30       596\n",
      "          29       0.08      0.38      0.13       190\n",
      "          30       0.04      0.31      0.07        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.01      0.08      0.01        26\n",
      "          33       0.04      0.28      0.07        64\n",
      "          34       0.02      0.28      0.03        25\n",
      "          35       0.01      0.17      0.02        29\n",
      "          36       0.04      0.34      0.07        92\n",
      "          37       0.07      0.37      0.11       172\n",
      "          38       0.06      0.35      0.10       136\n",
      "          39       0.01      0.06      0.01        32\n",
      "          40       0.02      0.12      0.03        40\n",
      "          41       0.01      0.20      0.02         5\n",
      "          42       0.04      0.30      0.07        93\n",
      "          43       0.41      0.49      0.44      1155\n",
      "          44       0.04      0.28      0.07       117\n",
      "          45       0.05      0.33      0.09       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.05      0.33      0.09       129\n",
      "          48       0.01      0.14      0.02        36\n",
      "          49       0.02      0.27      0.04        44\n",
      "          50       0.01      0.14      0.02        28\n",
      "          51       0.02      0.20      0.04        51\n",
      "          52       0.15      0.41      0.22       395\n",
      "          53       0.03      0.29      0.06        65\n",
      "          54       0.01      0.20      0.02        15\n",
      "          55       0.01      0.22      0.03        37\n",
      "          56       0.19      0.44      0.27       507\n",
      "          57       0.22      0.44      0.29       587\n",
      "          58       0.05      0.30      0.09       148\n",
      "          59       0.07      0.37      0.12       173\n",
      "          60       0.04      0.32      0.07        66\n",
      "          61       0.02      0.22      0.04        51\n",
      "          62       0.03      0.27      0.05        66\n",
      "          63       0.00      0.03      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.08      0.36      0.14       228\n",
      "          66       0.02      0.22      0.04        27\n",
      "          67       0.04      0.29      0.08       119\n",
      "          68       0.32      0.48      0.39       911\n",
      "          69       0.02      0.21      0.03        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.08      0.38      0.14      9022\n",
      "   macro avg       0.05      0.24      0.07      9022\n",
      "weighted avg       0.17      0.38      0.22      9022\n",
      " samples avg       0.11      0.40      0.14      9022\n",
      "\n",
      "t2_10000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.209    |             |\n",
      "| Term Wise Accuracy |    0.791    |             |\n",
      "|      Accuracy      |    0.006    |             |\n",
      "|     Precision      |    0.048    |    0.081    |\n",
      "|       Recall       |    0.232    |    0.372    |\n",
      "|     F1-measure     |    0.072    |    0.133    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.209 |\n",
      "|   Literature Accuracy    | 0.080 |\n",
      "|   Literature Precision   | 0.100 |\n",
      "|     LiteratureRecall     | 0.388 |\n",
      "|   LiteratureF1-measure   | 0.159 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.23      0.04        56\n",
      "           1       0.05      0.30      0.08       129\n",
      "           2       0.01      0.18      0.03        28\n",
      "           3       0.01      0.23      0.03        22\n",
      "           4       0.01      0.11      0.02        18\n",
      "           5       0.02      0.31      0.05        35\n",
      "           6       0.01      0.07      0.01        30\n",
      "           7       0.04      0.35      0.07        79\n",
      "           8       0.02      0.12      0.03         8\n",
      "           9       0.03      0.29      0.05        45\n",
      "          10       0.00      0.09      0.01        11\n",
      "          11       0.01      0.20      0.03        40\n",
      "          12       0.05      0.32      0.08       115\n",
      "          13       0.01      0.22      0.02        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.01      0.13      0.02        15\n",
      "          16       0.01      0.15      0.02        13\n",
      "          17       0.14      0.40      0.20       368\n",
      "          18       0.00      0.04      0.01        27\n",
      "          19       0.04      0.27      0.06        97\n",
      "          20       0.18      0.39      0.24       551\n",
      "          21       0.02      0.22      0.04        41\n",
      "          22       0.03      0.26      0.06        85\n",
      "          23       0.01      0.17      0.03        42\n",
      "          24       0.02      0.19      0.04        83\n",
      "          25       0.06      0.33      0.10       159\n",
      "          26       0.06      0.35      0.10       113\n",
      "          27       0.01      0.09      0.01        11\n",
      "          28       0.21      0.43      0.28       596\n",
      "          29       0.08      0.38      0.13       190\n",
      "          30       0.04      0.37      0.08        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.04      0.00        26\n",
      "          33       0.03      0.25      0.06        64\n",
      "          34       0.01      0.20      0.03        25\n",
      "          35       0.01      0.07      0.01        29\n",
      "          36       0.04      0.29      0.06        92\n",
      "          37       0.08      0.41      0.13       172\n",
      "          38       0.05      0.28      0.08       136\n",
      "          39       0.02      0.12      0.03        32\n",
      "          40       0.01      0.10      0.02        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.04      0.29      0.06        93\n",
      "          43       0.41      0.50      0.45      1155\n",
      "          44       0.05      0.32      0.09       117\n",
      "          45       0.06      0.35      0.10       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.05      0.29      0.09       129\n",
      "          48       0.01      0.22      0.03        36\n",
      "          49       0.01      0.14      0.03        44\n",
      "          50       0.02      0.25      0.03        28\n",
      "          51       0.02      0.22      0.03        51\n",
      "          52       0.14      0.38      0.21       395\n",
      "          53       0.02      0.18      0.04        65\n",
      "          54       0.02      0.33      0.03        15\n",
      "          55       0.01      0.16      0.02        37\n",
      "          56       0.18      0.41      0.25       507\n",
      "          57       0.21      0.43      0.28       587\n",
      "          58       0.05      0.30      0.09       148\n",
      "          59       0.07      0.37      0.12       173\n",
      "          60       0.04      0.36      0.08        66\n",
      "          61       0.02      0.20      0.03        51\n",
      "          62       0.03      0.26      0.05        66\n",
      "          63       0.01      0.15      0.02        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.07      0.33      0.12       228\n",
      "          66       0.02      0.22      0.04        27\n",
      "          67       0.05      0.36      0.09       119\n",
      "          68       0.33      0.48      0.39       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.08      0.37      0.13      9022\n",
      "   macro avg       0.05      0.23      0.07      9022\n",
      "weighted avg       0.16      0.37      0.21      9022\n",
      " samples avg       0.10      0.39      0.13      9022\n",
      "\n",
      "t1_15000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.075    |             |\n",
      "| Term Wise Accuracy |    0.925    |             |\n",
      "|      Accuracy      |    0.014    |             |\n",
      "|     Precision      |    0.103    |    0.203    |\n",
      "|       Recall       |    0.080    |    0.254    |\n",
      "|     F1-measure     |    0.074    |    0.226    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.075 |\n",
      "|   Literature Accuracy    | 0.136 |\n",
      "|   Literature Precision   | 0.215 |\n",
      "|     LiteratureRecall     | 0.283 |\n",
      "|   LiteratureF1-measure   | 0.245 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.02      0.02        56\n",
      "           1       0.08      0.12      0.09       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.50      0.05      0.08        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.33      0.03      0.06        30\n",
      "           7       0.01      0.01      0.01        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.13      0.04      0.07        45\n",
      "          10       1.00      0.09      0.17        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.03      0.03      0.03       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.16      0.29      0.21       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.01      0.01      0.01        97\n",
      "          20       0.21      0.33      0.26       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.07      0.08      0.08        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.03      0.04      0.03        83\n",
      "          25       0.06      0.08      0.07       159\n",
      "          26       0.18      0.17      0.17       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.21      0.31      0.25       596\n",
      "          29       0.14      0.22      0.17       190\n",
      "          30       0.08      0.10      0.09        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.25      0.04      0.07        26\n",
      "          33       0.05      0.02      0.02        64\n",
      "          34       0.20      0.08      0.11        25\n",
      "          35       0.14      0.03      0.06        29\n",
      "          36       0.06      0.09      0.07        92\n",
      "          37       0.05      0.09      0.07       172\n",
      "          38       0.09      0.14      0.11       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.05      0.06      0.06        93\n",
      "          43       0.45      0.50      0.47      1155\n",
      "          44       0.06      0.07      0.06       117\n",
      "          45       0.09      0.16      0.11       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.07      0.08      0.07       129\n",
      "          48       0.08      0.03      0.04        36\n",
      "          49       0.15      0.05      0.07        44\n",
      "          50       0.10      0.04      0.05        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.20      0.27      0.23       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.19      0.31      0.24       507\n",
      "          57       0.25      0.35      0.29       587\n",
      "          58       0.09      0.11      0.10       148\n",
      "          59       0.13      0.20      0.16       173\n",
      "          60       0.23      0.15      0.18        66\n",
      "          61       0.03      0.02      0.02        51\n",
      "          62       0.03      0.03      0.03        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.08      0.15      0.11       228\n",
      "          66       0.50      0.04      0.07        27\n",
      "          67       0.08      0.11      0.09       119\n",
      "          68       0.38      0.49      0.43       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.20      0.25      0.23      9022\n",
      "   macro avg       0.10      0.08      0.07      9022\n",
      "weighted avg       0.20      0.25      0.22      9022\n",
      " samples avg       0.22      0.28      0.20      9022\n",
      "\n",
      "t1_15000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.082    |             |\n",
      "| Term Wise Accuracy |    0.918    |             |\n",
      "|      Accuracy      |    0.011    |             |\n",
      "|     Precision      |    0.088    |    0.185    |\n",
      "|       Recall       |    0.095    |    0.272    |\n",
      "|     F1-measure     |    0.080    |    0.221    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.082 |\n",
      "|   Literature Accuracy    | 0.135 |\n",
      "|   Literature Precision   | 0.205 |\n",
      "|     LiteratureRecall     | 0.306 |\n",
      "|   LiteratureF1-measure   | 0.246 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.02      0.02        56\n",
      "           1       0.12      0.19      0.15       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.07      0.06      0.06        35\n",
      "           6       0.33      0.03      0.06        30\n",
      "           7       0.02      0.04      0.03        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.03      0.02      0.03        45\n",
      "          10       0.67      0.18      0.29        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.05      0.08      0.06       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.14      0.27      0.18       368\n",
      "          18       0.50      0.04      0.07        27\n",
      "          19       0.02      0.04      0.03        97\n",
      "          20       0.22      0.35      0.27       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.08      0.12      0.09        85\n",
      "          23       0.05      0.05      0.05        42\n",
      "          24       0.04      0.06      0.05        83\n",
      "          25       0.05      0.09      0.07       159\n",
      "          26       0.18      0.24      0.21       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.24      0.36      0.29       596\n",
      "          29       0.13      0.23      0.16       190\n",
      "          30       0.04      0.08      0.06        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.08      0.04      0.05        26\n",
      "          33       0.07      0.05      0.06        64\n",
      "          34       0.18      0.08      0.11        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.04      0.08      0.05        92\n",
      "          37       0.06      0.15      0.09       172\n",
      "          38       0.10      0.18      0.13       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.05      0.09      0.06        93\n",
      "          43       0.45      0.52      0.48      1155\n",
      "          44       0.07      0.12      0.09       117\n",
      "          45       0.07      0.17      0.10       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.08      0.12      0.09       129\n",
      "          48       0.06      0.08      0.07        36\n",
      "          49       0.09      0.07      0.08        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.06      0.08      0.07        51\n",
      "          52       0.18      0.27      0.21       395\n",
      "          53       0.02      0.02      0.02        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.02      0.03      0.02        37\n",
      "          56       0.21      0.35      0.26       507\n",
      "          57       0.25      0.37      0.30       587\n",
      "          58       0.05      0.11      0.07       148\n",
      "          59       0.12      0.22      0.15       173\n",
      "          60       0.17      0.17      0.17        66\n",
      "          61       0.02      0.02      0.02        51\n",
      "          62       0.03      0.05      0.03        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.08      0.15      0.10       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.06      0.10      0.07       119\n",
      "          68       0.36      0.47      0.41       911\n",
      "          69       0.20      0.07      0.11        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.19      0.27      0.22      9022\n",
      "   macro avg       0.09      0.10      0.08      9022\n",
      "weighted avg       0.20      0.27      0.22      9022\n",
      " samples avg       0.21      0.31      0.20      9022\n",
      "\n",
      "t1_15000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.074    |             |\n",
      "| Term Wise Accuracy |    0.926    |             |\n",
      "|      Accuracy      |    0.014    |             |\n",
      "|     Precision      |    0.094    |    0.207    |\n",
      "|       Recall       |    0.084    |    0.254    |\n",
      "|     F1-measure     |    0.076    |    0.228    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.074 |\n",
      "|   Literature Accuracy    | 0.136 |\n",
      "|   Literature Precision   | 0.211 |\n",
      "|     LiteratureRecall     | 0.281 |\n",
      "|   LiteratureF1-measure   | 0.241 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.09      0.12      0.10       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.17      0.05      0.07        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.50      0.03      0.06        30\n",
      "           7       0.06      0.06      0.06        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.10      0.04      0.06        45\n",
      "          10       1.00      0.09      0.17        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.07      0.10      0.08       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.12      0.21      0.15       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.08      0.09      0.09        97\n",
      "          20       0.21      0.32      0.25       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.09      0.09      0.09        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.03      0.04      0.03        83\n",
      "          25       0.08      0.11      0.09       159\n",
      "          26       0.18      0.19      0.19       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.22      0.33      0.26       596\n",
      "          29       0.14      0.22      0.17       190\n",
      "          30       0.07      0.08      0.08        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.25      0.08      0.12        26\n",
      "          33       0.10      0.03      0.05        64\n",
      "          34       0.33      0.08      0.13        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.05      0.07      0.06        92\n",
      "          37       0.11      0.19      0.14       172\n",
      "          38       0.10      0.14      0.11       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.09      0.13      0.11        93\n",
      "          43       0.45      0.50      0.47      1155\n",
      "          44       0.08      0.10      0.09       117\n",
      "          45       0.11      0.21      0.15       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.10      0.11      0.11       129\n",
      "          48       0.05      0.03      0.04        36\n",
      "          49       0.07      0.02      0.03        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.18      0.29      0.22       395\n",
      "          53       0.02      0.02      0.02        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.19      0.28      0.23       507\n",
      "          57       0.25      0.36      0.30       587\n",
      "          58       0.08      0.10      0.09       148\n",
      "          59       0.09      0.13      0.11       173\n",
      "          60       0.15      0.09      0.11        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.04      0.03      0.03        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.12      0.21      0.15       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.08      0.10      0.09       119\n",
      "          68       0.38      0.45      0.41       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.21      0.25      0.23      9022\n",
      "   macro avg       0.09      0.08      0.08      9022\n",
      "weighted avg       0.20      0.25      0.22      9022\n",
      " samples avg       0.21      0.28      0.20      9022\n",
      "\n",
      "t2_15000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.077    |             |\n",
      "| Term Wise Accuracy |    0.923    |             |\n",
      "|      Accuracy      |    0.013    |             |\n",
      "|     Precision      |    0.087    |    0.197    |\n",
      "|       Recall       |    0.089    |    0.259    |\n",
      "|     F1-measure     |    0.079    |    0.224    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.077 |\n",
      "|   Literature Accuracy    | 0.133 |\n",
      "|   Literature Precision   | 0.209 |\n",
      "|     LiteratureRecall     | 0.290 |\n",
      "|   LiteratureF1-measure   | 0.243 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.02      0.02        56\n",
      "           1       0.10      0.16      0.12       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.25      0.05      0.08        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.20      0.03      0.06        30\n",
      "           7       0.07      0.11      0.09        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.04      0.02      0.03        45\n",
      "          10       0.50      0.09      0.15        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.08      0.13      0.10       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.13      0.22      0.16       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.05      0.08      0.07        97\n",
      "          20       0.21      0.32      0.25       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.05      0.07      0.06        85\n",
      "          23       0.08      0.02      0.04        42\n",
      "          24       0.02      0.02      0.02        83\n",
      "          25       0.06      0.09      0.07       159\n",
      "          26       0.21      0.24      0.22       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.22      0.33      0.26       596\n",
      "          29       0.12      0.20      0.15       190\n",
      "          30       0.13      0.16      0.14        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.11      0.04      0.06        26\n",
      "          33       0.11      0.06      0.08        64\n",
      "          34       0.29      0.08      0.12        25\n",
      "          35       0.08      0.03      0.05        29\n",
      "          36       0.04      0.05      0.04        92\n",
      "          37       0.08      0.16      0.11       172\n",
      "          38       0.05      0.09      0.07       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.11      0.03      0.04        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.08      0.13      0.10        93\n",
      "          43       0.46      0.52      0.49      1155\n",
      "          44       0.07      0.09      0.08       117\n",
      "          45       0.08      0.17      0.11       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.07      0.10      0.08       129\n",
      "          48       0.07      0.06      0.06        36\n",
      "          49       0.17      0.07      0.10        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.06      0.06      0.06        51\n",
      "          52       0.21      0.32      0.25       395\n",
      "          53       0.08      0.06      0.07        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.19      0.29      0.23       507\n",
      "          57       0.23      0.33      0.27       587\n",
      "          58       0.05      0.07      0.06       148\n",
      "          59       0.10      0.20      0.13       173\n",
      "          60       0.26      0.17      0.20        66\n",
      "          61       0.03      0.02      0.02        51\n",
      "          62       0.02      0.03      0.03        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.11      0.19      0.14       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.05      0.08      0.07       119\n",
      "          68       0.39      0.47      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.20      0.26      0.22      9022\n",
      "   macro avg       0.09      0.09      0.08      9022\n",
      "weighted avg       0.20      0.26      0.22      9022\n",
      " samples avg       0.21      0.29      0.20      9022\n",
      "\n",
      "t2_15000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.085    |             |\n",
      "| Term Wise Accuracy |    0.915    |             |\n",
      "|      Accuracy      |    0.010    |             |\n",
      "|     Precision      |    0.078    |    0.183    |\n",
      "|       Recall       |    0.102    |    0.284    |\n",
      "|     F1-measure     |    0.081    |    0.223    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.085 |\n",
      "|   Literature Accuracy    | 0.133 |\n",
      "|   Literature Precision   | 0.199 |\n",
      "|     LiteratureRecall     | 0.312 |\n",
      "|   LiteratureF1-measure   | 0.243 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.08      0.16      0.11       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.04      0.03      0.03        35\n",
      "           6       0.17      0.03      0.06        30\n",
      "           7       0.02      0.04      0.03        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.08      0.09      0.09        45\n",
      "          10       0.67      0.18      0.29        11\n",
      "          11       0.02      0.03      0.02        40\n",
      "          12       0.04      0.08      0.05       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.15      0.28      0.19       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.05      0.08      0.06        97\n",
      "          20       0.21      0.35      0.26       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.06      0.11      0.08        85\n",
      "          23       0.03      0.02      0.02        42\n",
      "          24       0.03      0.06      0.04        83\n",
      "          25       0.06      0.13      0.08       159\n",
      "          26       0.15      0.22      0.18       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.22      0.37      0.27       596\n",
      "          29       0.12      0.26      0.17       190\n",
      "          30       0.06      0.12      0.08        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.05      0.04      0.04        26\n",
      "          33       0.06      0.05      0.05        64\n",
      "          34       0.12      0.08      0.10        25\n",
      "          35       0.08      0.07      0.08        29\n",
      "          36       0.04      0.10      0.06        92\n",
      "          37       0.09      0.20      0.12       172\n",
      "          38       0.10      0.21      0.14       136\n",
      "          39       0.07      0.03      0.04        32\n",
      "          40       0.07      0.03      0.04        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.02      0.04      0.03        93\n",
      "          43       0.47      0.53      0.50      1155\n",
      "          44       0.07      0.12      0.09       117\n",
      "          45       0.09      0.25      0.13       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.05      0.10      0.07       129\n",
      "          48       0.04      0.06      0.05        36\n",
      "          49       0.04      0.05      0.04        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.02      0.02      0.02        51\n",
      "          52       0.21      0.35      0.26       395\n",
      "          53       0.04      0.05      0.05        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.22      0.38      0.28       507\n",
      "          57       0.22      0.34      0.27       587\n",
      "          58       0.06      0.11      0.08       148\n",
      "          59       0.10      0.22      0.14       173\n",
      "          60       0.17      0.18      0.18        66\n",
      "          61       0.02      0.02      0.02        51\n",
      "          62       0.03      0.05      0.03        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.13      0.29      0.18       228\n",
      "          66       0.20      0.07      0.11        27\n",
      "          67       0.06      0.11      0.08       119\n",
      "          68       0.38      0.47      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.18      0.28      0.22      9022\n",
      "   macro avg       0.08      0.10      0.08      9022\n",
      "weighted avg       0.20      0.28      0.23      9022\n",
      " samples avg       0.20      0.31      0.20      9022\n",
      "\n",
      "t2_15000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.078    |             |\n",
      "| Term Wise Accuracy |    0.922    |             |\n",
      "|      Accuracy      |    0.009    |             |\n",
      "|     Precision      |    0.096    |    0.192    |\n",
      "|       Recall       |    0.087    |    0.260    |\n",
      "|     F1-measure     |    0.077    |    0.221    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.078 |\n",
      "|   Literature Accuracy    | 0.131 |\n",
      "|   Literature Precision   | 0.208 |\n",
      "|     LiteratureRecall     | 0.294 |\n",
      "|   LiteratureF1-measure   | 0.244 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.11      0.18      0.13       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.20      0.05      0.07        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.33      0.03      0.06        30\n",
      "           7       0.03      0.04      0.03        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.07      0.04      0.05        45\n",
      "          10       1.00      0.09      0.17        11\n",
      "          11       0.03      0.03      0.03        40\n",
      "          12       0.05      0.06      0.05       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.15      0.27      0.19       368\n",
      "          18       0.33      0.04      0.07        27\n",
      "          19       0.05      0.05      0.05        97\n",
      "          20       0.22      0.32      0.26       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.03      0.05      0.04        85\n",
      "          23       0.11      0.05      0.07        42\n",
      "          24       0.02      0.02      0.02        83\n",
      "          25       0.03      0.06      0.04       159\n",
      "          26       0.16      0.19      0.18       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.20      0.30      0.24       596\n",
      "          29       0.12      0.22      0.15       190\n",
      "          30       0.15      0.16      0.15        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.29      0.08      0.12        26\n",
      "          33       0.07      0.03      0.04        64\n",
      "          34       0.17      0.08      0.11        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.05      0.10      0.07        92\n",
      "          37       0.07      0.14      0.09       172\n",
      "          38       0.07      0.11      0.08       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.11      0.03      0.04        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.08      0.12      0.10        93\n",
      "          43       0.46      0.51      0.49      1155\n",
      "          44       0.06      0.11      0.08       117\n",
      "          45       0.09      0.19      0.12       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.09      0.12      0.10       129\n",
      "          48       0.04      0.03      0.03        36\n",
      "          49       0.11      0.05      0.06        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.03      0.02      0.02        51\n",
      "          52       0.19      0.31      0.24       395\n",
      "          53       0.07      0.05      0.06        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.03      0.03      0.03        37\n",
      "          56       0.18      0.31      0.23       507\n",
      "          57       0.23      0.36      0.28       587\n",
      "          58       0.07      0.12      0.09       148\n",
      "          59       0.10      0.18      0.13       173\n",
      "          60       0.16      0.09      0.12        66\n",
      "          61       0.09      0.06      0.07        51\n",
      "          62       0.03      0.05      0.04        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.09      0.17      0.12       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.05      0.08      0.06       119\n",
      "          68       0.37      0.47      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.19      0.26      0.22      9022\n",
      "   macro avg       0.10      0.09      0.08      9022\n",
      "weighted avg       0.20      0.26      0.22      9022\n",
      " samples avg       0.21      0.29      0.20      9022\n",
      "\n",
      "t1_20000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.060    |             |\n",
      "| Term Wise Accuracy |    0.940    |             |\n",
      "|      Accuracy      |    0.026    |             |\n",
      "|     Precision      |    0.153    |    0.269    |\n",
      "|       Recall       |    0.064    |    0.230    |\n",
      "|     F1-measure     |    0.071    |    0.248    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.060 |\n",
      "|   Literature Accuracy    | 0.155 |\n",
      "|   Literature Precision   | 0.267 |\n",
      "|     LiteratureRecall     | 0.267 |\n",
      "|   LiteratureF1-measure   | 0.267 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.11      0.07      0.09       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.50      0.05      0.08        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.33      0.03      0.05        35\n",
      "           6       0.50      0.03      0.06        30\n",
      "           7       0.12      0.05      0.07        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       1.00      0.02      0.04        45\n",
      "          10       1.00      0.09      0.17        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.07      0.03      0.04       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.14      0.20      0.16       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.25      0.29      0.27       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.09      0.02      0.04        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.04      0.01      0.02        83\n",
      "          25       0.08      0.04      0.06       159\n",
      "          26       0.37      0.16      0.22       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.25      0.32      0.28       596\n",
      "          29       0.18      0.14      0.16       190\n",
      "          30       0.10      0.05      0.07        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.50      0.08      0.13        26\n",
      "          33       0.14      0.02      0.03        64\n",
      "          34       0.29      0.08      0.12        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.03      0.01      0.02        92\n",
      "          37       0.05      0.05      0.05       172\n",
      "          38       0.15      0.11      0.13       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.08      0.03      0.05        93\n",
      "          43       0.49      0.53      0.51      1155\n",
      "          44       0.06      0.03      0.04       117\n",
      "          45       0.13      0.12      0.12       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.19      0.08      0.11       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.33      0.02      0.04        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.26      0.28      0.27       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.22      0.28      0.25       507\n",
      "          57       0.29      0.34      0.31       587\n",
      "          58       0.09      0.04      0.06       148\n",
      "          59       0.13      0.10      0.12       173\n",
      "          60       0.54      0.11      0.18        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.22      0.03      0.05        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.10      0.10      0.10       228\n",
      "          66       1.00      0.04      0.07        27\n",
      "          67       0.07      0.03      0.04       119\n",
      "          68       0.38      0.43      0.40       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.27      0.23      0.25      9022\n",
      "   macro avg       0.15      0.06      0.07      9022\n",
      "weighted avg       0.24      0.23      0.22      9022\n",
      " samples avg       0.27      0.27      0.22      9022\n",
      "\n",
      "t1_20000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.066    |             |\n",
      "| Term Wise Accuracy |    0.934    |             |\n",
      "|      Accuracy      |    0.021    |             |\n",
      "|     Precision      |    0.130    |    0.234    |\n",
      "|       Recall       |    0.077    |    0.240    |\n",
      "|     F1-measure     |    0.081    |    0.237    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.066 |\n",
      "|   Literature Accuracy    | 0.144 |\n",
      "|   Literature Precision   | 0.236 |\n",
      "|     LiteratureRecall     | 0.269 |\n",
      "|   LiteratureF1-measure   | 0.251 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.13      0.12      0.13       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.20      0.05      0.07        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.36      0.11      0.17        35\n",
      "           6       0.33      0.03      0.06        30\n",
      "           7       0.05      0.04      0.04        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.13      0.04      0.07        45\n",
      "          10       0.33      0.09      0.14        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.08      0.07      0.07       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.14      0.23      0.18       368\n",
      "          18       0.50      0.04      0.07        27\n",
      "          19       0.02      0.01      0.01        97\n",
      "          20       0.21      0.28      0.24       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.16      0.07      0.10        85\n",
      "          23       0.25      0.02      0.04        42\n",
      "          24       0.02      0.01      0.01        83\n",
      "          25       0.08      0.08      0.08       159\n",
      "          26       0.26      0.19      0.22       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.25      0.33      0.28       596\n",
      "          29       0.13      0.14      0.14       190\n",
      "          30       0.13      0.11      0.12        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.29      0.08      0.12        26\n",
      "          33       0.06      0.02      0.02        64\n",
      "          34       0.25      0.08      0.12        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.08      0.08      0.08        92\n",
      "          37       0.08      0.10      0.09       172\n",
      "          38       0.09      0.10      0.09       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.20      0.03      0.04        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.06      0.04      0.05        93\n",
      "          43       0.47      0.52      0.49      1155\n",
      "          44       0.08      0.06      0.07       117\n",
      "          45       0.10      0.12      0.11       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.07      0.06      0.07       129\n",
      "          48       0.07      0.03      0.04        36\n",
      "          49       0.18      0.05      0.07        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.07      0.02      0.03        51\n",
      "          52       0.21      0.25      0.23       395\n",
      "          53       0.05      0.02      0.02        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.19      0.26      0.22       507\n",
      "          57       0.26      0.33      0.29       587\n",
      "          58       0.07      0.07      0.07       148\n",
      "          59       0.12      0.12      0.12       173\n",
      "          60       0.42      0.17      0.24        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.06      0.03      0.04        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.10      0.14      0.12       228\n",
      "          66       1.00      0.04      0.07        27\n",
      "          67       0.06      0.05      0.05       119\n",
      "          68       0.39      0.46      0.42       911\n",
      "          69       0.33      0.07      0.12        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.23      0.24      0.24      9022\n",
      "   macro avg       0.13      0.08      0.08      9022\n",
      "weighted avg       0.22      0.24      0.22      9022\n",
      " samples avg       0.24      0.27      0.21      9022\n",
      "\n",
      "t1_20000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.060    |             |\n",
      "| Term Wise Accuracy |    0.940    |             |\n",
      "|      Accuracy      |    0.019    |             |\n",
      "|     Precision      |    0.127    |    0.267    |\n",
      "|       Recall       |    0.059    |    0.223    |\n",
      "|     F1-measure     |    0.066    |    0.243    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.060 |\n",
      "|   Literature Accuracy    | 0.146 |\n",
      "|   Literature Precision   | 0.252 |\n",
      "|     LiteratureRecall     | 0.255 |\n",
      "|   LiteratureF1-measure   | 0.253 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.15      0.09      0.11       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.50      0.03      0.06        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.33      0.02      0.04        45\n",
      "          10       1.00      0.09      0.17        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.03      0.01      0.01       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.17      0.21      0.18       368\n",
      "          18       1.00      0.04      0.07        27\n",
      "          19       0.13      0.04      0.06        97\n",
      "          20       0.22      0.27      0.24       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.09      0.02      0.04        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.09      0.06      0.07       159\n",
      "          26       0.27      0.12      0.16       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.24      0.30      0.26       596\n",
      "          29       0.15      0.11      0.12       190\n",
      "          30       0.19      0.08      0.12        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.33      0.04      0.07        26\n",
      "          33       0.25      0.02      0.03        64\n",
      "          34       0.33      0.08      0.13        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.22      0.10      0.14        92\n",
      "          37       0.11      0.08      0.09       172\n",
      "          38       0.10      0.07      0.08       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.05      0.02      0.03        93\n",
      "          43       0.48      0.51      0.49      1155\n",
      "          44       0.05      0.02      0.03       117\n",
      "          45       0.13      0.12      0.12       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.10      0.05      0.06       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.25      0.02      0.04        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.23      0.26      0.24       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.21      0.26      0.23       507\n",
      "          57       0.26      0.31      0.29       587\n",
      "          58       0.07      0.03      0.04       148\n",
      "          59       0.10      0.08      0.09       173\n",
      "          60       0.40      0.06      0.11        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.14      0.02      0.03        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.13      0.12      0.13       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.10      0.03      0.05       119\n",
      "          68       0.40      0.46      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.27      0.22      0.24      9022\n",
      "   macro avg       0.13      0.06      0.07      9022\n",
      "weighted avg       0.22      0.22      0.21      9022\n",
      " samples avg       0.25      0.25      0.21      9022\n",
      "\n",
      "t2_20000_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.061    |             |\n",
      "| Term Wise Accuracy |    0.939    |             |\n",
      "|      Accuracy      |    0.021    |             |\n",
      "|     Precision      |    0.116    |    0.259    |\n",
      "|       Recall       |    0.063    |    0.230    |\n",
      "|     F1-measure     |    0.068    |    0.243    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.061 |\n",
      "|   Literature Accuracy    | 0.151 |\n",
      "|   Literature Precision   | 0.260 |\n",
      "|     LiteratureRecall     | 0.263 |\n",
      "|   LiteratureF1-measure   | 0.262 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.12      0.10      0.11       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.50      0.03      0.06        30\n",
      "           7       0.10      0.04      0.06        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.50      0.02      0.04        45\n",
      "          10       0.50      0.09      0.15        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.16      0.21      0.19       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.06      0.03      0.04        97\n",
      "          20       0.23      0.28      0.25       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.12      0.04      0.05        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.05      0.04      0.05       159\n",
      "          26       0.32      0.17      0.22       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.21      0.27      0.24       596\n",
      "          29       0.17      0.14      0.15       190\n",
      "          30       0.12      0.06      0.08        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.33      0.04      0.07        26\n",
      "          33       0.08      0.02      0.03        64\n",
      "          34       0.25      0.08      0.12        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.04      0.02      0.03        92\n",
      "          37       0.06      0.05      0.06       172\n",
      "          38       0.10      0.08      0.09       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       1.00      0.03      0.05        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.07      0.04      0.05        93\n",
      "          43       0.47      0.52      0.49      1155\n",
      "          44       0.04      0.02      0.02       117\n",
      "          45       0.11      0.11      0.11       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.16      0.08      0.10       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.25      0.05      0.08        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.24      0.26      0.25       395\n",
      "          53       0.10      0.02      0.03        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.21      0.27      0.24       507\n",
      "          57       0.27      0.34      0.30       587\n",
      "          58       0.08      0.05      0.07       148\n",
      "          59       0.15      0.12      0.13       173\n",
      "          60       0.39      0.11      0.17        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.09      0.09      0.09       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.14      0.08      0.10       119\n",
      "          68       0.41      0.48      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.26      0.23      0.24      9022\n",
      "   macro avg       0.12      0.06      0.07      9022\n",
      "weighted avg       0.22      0.23      0.22      9022\n",
      " samples avg       0.26      0.26      0.22      9022\n",
      "\n",
      "t2_20000_relu_leaky_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.067    |             |\n",
      "| Term Wise Accuracy |    0.933    |             |\n",
      "|      Accuracy      |    0.017    |             |\n",
      "|     Precision      |    0.101    |    0.235    |\n",
      "|       Recall       |    0.077    |    0.254    |\n",
      "|     F1-measure     |    0.077    |    0.244    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.067 |\n",
      "|   Literature Accuracy    | 0.152 |\n",
      "|   Literature Precision   | 0.247 |\n",
      "|     LiteratureRecall     | 0.291 |\n",
      "|   LiteratureF1-measure   | 0.268 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.14      0.14      0.14       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.25      0.05      0.08        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.33      0.03      0.06        30\n",
      "           7       0.02      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.10      0.02      0.04        45\n",
      "          10       0.50      0.09      0.15        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.03      0.03      0.03       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.16      0.26      0.20       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.03      0.03      0.03        97\n",
      "          20       0.22      0.30      0.26       551\n",
      "          21       0.33      0.02      0.05        41\n",
      "          22       0.05      0.04      0.04        85\n",
      "          23       0.14      0.02      0.04        42\n",
      "          24       0.03      0.02      0.03        83\n",
      "          25       0.04      0.04      0.04       159\n",
      "          26       0.27      0.24      0.25       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.24      0.33      0.28       596\n",
      "          29       0.16      0.18      0.17       190\n",
      "          30       0.12      0.12      0.12        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.33      0.08      0.12        26\n",
      "          33       0.09      0.03      0.05        64\n",
      "          34       0.18      0.08      0.11        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.10      0.09      0.09        92\n",
      "          37       0.08      0.10      0.09       172\n",
      "          38       0.10      0.12      0.11       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.33      0.03      0.05        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.01      0.01      0.01        93\n",
      "          43       0.47      0.54      0.51      1155\n",
      "          44       0.08      0.07      0.07       117\n",
      "          45       0.12      0.18      0.14       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.07      0.06      0.06       129\n",
      "          48       0.06      0.03      0.04        36\n",
      "          49       0.06      0.02      0.03        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.23      0.27      0.25       395\n",
      "          53       0.06      0.02      0.02        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.20      0.32      0.24       507\n",
      "          57       0.27      0.36      0.31       587\n",
      "          58       0.06      0.07      0.07       148\n",
      "          59       0.14      0.15      0.14       173\n",
      "          60       0.30      0.14      0.19        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.05      0.03      0.04        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.11      0.14      0.12       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.11      0.12      0.11       119\n",
      "          68       0.38      0.47      0.42       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.23      0.25      0.24      9022\n",
      "   macro avg       0.10      0.08      0.08      9022\n",
      "weighted avg       0.21      0.25      0.23      9022\n",
      " samples avg       0.25      0.29      0.22      9022\n",
      "\n",
      "t2_20000_biploar_step_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.060    |             |\n",
      "| Term Wise Accuracy |    0.940    |             |\n",
      "|      Accuracy      |    0.020    |             |\n",
      "|     Precision      |    0.112    |    0.267    |\n",
      "|       Recall       |    0.063    |    0.231    |\n",
      "|     F1-measure     |    0.068    |    0.248    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.060 |\n",
      "|   Literature Accuracy    | 0.151 |\n",
      "|   Literature Precision   | 0.260 |\n",
      "|     LiteratureRecall     | 0.263 |\n",
      "|   LiteratureF1-measure   | 0.262 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.14      0.09      0.11       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.50      0.03      0.06        30\n",
      "           7       0.04      0.01      0.02        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.33      0.04      0.08        45\n",
      "          10       1.00      0.09      0.17        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.16      0.22      0.19       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.23      0.28      0.26       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.08      0.02      0.04        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.05      0.01      0.02        83\n",
      "          25       0.07      0.06      0.06       159\n",
      "          26       0.35      0.15      0.21       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.23      0.28      0.25       596\n",
      "          29       0.19      0.17      0.18       190\n",
      "          30       0.15      0.08      0.11        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.33      0.04      0.07        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.29      0.08      0.12        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.08      0.04      0.06        92\n",
      "          37       0.11      0.09      0.10       172\n",
      "          38       0.11      0.07      0.09       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.10      0.04      0.06        93\n",
      "          43       0.47      0.51      0.49      1155\n",
      "          44       0.08      0.03      0.05       117\n",
      "          45       0.08      0.08      0.08       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.12      0.06      0.08       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.25      0.02      0.04        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.24      0.27      0.25       395\n",
      "          53       0.18      0.03      0.05        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.22      0.28      0.25       507\n",
      "          57       0.27      0.33      0.30       587\n",
      "          58       0.09      0.05      0.06       148\n",
      "          59       0.15      0.12      0.14       173\n",
      "          60       0.50      0.08      0.13        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.08      0.02      0.03        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.12      0.12      0.12       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.15      0.08      0.10       119\n",
      "          68       0.41      0.47      0.44       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.27      0.23      0.25      9022\n",
      "   macro avg       0.11      0.06      0.07      9022\n",
      "weighted avg       0.23      0.23      0.22      9022\n",
      " samples avg       0.26      0.26      0.22      9022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in list_of_models_hidden_nodes:\n",
    "    print( \"t1_\"   +str(i)+\"_bipolar_sigmoid_test_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_bipolar_sigmoid\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_start\"], \"test\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_relu_leaky_test_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_relu_leaky\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(i)+\"_relu_leaky_test_start\"], \"test\", time_log[\"t1_\" +str(i)+\"_relu_leaky_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_biploar_step_test_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_biploar_step\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(i)+\"_biploar_step_test_start\"], \"test\", time_log[\"t1_\" +str(i)+\"_biploar_step_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_bipolar_sigmoid_test_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_bipolar_sigmoid\"].predict(type2_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type2_BERT_Embeddings_Test\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_start\"], \"test\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_relu_leaky_test_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_relu_leaky\"].predict(type2_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type2_BERT_Embeddings_Test\", time_log[\"t2_\" +str(i)+\"_relu_leaky_test_start\"], \"test\", time_log[\"t2_\" +str(i)+\"_relu_leaky_test_end\"])\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_biploar_step_test_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_biploar_step\"].predict(type2_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type2_BERT_Embeddings_Test\", time_log[\"t2_\" +str(i)+\"_biploar_step_test_start\"], \"test\", time_log[\"t2_\" +str(i)+\"_biploar_step_test_end\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_metrics_df= pd.DataFrame(metrics_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_metrics_df.to_csv(\"Final_ELM_Mertics_BERT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     HiddenNodes  lit_HamminLosss  lit_accuracy  lit_precision  lit_recall  \\\n",
       "0             50         0.040753      0.074420       0.164234    0.082254   \n",
       "1             50         0.042244      0.072517       0.162059    0.081746   \n",
       "2            100         0.040416      0.086925       0.187507    0.096075   \n",
       "3            100         0.040167      0.096955       0.199020    0.106567   \n",
       "4            100         0.040569      0.076974       0.165178    0.084525   \n",
       "..           ...              ...           ...            ...         ...   \n",
       "153        20000         0.066187      0.144064       0.235657    0.268832   \n",
       "154        20000         0.059534      0.146378       0.251591    0.254838   \n",
       "155        20000         0.061243      0.150862       0.260281    0.263048   \n",
       "156        20000         0.067431      0.151865       0.247488    0.291056   \n",
       "157        20000         0.060089      0.151144       0.259925    0.263257   \n",
       "\n",
       "       lit_f1  sklearn_hamLoss  sklearn_accuracy  sklearn_macro_precision  \\\n",
       "0    0.109611         0.074420          0.025292                 0.057291   \n",
       "1    0.108674         0.072517          0.024275                 0.028466   \n",
       "2    0.127051         0.086925          0.031826                 0.051584   \n",
       "3    0.138808         0.096955          0.038782                 0.092622   \n",
       "4    0.111826         0.076974          0.028981                 0.125406   \n",
       "..        ...              ...               ...                      ...   \n",
       "153  0.251154         0.144064          0.020566                 0.129531   \n",
       "154  0.253204         0.146378          0.019218                 0.126600   \n",
       "155  0.261657         0.150862          0.020904                 0.115964   \n",
       "156  0.267510         0.151865          0.017195                 0.101379   \n",
       "157  0.261581         0.151144          0.019892                 0.112374   \n",
       "\n",
       "     sklearn_micro_precision-  sklearn_macro_recall  sklearn_micro_precision  \\\n",
       "0                    0.074420              0.008273                 0.584176   \n",
       "1                    0.072517              0.008220                 0.556655   \n",
       "2                    0.086925              0.009844                 0.605033   \n",
       "3                    0.096955              0.010989                 0.622420   \n",
       "4                    0.076974              0.008986                 0.605699   \n",
       "..                        ...                   ...                      ...   \n",
       "153                  0.144064              0.076673                 0.234270   \n",
       "154                  0.146378              0.059362                 0.266755   \n",
       "155                  0.150862              0.062819                 0.258507   \n",
       "156                  0.151865              0.077476                 0.234788   \n",
       "157                  0.151144              0.062882                 0.267179   \n",
       "\n",
       "     sklearn_macro_f1  sklearn_micro_f1  term_wise_accuracy        activation  \\\n",
       "0            0.011012          0.125159            0.959247  _bipolar_sigmoid   \n",
       "1            0.010789          0.122163            0.957756  _bipolar_sigmoid   \n",
       "2            0.012870          0.143661            0.959584  _bipolar_sigmoid   \n",
       "3            0.014725          0.152941            0.959833       _relu_leaky   \n",
       "4            0.012675          0.126382            0.959431     _biploar_step   \n",
       "..                ...               ...                 ...               ...   \n",
       "153          0.081326          0.237194            0.933813       _relu_leaky   \n",
       "154          0.065623          0.242798            0.940466     _biploar_step   \n",
       "155          0.067771          0.243356            0.938757  _bipolar_sigmoid   \n",
       "156          0.076517          0.244037            0.932569       _relu_leaky   \n",
       "157          0.068356          0.247771            0.939911     _biploar_step   \n",
       "\n",
       "                            type         start  phase           end  \n",
       "0    type1_BERT_Embeddings_Train  1.603016e+09  train  1.603016e+09  \n",
       "1     type1_BERT_Embeddings_Test  1.603016e+09   test  1.603016e+09  \n",
       "2    type1_BERT_Embeddings_Train  1.603016e+09  train  1.603016e+09  \n",
       "3    type1_BERT_Embeddings_Train  1.603016e+09  train  1.603016e+09  \n",
       "4    type1_BERT_Embeddings_Train  1.603016e+09  train  1.603016e+09  \n",
       "..                           ...           ...    ...           ...  \n",
       "153   type1_BERT_Embeddings_Test  1.603049e+09   test  1.603049e+09  \n",
       "154   type1_BERT_Embeddings_Test  1.603049e+09   test  1.603049e+09  \n",
       "155   type2_BERT_Embeddings_Test  1.603049e+09   test  1.603050e+09  \n",
       "156   type2_BERT_Embeddings_Test  1.603050e+09   test  1.603050e+09  \n",
       "157   type2_BERT_Embeddings_Test  1.603050e+09   test  1.603050e+09  \n",
       "\n",
       "[158 rows x 20 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>HiddenNodes</th>\n      <th>lit_HamminLosss</th>\n      <th>lit_accuracy</th>\n      <th>lit_precision</th>\n      <th>lit_recall</th>\n      <th>lit_f1</th>\n      <th>sklearn_hamLoss</th>\n      <th>sklearn_accuracy</th>\n      <th>sklearn_macro_precision</th>\n      <th>sklearn_micro_precision-</th>\n      <th>sklearn_macro_recall</th>\n      <th>sklearn_micro_precision</th>\n      <th>sklearn_macro_f1</th>\n      <th>sklearn_micro_f1</th>\n      <th>term_wise_accuracy</th>\n      <th>activation</th>\n      <th>type</th>\n      <th>start</th>\n      <th>phase</th>\n      <th>end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50</td>\n      <td>0.040753</td>\n      <td>0.074420</td>\n      <td>0.164234</td>\n      <td>0.082254</td>\n      <td>0.109611</td>\n      <td>0.074420</td>\n      <td>0.025292</td>\n      <td>0.057291</td>\n      <td>0.074420</td>\n      <td>0.008273</td>\n      <td>0.584176</td>\n      <td>0.011012</td>\n      <td>0.125159</td>\n      <td>0.959247</td>\n      <td>_bipolar_sigmoid</td>\n      <td>type1_BERT_Embeddings_Train</td>\n      <td>1.603016e+09</td>\n      <td>train</td>\n      <td>1.603016e+09</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>0.042244</td>\n      <td>0.072517</td>\n      <td>0.162059</td>\n      <td>0.081746</td>\n      <td>0.108674</td>\n      <td>0.072517</td>\n      <td>0.024275</td>\n      <td>0.028466</td>\n      <td>0.072517</td>\n      <td>0.008220</td>\n      <td>0.556655</td>\n      <td>0.010789</td>\n      <td>0.122163</td>\n      <td>0.957756</td>\n      <td>_bipolar_sigmoid</td>\n      <td>type1_BERT_Embeddings_Test</td>\n      <td>1.603016e+09</td>\n      <td>test</td>\n      <td>1.603016e+09</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100</td>\n      <td>0.040416</td>\n      <td>0.086925</td>\n      <td>0.187507</td>\n      <td>0.096075</td>\n      <td>0.127051</td>\n      <td>0.086925</td>\n      <td>0.031826</td>\n      <td>0.051584</td>\n      <td>0.086925</td>\n      <td>0.009844</td>\n      <td>0.605033</td>\n      <td>0.012870</td>\n      <td>0.143661</td>\n      <td>0.959584</td>\n      <td>_bipolar_sigmoid</td>\n      <td>type1_BERT_Embeddings_Train</td>\n      <td>1.603016e+09</td>\n      <td>train</td>\n      <td>1.603016e+09</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100</td>\n      <td>0.040167</td>\n      <td>0.096955</td>\n      <td>0.199020</td>\n      <td>0.106567</td>\n      <td>0.138808</td>\n      <td>0.096955</td>\n      <td>0.038782</td>\n      <td>0.092622</td>\n      <td>0.096955</td>\n      <td>0.010989</td>\n      <td>0.622420</td>\n      <td>0.014725</td>\n      <td>0.152941</td>\n      <td>0.959833</td>\n      <td>_relu_leaky</td>\n      <td>type1_BERT_Embeddings_Train</td>\n      <td>1.603016e+09</td>\n      <td>train</td>\n      <td>1.603016e+09</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100</td>\n      <td>0.040569</td>\n      <td>0.076974</td>\n      <td>0.165178</td>\n      <td>0.084525</td>\n      <td>0.111826</td>\n      <td>0.076974</td>\n      <td>0.028981</td>\n      <td>0.125406</td>\n      <td>0.076974</td>\n      <td>0.008986</td>\n      <td>0.605699</td>\n      <td>0.012675</td>\n      <td>0.126382</td>\n      <td>0.959431</td>\n      <td>_biploar_step</td>\n      <td>type1_BERT_Embeddings_Train</td>\n      <td>1.603016e+09</td>\n      <td>train</td>\n      <td>1.603016e+09</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>20000</td>\n      <td>0.066187</td>\n      <td>0.144064</td>\n      <td>0.235657</td>\n      <td>0.268832</td>\n      <td>0.251154</td>\n      <td>0.144064</td>\n      <td>0.020566</td>\n      <td>0.129531</td>\n      <td>0.144064</td>\n      <td>0.076673</td>\n      <td>0.234270</td>\n      <td>0.081326</td>\n      <td>0.237194</td>\n      <td>0.933813</td>\n      <td>_relu_leaky</td>\n      <td>type1_BERT_Embeddings_Test</td>\n      <td>1.603049e+09</td>\n      <td>test</td>\n      <td>1.603049e+09</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>20000</td>\n      <td>0.059534</td>\n      <td>0.146378</td>\n      <td>0.251591</td>\n      <td>0.254838</td>\n      <td>0.253204</td>\n      <td>0.146378</td>\n      <td>0.019218</td>\n      <td>0.126600</td>\n      <td>0.146378</td>\n      <td>0.059362</td>\n      <td>0.266755</td>\n      <td>0.065623</td>\n      <td>0.242798</td>\n      <td>0.940466</td>\n      <td>_biploar_step</td>\n      <td>type1_BERT_Embeddings_Test</td>\n      <td>1.603049e+09</td>\n      <td>test</td>\n      <td>1.603049e+09</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>20000</td>\n      <td>0.061243</td>\n      <td>0.150862</td>\n      <td>0.260281</td>\n      <td>0.263048</td>\n      <td>0.261657</td>\n      <td>0.150862</td>\n      <td>0.020904</td>\n      <td>0.115964</td>\n      <td>0.150862</td>\n      <td>0.062819</td>\n      <td>0.258507</td>\n      <td>0.067771</td>\n      <td>0.243356</td>\n      <td>0.938757</td>\n      <td>_bipolar_sigmoid</td>\n      <td>type2_BERT_Embeddings_Test</td>\n      <td>1.603049e+09</td>\n      <td>test</td>\n      <td>1.603050e+09</td>\n    </tr>\n    <tr>\n      <th>156</th>\n      <td>20000</td>\n      <td>0.067431</td>\n      <td>0.151865</td>\n      <td>0.247488</td>\n      <td>0.291056</td>\n      <td>0.267510</td>\n      <td>0.151865</td>\n      <td>0.017195</td>\n      <td>0.101379</td>\n      <td>0.151865</td>\n      <td>0.077476</td>\n      <td>0.234788</td>\n      <td>0.076517</td>\n      <td>0.244037</td>\n      <td>0.932569</td>\n      <td>_relu_leaky</td>\n      <td>type2_BERT_Embeddings_Test</td>\n      <td>1.603050e+09</td>\n      <td>test</td>\n      <td>1.603050e+09</td>\n    </tr>\n    <tr>\n      <th>157</th>\n      <td>20000</td>\n      <td>0.060089</td>\n      <td>0.151144</td>\n      <td>0.259925</td>\n      <td>0.263257</td>\n      <td>0.261581</td>\n      <td>0.151144</td>\n      <td>0.019892</td>\n      <td>0.112374</td>\n      <td>0.151144</td>\n      <td>0.062882</td>\n      <td>0.267179</td>\n      <td>0.068356</td>\n      <td>0.247771</td>\n      <td>0.939911</td>\n      <td>_biploar_step</td>\n      <td>type2_BERT_Embeddings_Test</td>\n      <td>1.603050e+09</td>\n      <td>test</td>\n      <td>1.603050e+09</td>\n    </tr>\n  </tbody>\n</table>\n<p>158 rows Ã— 20 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "Final_metrics_df"
   ]
  },
  {
   "source": [
    "Doing Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "display_name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "870dc015ab544cf4fd4c91e2f8042f40253e24aac5eeb5e28336055566efd434"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}