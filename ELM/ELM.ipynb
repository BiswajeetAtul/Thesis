{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "main_start=time.time()\n",
    "time_log={\"main_start\":main_start}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         imdb_id                                          title  \\\n",
       "0      tt0057603                        I tre volti della paura   \n",
       "1      tt1733125  Dungeons & Dragons: The Book of Vile Darkness   \n",
       "2      tt0033045                     The Shop Around the Corner   \n",
       "3      tt0113862                             Mr. Holland's Opus   \n",
       "4      tt0086250                                       Scarface   \n",
       "...          ...                                            ...   \n",
       "14823  tt0219952                                  Lucky Numbers   \n",
       "14824  tt1371159                                     Iron Man 2   \n",
       "14825  tt0063443                                     Play Dirty   \n",
       "14826  tt0039464                                      High Wall   \n",
       "14827  tt0235166                               Against All Hope   \n",
       "\n",
       "                                           plot_synopsis  \\\n",
       "0      Note: this synopsis is for the orginal Italian...   \n",
       "1      Two thousand years ago, Nhagruul the Foul, a s...   \n",
       "2      Matuschek's, a gift store in Budapest, is the ...   \n",
       "3      Glenn Holland, not a morning person by anyone'...   \n",
       "4      In May 1980, a Cuban man named Tony Montana (A...   \n",
       "...                                                  ...   \n",
       "14823  In 1988 Russ Richards (John Travolta), the wea...   \n",
       "14824  In Russia, the media covers Tony Stark's discl...   \n",
       "14825  During the North African Campaign in World War...   \n",
       "14826  Steven Kenet catches his unfaithful wife in th...   \n",
       "14827  Sometime in the 1950s in Chicago a man, Cecil ...   \n",
       "\n",
       "                                                    tags  split  \\\n",
       "0              cult, horror, gothic, murder, atmospheric  train   \n",
       "1                                               violence  train   \n",
       "2                                               romantic   test   \n",
       "3                 inspiring, romantic, stupid, feel-good  train   \n",
       "4      cruelty, murder, dramatic, cult, violence, atm...    val   \n",
       "...                                                  ...    ...   \n",
       "14823                                     comedy, murder   test   \n",
       "14824                         good versus evil, violence  train   \n",
       "14825                                           anti war  train   \n",
       "14826                                             murder   test   \n",
       "14827                                     christian film   test   \n",
       "\n",
       "      synopsis_source  \n",
       "0                imdb  \n",
       "1                imdb  \n",
       "2                imdb  \n",
       "3                imdb  \n",
       "4                imdb  \n",
       "...               ...  \n",
       "14823       wikipedia  \n",
       "14824       wikipedia  \n",
       "14825       wikipedia  \n",
       "14826       wikipedia  \n",
       "14827       wikipedia  \n",
       "\n",
       "[14828 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imdb_id</th>\n      <th>title</th>\n      <th>plot_synopsis</th>\n      <th>tags</th>\n      <th>split</th>\n      <th>synopsis_source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>tt0057603</td>\n      <td>I tre volti della paura</td>\n      <td>Note: this synopsis is for the orginal Italian...</td>\n      <td>cult, horror, gothic, murder, atmospheric</td>\n      <td>train</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tt1733125</td>\n      <td>Dungeons &amp; Dragons: The Book of Vile Darkness</td>\n      <td>Two thousand years ago, Nhagruul the Foul, a s...</td>\n      <td>violence</td>\n      <td>train</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tt0033045</td>\n      <td>The Shop Around the Corner</td>\n      <td>Matuschek's, a gift store in Budapest, is the ...</td>\n      <td>romantic</td>\n      <td>test</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tt0113862</td>\n      <td>Mr. Holland's Opus</td>\n      <td>Glenn Holland, not a morning person by anyone'...</td>\n      <td>inspiring, romantic, stupid, feel-good</td>\n      <td>train</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>tt0086250</td>\n      <td>Scarface</td>\n      <td>In May 1980, a Cuban man named Tony Montana (A...</td>\n      <td>cruelty, murder, dramatic, cult, violence, atm...</td>\n      <td>val</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14823</th>\n      <td>tt0219952</td>\n      <td>Lucky Numbers</td>\n      <td>In 1988 Russ Richards (John Travolta), the wea...</td>\n      <td>comedy, murder</td>\n      <td>test</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14824</th>\n      <td>tt1371159</td>\n      <td>Iron Man 2</td>\n      <td>In Russia, the media covers Tony Stark's discl...</td>\n      <td>good versus evil, violence</td>\n      <td>train</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14825</th>\n      <td>tt0063443</td>\n      <td>Play Dirty</td>\n      <td>During the North African Campaign in World War...</td>\n      <td>anti war</td>\n      <td>train</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14826</th>\n      <td>tt0039464</td>\n      <td>High Wall</td>\n      <td>Steven Kenet catches his unfaithful wife in th...</td>\n      <td>murder</td>\n      <td>test</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14827</th>\n      <td>tt0235166</td>\n      <td>Against All Hope</td>\n      <td>Sometime in the 1950s in Chicago a man, Cecil ...</td>\n      <td>christian film</td>\n      <td>test</td>\n      <td>wikipedia</td>\n    </tr>\n  </tbody>\n</table>\n<p>14828 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "mpstDF= pd.read_csv(\"mpst.csv\")\n",
    "mpstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape Train (9489, 6)\nShape Validation (2373, 6)\nShape Test (2966, 6)\n"
     ]
    }
   ],
   "source": [
    "final_mpstDF_train=mpstDF[mpstDF[\"split\"]==\"train\"]\n",
    "final_mpstDF_validation=mpstDF[mpstDF[\"split\"]==\"val\"]\n",
    "final_mpstDF_test=mpstDF[mpstDF[\"split\"]==\"test\"]\n",
    "print(\"Shape Train\",final_mpstDF_train.shape)\n",
    "print(\"Shape Validation\",final_mpstDF_validation.shape)\n",
    "print(\"Shape Test\",final_mpstDF_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Int64Index([    0,     1,     3,     6,     7,     8,     9,    10,    11,\n",
       "               12,\n",
       "            ...\n",
       "            14810, 14811, 14813, 14815, 14817, 14818, 14820, 14822, 14824,\n",
       "            14825],\n",
       "           dtype='int64', length=9489)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "final_mpstDF_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Int64Index([    4,     5,    18,    20,    26,    32,    40,    48,    53,\n",
       "               65,\n",
       "            ...\n",
       "            14767, 14769, 14772, 14777, 14779, 14789, 14795, 14796, 14812,\n",
       "            14821],\n",
       "           dtype='int64', length=2373)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "final_mpstDF_validation.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Int64Index([    2,    15,    19,    24,    27,    31,    35,    37,    41,\n",
       "               42,\n",
       "            ...\n",
       "            14776, 14801, 14806, 14808, 14814, 14816, 14819, 14823, 14826,\n",
       "            14827],\n",
       "           dtype='int64', length=2966)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "final_mpstDF_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING THE BERT EMBEDDINGS AND Y MATRIX\n",
    "bert_embedding=np.load(\"D:\\CodeRepo\\Thesis\\Thesis\\BERT\\embeddings.npz\")\n",
    "label_values=np.load(\"D:\\CodeRepo\\Thesis\\Thesis\\EDA\\Y.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "type1_BERT_Embeddings=bert_embedding[\"t1\"]\n",
    "type2_BERT_Embeddings=bert_embedding[\"t2\"]\n",
    "label_values=label_values[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(14828, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "type1_BERT_Embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(14828, 71)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "label_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_Embeddings(x_t1,x_t2,y,df,partition_nm):\n",
    "    _df=df[df[\"split\"]==partition_nm]\n",
    "    index_list=list(_df.index)\n",
    "    temp_array_x_t1=[]\n",
    "    temp_array_x_t2=[]\n",
    "    temp_array_y=[]\n",
    "    for index in index_list:\n",
    "        temp_array_x_t1.append(x_t1[index,:])\n",
    "        temp_array_x_t2.append(x_t2[index,:])\n",
    "        temp_array_y.append(y[index,:])\n",
    "    temp_array_x_t1=np.array(temp_array_x_t1)\n",
    "    temp_array_x_t2=np.array(temp_array_x_t2)\n",
    "    temp_array_y=np.array(temp_array_y)\n",
    "    return temp_array_x_t1,temp_array_x_t2, temp_array_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TRAIN\n",
    "type1_BERT_Embeddings_Train,type2_BERT_Embeddings_Train,label_values_Train=get_partition_Embeddings(type1_BERT_Embeddings,type2_BERT_Embeddings,label_values,mpstDF,\"train\")\n",
    "# FOR VALIDATION\n",
    "type1_BERT_Embeddings_Val,type2_BERT_Embeddings_Val,label_values_Val=get_partition_Embeddings(type1_BERT_Embeddings,type2_BERT_Embeddings,label_values,mpstDF,\"val\")\n",
    "# FOR TEST\n",
    "type1_BERT_Embeddings_Test,type2_BERT_Embeddings_Test,label_values_Test=get_partition_Embeddings(type1_BERT_Embeddings,type2_BERT_Embeddings,label_values,mpstDF,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9489, 768)\n(9489, 768)\n(2373, 768)\n(2373, 768)\n(2966, 768)\n(2966, 768)\n(9489, 71)\n(2373, 71)\n(2966, 71)\n"
     ]
    }
   ],
   "source": [
    "# SEPARTAING THE TRAIN TEST AND VALIDATION ROWS\n",
    "print(type1_BERT_Embeddings_Train.shape)\n",
    "print(type2_BERT_Embeddings_Train.shape)\n",
    "print(type1_BERT_Embeddings_Val.shape)\n",
    "print(type2_BERT_Embeddings_Val.shape)\n",
    "print(type1_BERT_Embeddings_Test.shape)\n",
    "print(type2_BERT_Embeddings_Test.shape)\n",
    "print(label_values_Train.shape)\n",
    "print(label_values_Val.shape)\n",
    "print(label_values_Test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix, hamming_loss,classification_report\n",
    "\n",
    "# Activation Functions\n",
    "\n",
    "# def _identity(x):\n",
    "#     return x\n",
    "# def _binary_step(x, threshold = 0):\n",
    "#     return 1 if x=threshold else 0\n",
    "# def _biploar_step(x ,threshold = 0):\n",
    "#     return 1 if x>=threshold else -1\n",
    "# def _binary_sigmoid(x):\n",
    "#     return 1. / (1. + np.exp(-x))\n",
    "# def _bipolar_sigmoid(x):\n",
    "#     return (1. - np.exp(-x))/(1. + np.exp(-x))\n",
    "# def _relu_function(x):\n",
    "#     return np.max(0, x)\n",
    "# def _relu_leaky(x):\n",
    "#     return np.max(0.01*x, x)\n",
    "\n",
    "\n",
    "_identity =np.vectorize(lambda x: x)\n",
    "_binary_step =np.vectorize(lambda x,t=0: 1 if x>t else 0)\n",
    "_biploar_step =np.vectorize(lambda x,t=0: 1 if x>t else -1)\n",
    "_binary_sigmoid=np.vectorize(lambda x: 1. / (1. + np.exp(-x)))\n",
    "_bipolar_sigmoid=np.vectorize(lambda x: (1. - np.exp(-x))/(1. + np.exp(-x)))\n",
    "_relu_function=np.vectorize(lambda x: np.max([0, x]))\n",
    "_relu_leaky=np.vectorize(lambda x: np.max([0.01*x, x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ELM_MultiLabel:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, activation, bias=True, random_gen=\"uniform\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_nodes ([integer]): Number of Input nodes\n",
    "            hidden_nodes ([integer]): Number of hidden nodes\n",
    "            output_nodes ([integer]): Number of output nodes\n",
    "            activation ([function]): The function which will be used as the activation function in the hidden layer\n",
    "            bias ([boolean]): Flag to use bias, if True then randomly generate bias @random_gen else bias - 0.\n",
    "            random_gen (str, optional): The type way in which random weight are generated. Defaults to \"uniform\".\n",
    "        \"\"\"\n",
    "        self.__input_nodes = input_nodes\n",
    "        self.__hidden_nodes = hidden_nodes\n",
    "        self.__output_nodes = output_nodes\n",
    "        if random_gen == \"uniform\":\n",
    "            self.__beta = np.random.uniform(-1.,1.,size = (self.__hidden_nodes, self.__output_nodes))\n",
    "            self.__alpha = np.random.uniform(-1.,1.,size = (self.__input_nodes, self.__hidden_nodes))\n",
    "            self.__bias = np.random.uniform(size = (self.__hidden_nodes,))\n",
    "        else:\n",
    "            self.__beta = np.random.normal(-1.,1.,size=(self.__hidden_nodes, self.__output_nodes))\n",
    "            self.__alpha = np.random.normal(-1.,1.,size=(self.__input_nodes, self.__hidden_nodes))\n",
    "            self.__bias = np.random.normal(size=(self.__n_hidden_nodes,)) \n",
    "        self.__activation = activation # Sigmoid Function\n",
    "\n",
    "    def getInputNodes(self):\n",
    "        return  self.__input_nodes\n",
    "\n",
    "    def getHiddenNodes(self):\n",
    "        return  self.__hidden_nodes\n",
    "\n",
    "    def getOutputNodes(self):\n",
    "        return  self.__output_nodes\n",
    "    \n",
    "    def getBetaWeights(self):\n",
    "        return self.__beta\n",
    "    \n",
    "    def getAlphaWeight(self):\n",
    "        return self.__alphs\n",
    "    \n",
    "    def getBias(self):\n",
    "        return self.__bias\n",
    "\n",
    "    def __get_H_matrix(self, train_x, verbose=False):\n",
    "        # 1 Propagate data from Input to hidden Layer\n",
    "        if verbose:\n",
    "            print(\"Propagate data from Input to hidden Layer\")\n",
    "        inp = np.dot(train_x , self.__alpha)\n",
    "        if verbose:\n",
    "            print(inp)\n",
    "            print(\"Adding Biases\")\n",
    "        inp = inp  + self.__bias\n",
    "        if verbose:\n",
    "            print(inp)\n",
    "            print(\"Applyin activation function\")\n",
    "        inp_activation = np.apply_along_axis(self.__activation, 1, inp)\n",
    "        return inp_activation\n",
    "\n",
    "    def fit(self, train_x, train_y, verbose = False, show_metrics = True):\n",
    "        \"\"\"\n",
    "        This function calculates the Beta weights or the output weights\n",
    "        train_x : input matrix\n",
    "        train_y : output matrix to be predicted or learned upon unipolar\n",
    "\n",
    "        returns: if test_y is not given then\n",
    "                returns the predicted output\n",
    "                if test_y is given then returns predicted output and evaluation metrics dict \n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"train_x shape:\", train_x.shape)\n",
    "            print(\"train_y shape:\", train_y.shape)\n",
    "        inp_activation = self.__get_H_matrix(train_x, verbose)\n",
    "        # This is the H matrix getting its Moore Penrose Inverse\n",
    "        if verbose:\n",
    "            print(inp_activation)\n",
    "            print(\"Getting the Generalized Moore Penrose Inverse\")\n",
    "        generalizedInverse = np.linalg.pinv(inp_activation)\n",
    "        if verbose:\n",
    "            print(generalizedInverse)\n",
    "            print(\"Finding Beta, output weights\")\n",
    "        # Now find output weight matrix Beta \n",
    "        # convert input Y values according to the threshold using biploar step function\n",
    "        predicted_bipolar=  np.apply_along_axis(_biploar_step, 1, train_y)\n",
    "        self.__beta = np.dot(generalizedInverse, predicted_bipolar)\n",
    "        if verbose:\n",
    "            print(\"Beta Matrix Weights\")\n",
    "            print(self.__beta)\n",
    "\n",
    "        print(\"Model Metrics, for Training :\")\n",
    "        return self.predict(train_x, train_y,verbose,show_metrics)\n",
    "    \n",
    "    def predict(self, test_x, test_y = None, verbose = False, show_metrics= True):\n",
    "        \"\"\"\n",
    "        preditcts the output for the input test data\n",
    "        call this after calling the fit.\n",
    "        test_data shape should be (batch_size,768 or input_nodes)\n",
    "        output_shape will be (batch_size, 71 or output_nodes)\n",
    "\n",
    "        returns: if test_y is not given then\n",
    "                returns the predicted output\n",
    "                if test_y is given then returns predicted output and evaluation metrics dict\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"Predicting outputs\")\n",
    "        inp_activation = self.__get_H_matrix(test_x, verbose)\n",
    "        output_predicted = np.dot(inp_activation, self.__beta)\n",
    "        # convert predicted according to the threshold using biploar step function\n",
    "        predicted_bipolar =  np.apply_along_axis(_biploar_step, 1, output_predicted)\n",
    "        predicted_binary = np.apply_along_axis(_binary_step, 1, predicted_bipolar)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"predicted output\")\n",
    "            print(output_predicted)\n",
    "            print(\"predicted_bipolar\")\n",
    "            print(predicted_bipolar)\n",
    "            print(\"predicted_binary\")\n",
    "            print(predicted_binary)\n",
    "            print(\"Original Binary\")\n",
    "            print(test_y)\n",
    "\n",
    "        eval_dict={}\n",
    "        if (test_y is not None):\n",
    "            eval_dict=self.__evaluate(test_y,predicted_binary, for_test=False)\n",
    "        if(test_y is not None):\n",
    "            return predicted_binary, eval_dict\n",
    "        else:\n",
    "            return predicted_binary\n",
    "\n",
    "    def __evaluate(self, real, predicted, for_test=True):\n",
    "        \"\"\"\n",
    "        real values as 0,1\n",
    "        predicted values as 0,1\n",
    "        \"\"\"\n",
    "        # Now we find accuracy, precision, recall, Hamming Loss and F1 Measure\n",
    "        accuracy = accuracy_score(real, predicted)\n",
    "        hamLoss = hamming_loss(real, predicted)\n",
    "        # element wise correctness\n",
    "        term_wise_accuracy=np.sum(np.logical_not(np.logical_xor(real, predicted)))/real.size\n",
    "\n",
    "        macro_precision = precision_score(real, predicted, average='macro')\n",
    "        macro_recall = recall_score(real, predicted, average='macro')\n",
    "        macro_f1 = f1_score(real, predicted, average='macro')\n",
    "\n",
    "        micro_precision = precision_score(real, predicted, average='micro')\n",
    "        micro_recall = recall_score(real, predicted, average='micro')\n",
    "        micro_f1 = f1_score(real, predicted, average='micro')\n",
    "        \n",
    "        metricTable=prettytable.PrettyTable()\n",
    "        metricTable.field_names = [\"Metric\", \"Macro Value\", \"Micro Value\"]\n",
    "        metricTable.add_row([\"Hamming Loss\",\"{0:.3f}\".format(hamLoss) ,\"\"])\n",
    "        metricTable.add_row([\"Term Wise Accuracy\",\"{0:.3f}\".format(term_wise_accuracy) ,\"\"])\n",
    "\n",
    "        metricTable.add_row([\"Accuracy\",\"{0:.3f}\".format(accuracy),\"\"])\n",
    "        metricTable.add_row([\"Precision\",\"{0:.3f}\".format(macro_precision),\"{0:.3f}\".format(micro_precision)])\n",
    "        metricTable.add_row([\"Recall\",\"{0:.3f}\".format(macro_recall),\"{0:.3f}\".format(micro_recall)])\n",
    "        metricTable.add_row([\"F1-measure\",\"{0:.3f}\".format(macro_f1),\"{0:.3f}\".format(micro_f1)])\n",
    "\n",
    "        print(metricTable)\n",
    "\n",
    "        print(\"Metrics @ Literature\")\n",
    "        lit_HamminLosss, lit_accuracy, lit_precision, lit_recall, lit_f1 = self.get_eval_metrics(real,predicted)\n",
    "\n",
    "        return_dict = {\"HiddenNodes\": self.getHiddenNodes(),\n",
    "                \"lit_HamminLosss\": lit_HamminLosss,\n",
    "                \"lit_accuracy\": lit_accuracy,\n",
    "                \"lit_precision\": lit_precision,\n",
    "                \"lit_recall\": lit_recall,\n",
    "                \"lit_f1\": lit_f1,\n",
    "                \"sklearn_hamLoss\": lit_accuracy,\n",
    "                \"sklearn_accuracy\": accuracy,\n",
    "                \"sklearn_macro_precision\": macro_precision,\n",
    "                \"sklearn_micro_precision-\": lit_accuracy,\n",
    "                \"sklearn_macro_recall\": macro_recall,\n",
    "                \"sklearn_micro_precision\": micro_precision,\n",
    "                \"sklearn_macro_f1\": macro_f1,\n",
    "                \"sklearn_micro_f1\": micro_f1,\n",
    "                \"term_wise_accuracy\": term_wise_accuracy,\n",
    "                }\n",
    "\n",
    "\n",
    "        print(\"Test Classification Report\")\n",
    "        print(classification_report(real,predicted))\n",
    "\n",
    "        return return_dict\n",
    "\n",
    "    def get_eval_metrics(self, real, predicted, verbose= False):\n",
    "        err_cnt_accuracy=0\n",
    "        err_cnt_precision=0\n",
    "        err_cnt_recall=0\n",
    "        if verbose:\n",
    "            print(real)\n",
    "            print(predicted)\n",
    "        for x in range(real.shape[0]):\n",
    "            err_and= np.logical_and(real[x],predicted[x])\n",
    "            err_or = np.logical_or(real[x],predicted[x])\n",
    "            # Accuracy\n",
    "            err_cnt_accuracy +=(sum(err_and)/sum(err_or))\n",
    "\n",
    "            # Precision\n",
    "            if sum(err_and) != 0:\n",
    "                err_cnt_precision += (sum(err_and) / sum(predicted[x]))\n",
    "            # Recall\n",
    "            err_cnt_recall += (sum(err_and) / sum(real[x]))\n",
    "            if verbose:\n",
    "                print(\"Iteration :\",x)\n",
    "                print((sum(err_and)/sum(err_or)))\n",
    "                print(err_and)\n",
    "                print(err_or)\n",
    "                print(err_cnt)\n",
    "        \n",
    "        err_count_hamming = np.zeros((real.shape))\n",
    "\n",
    "        for i in range(real.shape[0]):\n",
    "            for j in range(real.shape[1]):\n",
    "                if real[i,j] != predicted[i,j]:\n",
    "                    err_count_hamming[1,j] = err_count_hamming[1,j]+1\n",
    "\n",
    "        sum_err = np.sum(err_count_hamming);\n",
    "        HammingLoss = sum_err/real.size;\n",
    "        accuracy = err_cnt_accuracy / real.shape[0]\n",
    "        precision = err_cnt_precision / real.shape[0]\n",
    "        recall = err_cnt_recall / real.shape[0]\n",
    "        f1 = 2*((precision*recall)/(precision+recall))\n",
    "        if verbose:\n",
    "            print(\"Final: \")\n",
    "            print(\"Hamming Loss: \", HammingLoss)\n",
    "            print(\"Accuracy: \",accuracy)\n",
    "            print(\"precision: \",precision)\n",
    "            print(\"recall: \",recall)\n",
    "            print(\"f1: \",f1)\n",
    "\n",
    "        metricTable=prettytable.PrettyTable()\n",
    "        metricTable.field_names = [\"Metric\", \"Value\"]\n",
    "        metricTable.add_row([\" Literature Hamming Loss\",\"{0:.3f}\".format(HammingLoss)])\n",
    "        metricTable.add_row([\"Literature Accuracy\",\"{0:.3f}\".format(accuracy)])\n",
    "\n",
    "        metricTable.add_row([\"Literature Precision\",\"{0:.3f}\".format(precision)])\n",
    "        metricTable.add_row([\"LiteratureRecall\",\"{0:.3f}\".format(recall)])\n",
    "        metricTable.add_row([\"LiteratureF1-measure\",\"{0:.3f}\".format(f1)])\n",
    "\n",
    "        print(metricTable)\n",
    "\n",
    "        return HammingLoss,accuracy,precision,recall,f1\n"
   ]
  },
  {
   "source": [
    "Now The preprocessing and is done. We will save the time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a object to store the mertics for each model created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_log[\"End preprocessing\"]=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAINING \n",
      "\n",
      "Training Model with 100 hidden nodes\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.041    |             |\n",
      "| Term Wise Accuracy |    0.959    |             |\n",
      "|      Accuracy      |    0.024    |             |\n",
      "|     Precision      |    0.059    |    0.593    |\n",
      "|       Recall       |    0.008    |    0.065    |\n",
      "|     F1-measure     |    0.010    |    0.117    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.041 |\n",
      "|   Literature Accuracy    | 0.069 |\n",
      "|   Literature Precision   | 0.152 |\n",
      "|     LiteratureRecall     | 0.076 |\n",
      "|   LiteratureF1-measure   | 0.101 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.00      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       0.00      0.00      0.00       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       1.00      0.00      0.00      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.59      0.36      0.45      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       0.50      0.00      0.00      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.00      0.00      0.00      1556\n",
      "          57       0.50      0.00      0.00      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       1.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.59      0.17      0.27      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.59      0.06      0.12     28022\n",
      "   macro avg       0.06      0.01      0.01     28022\n",
      "weighted avg       0.28      0.06      0.09     28022\n",
      " samples avg       0.15      0.08      0.09     28022\n",
      "\n",
      "TESTING \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.043    |             |\n",
      "| Term Wise Accuracy |    0.957    |             |\n",
      "|      Accuracy      |    0.029    |             |\n",
      "|     Precision      |    0.044    |    0.506    |\n",
      "|       Recall       |    0.007    |    0.058    |\n",
      "|     F1-measure     |    0.009    |    0.105    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.043 |\n",
      "|   Literature Accuracy    | 0.066 |\n",
      "|   Literature Precision   | 0.146 |\n",
      "|     LiteratureRecall     | 0.072 |\n",
      "|   LiteratureF1-measure   | 0.097 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       1.00      0.01      0.01       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.56      0.35      0.43      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.50      0.00      0.01       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00       507\n",
      "          57       0.60      0.01      0.01       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.48      0.13      0.21       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.51      0.06      0.10      9022\n",
      "   macro avg       0.04      0.01      0.01      9022\n",
      "weighted avg       0.20      0.06      0.08      9022\n",
      " samples avg       0.15      0.07      0.08      9022\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]),\n",
       " {'HiddenNodes': 50,\n",
       "  'lit_HamminLosss': 0.04278062169374982,\n",
       "  'lit_accuracy': 0.06589435665260705,\n",
       "  'lit_precision': 0.1461663559557336,\n",
       "  'lit_recall': 0.072372643833762,\n",
       "  'lit_f1': 0.09681059792762586,\n",
       "  'sklearn_hamLoss': 0.06589435665260705,\n",
       "  'sklearn_accuracy': 0.02899527983816588,\n",
       "  'sklearn_macro_precision': 0.04424431646584347,\n",
       "  'sklearn_micro_precision-': 0.06589435665260705,\n",
       "  'sklearn_macro_recall': 0.0069424332243630815,\n",
       "  'sklearn_micro_precision': 0.5062439961575408,\n",
       "  'sklearn_macro_f1': 0.009328514704477566,\n",
       "  'sklearn_micro_f1': 0.10474013713604294,\n",
       "  'term_wise_accuracy': 0.9572193783062501})"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# doing a test for ELM model with 100 nodes\n",
    "#TRAIN\n",
    "print(\"TRAINING \\n\")\n",
    "t1_emlMLTC_50= ELM_MultiLabel(input_nodes=768, hidden_nodes=50, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 100 hidden nodes\")\n",
    "t1_emlMLTC_50.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "\n",
    "#TEST\n",
    "print(\"TESTING \\n\")\n",
    "t1_emlMLTC_50.predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n"
   ]
  },
  {
   "source": [
    "Now We will run the model for all the three types data sets we have viz.\n",
    "- TRAIN X\n",
    "  - type1_BERT_Embeddings_Train\n",
    "  - type2_BERT_Embeddings_Train\n",
    "- VALIDATION X\n",
    "  - type1_BERT_Embeddings_Val\n",
    "  - type2_BERT_Embeddings_Val\n",
    "- TEST X\n",
    "  - type1_BERT_Embeddings_Test\n",
    "  - type2_BERT_Embeddings_Test\n",
    "- TRAIN Y\n",
    "  - label_values_Train.shape\n",
    "- VALIDATION Y\n",
    "  - label_values_Val.shape\n",
    "- TEST Y\n",
    "  - label_values_Test.shape"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models_hidden_nodes=[100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000, 10000, 15000, 20000]\n",
    "t1_models={}\n",
    "t2_models={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_list=[]\n",
    "\n",
    "def add_data_to_metric_list(eval_dict, activation, type, start, phase, end, metrics_dict_list=metrics_dict_list):\n",
    "    eval_dict[\"activation\"]=activation\n",
    "    eval_dict[\"type\"]=type\n",
    "    eval_dict[\"start\"]=start\n",
    "    eval_dict[\"phase\"]=phase\n",
    "    eval_dict[\"end\"]=end\n",
    "\n",
    "    metrics_dict_list.append(eval_dict)\n"
   ]
  },
  {
   "source": [
    "Testing the above function with a simple 50 hidden layer node model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t1_50_bipolar_sigmoid_train_start \n",
      "\n",
      "Model Metrics, for Training :\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.041    |             |\n",
      "| Term Wise Accuracy |    0.959    |             |\n",
      "|      Accuracy      |    0.023    |             |\n",
      "|     Precision      |    0.067    |    0.591    |\n",
      "|       Recall       |    0.007    |    0.061    |\n",
      "|     F1-measure     |    0.010    |    0.110    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.041 |\n",
      "|   Literature Accuracy    | 0.065 |\n",
      "|   Literature Precision   | 0.148 |\n",
      "|     LiteratureRecall     | 0.071 |\n",
      "|   LiteratureF1-measure   | 0.096 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       169\n",
      "           1       0.00      0.00      0.00       423\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.00      0.00      0.00        93\n",
      "           4       0.00      0.00      0.00        64\n",
      "           5       0.00      0.00      0.00       129\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.00      0.00      0.00       264\n",
      "           8       0.00      0.00      0.00        28\n",
      "           9       0.00      0.00      0.00       158\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00       139\n",
      "          12       0.00      0.00      0.00       330\n",
      "          13       0.00      0.00      0.00        77\n",
      "          14       0.00      0.00      0.00        27\n",
      "          15       0.00      0.00      0.00        52\n",
      "          16       0.00      0.00      0.00        59\n",
      "          17       0.00      0.00      0.00      1194\n",
      "          18       0.00      0.00      0.00        63\n",
      "          19       0.00      0.00      0.00       282\n",
      "          20       0.00      0.00      0.00      1680\n",
      "          21       0.00      0.00      0.00       116\n",
      "          22       0.00      0.00      0.00       257\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.00      0.00      0.00       254\n",
      "          25       0.00      0.00      0.00       450\n",
      "          26       1.00      0.00      0.01       351\n",
      "          27       0.00      0.00      0.00        48\n",
      "          28       0.00      0.00      0.00      1849\n",
      "          29       0.00      0.00      0.00       548\n",
      "          30       0.00      0.00      0.00       285\n",
      "          31       0.00      0.00      0.00        44\n",
      "          32       0.00      0.00      0.00        99\n",
      "          33       0.00      0.00      0.00       171\n",
      "          34       0.00      0.00      0.00       102\n",
      "          35       0.00      0.00      0.00        97\n",
      "          36       0.00      0.00      0.00       304\n",
      "          37       0.00      0.00      0.00       527\n",
      "          38       0.00      0.00      0.00       413\n",
      "          39       0.00      0.00      0.00        68\n",
      "          40       0.00      0.00      0.00       106\n",
      "          41       0.00      0.00      0.00        35\n",
      "          42       0.00      0.00      0.00       298\n",
      "          43       0.60      0.35      0.44      3697\n",
      "          44       0.00      0.00      0.00       322\n",
      "          45       0.00      0.00      0.00       464\n",
      "          46       0.00      0.00      0.00        25\n",
      "          47       0.00      0.00      0.00       330\n",
      "          48       0.00      0.00      0.00       154\n",
      "          49       0.00      0.00      0.00       123\n",
      "          50       0.00      0.00      0.00       104\n",
      "          51       0.00      0.00      0.00       169\n",
      "          52       1.00      0.00      0.00      1184\n",
      "          53       0.00      0.00      0.00       176\n",
      "          54       0.00      0.00      0.00        64\n",
      "          55       0.00      0.00      0.00       140\n",
      "          56       0.60      0.00      0.00      1556\n",
      "          57       1.00      0.00      0.00      1850\n",
      "          58       0.00      0.00      0.00       392\n",
      "          59       0.00      0.00      0.00       532\n",
      "          60       0.00      0.00      0.00       197\n",
      "          61       0.00      0.00      0.00       156\n",
      "          62       0.00      0.00      0.00       233\n",
      "          63       0.00      0.00      0.00       111\n",
      "          64       0.00      0.00      0.00        28\n",
      "          65       0.00      0.00      0.00       661\n",
      "          66       0.00      0.00      0.00        80\n",
      "          67       0.00      0.00      0.00       370\n",
      "          68       0.57      0.15      0.23      2813\n",
      "          69       0.00      0.00      0.00        49\n",
      "          70       0.00      0.00      0.00        55\n",
      "\n",
      "   micro avg       0.59      0.06      0.11     28022\n",
      "   macro avg       0.07      0.01      0.01     28022\n",
      "weighted avg       0.29      0.06      0.08     28022\n",
      " samples avg       0.15      0.07      0.08     28022\n",
      "\n",
      "t1_50_bipolar_sigmoid_test_start \n",
      "\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.042    |             |\n",
      "| Term Wise Accuracy |    0.958    |             |\n",
      "|      Accuracy      |    0.025    |             |\n",
      "|     Precision      |    0.043    |    0.552    |\n",
      "|       Recall       |    0.007    |    0.058    |\n",
      "|     F1-measure     |    0.009    |    0.105    |\n",
      "+--------------------+-------------+-------------+\n",
      "Metrics @ Literature\n",
      "+--------------------------+-------+\n",
      "|          Metric          | Value |\n",
      "+--------------------------+-------+\n",
      "|  Literature Hamming Loss | 0.042 |\n",
      "|   Literature Accuracy    | 0.063 |\n",
      "|   Literature Precision   | 0.142 |\n",
      "|     LiteratureRecall     | 0.069 |\n",
      "|   LiteratureF1-measure   | 0.093 |\n",
      "+--------------------------+-------+\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        56\n",
      "           1       0.00      0.00      0.00       129\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        79\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.00      0.00      0.00        45\n",
      "          10       0.00      0.00      0.00        11\n",
      "          11       0.00      0.00      0.00        40\n",
      "          12       0.00      0.00      0.00       115\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00       368\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        97\n",
      "          20       0.00      0.00      0.00       551\n",
      "          21       0.00      0.00      0.00        41\n",
      "          22       0.00      0.00      0.00        85\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.00      0.00      0.00        83\n",
      "          25       0.00      0.00      0.00       159\n",
      "          26       0.00      0.00      0.00       113\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00       596\n",
      "          29       0.00      0.00      0.00       190\n",
      "          30       0.00      0.00      0.00        83\n",
      "          31       0.00      0.00      0.00        12\n",
      "          32       0.00      0.00      0.00        26\n",
      "          33       0.00      0.00      0.00        64\n",
      "          34       0.00      0.00      0.00        25\n",
      "          35       0.00      0.00      0.00        29\n",
      "          36       0.00      0.00      0.00        92\n",
      "          37       0.00      0.00      0.00       172\n",
      "          38       0.00      0.00      0.00       136\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        40\n",
      "          41       0.00      0.00      0.00         5\n",
      "          42       0.00      0.00      0.00        93\n",
      "          43       0.57      0.34      0.43      1155\n",
      "          44       0.00      0.00      0.00       117\n",
      "          45       0.00      0.00      0.00       145\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.00      0.00      0.00       129\n",
      "          48       0.00      0.00      0.00        36\n",
      "          49       0.00      0.00      0.00        44\n",
      "          50       0.00      0.00      0.00        28\n",
      "          51       0.00      0.00      0.00        51\n",
      "          52       0.00      0.00      0.00       395\n",
      "          53       0.00      0.00      0.00        65\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       1.00      0.00      0.00       507\n",
      "          57       1.00      0.00      0.00       587\n",
      "          58       0.00      0.00      0.00       148\n",
      "          59       0.00      0.00      0.00       173\n",
      "          60       0.00      0.00      0.00        66\n",
      "          61       0.00      0.00      0.00        51\n",
      "          62       0.00      0.00      0.00        66\n",
      "          63       0.00      0.00      0.00        39\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00       228\n",
      "          66       0.00      0.00      0.00        27\n",
      "          67       0.00      0.00      0.00       119\n",
      "          68       0.49      0.14      0.22       911\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.55      0.06      0.10      9022\n",
      "   macro avg       0.04      0.01      0.01      9022\n",
      "weighted avg       0.24      0.06      0.08      9022\n",
      " samples avg       0.14      0.07      0.08      9022\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'HiddenNodes': 50,\n",
       "  'lit_HamminLosss': 0.040816720324052015,\n",
       "  'lit_accuracy': 0.06517243102961386,\n",
       "  'lit_precision': 0.14781150103628765,\n",
       "  'lit_recall': 0.07125021350638472,\n",
       "  'lit_f1': 0.09615190887664843,\n",
       "  'sklearn_hamLoss': 0.06517243102961386,\n",
       "  'sklearn_accuracy': 0.023079355042681,\n",
       "  'sklearn_macro_precision': 0.06718106591129645,\n",
       "  'sklearn_micro_precision-': 0.06517243102961386,\n",
       "  'sklearn_macro_recall': 0.007055354298003807,\n",
       "  'sklearn_micro_precision': 0.5905784551437478,\n",
       "  'sklearn_macro_f1': 0.009658794470472982,\n",
       "  'sklearn_micro_f1': 0.11032385389368793,\n",
       "  'term_wise_accuracy': 0.959183279675948,\n",
       "  'activation': '_bipolar_sigmoid',\n",
       "  'type': 'type1_BERT_Embeddings_Train',\n",
       "  'start': 1603015170.3740754,\n",
       "  'phase': 'train',\n",
       "  'end': 1603015180.1090422},\n",
       " {'HiddenNodes': 50,\n",
       "  'lit_HamminLosss': 0.04237223747067706,\n",
       "  'lit_accuracy': 0.06261162443434075,\n",
       "  'lit_precision': 0.14227916385704653,\n",
       "  'lit_recall': 0.06872040757447923,\n",
       "  'lit_f1': 0.0926777439714883,\n",
       "  'sklearn_hamLoss': 0.06261162443434075,\n",
       "  'sklearn_accuracy': 0.024612272420768713,\n",
       "  'sklearn_macro_precision': 0.04321333803939541,\n",
       "  'sklearn_micro_precision-': 0.06261162443434075,\n",
       "  'sklearn_macro_recall': 0.006810913283398186,\n",
       "  'sklearn_micro_precision': 0.5523809523809524,\n",
       "  'sklearn_macro_f1': 0.009192850502792355,\n",
       "  'sklearn_micro_f1': 0.1047456606802448,\n",
       "  'term_wise_accuracy': 0.9576277625293229,\n",
       "  'activation': '_bipolar_sigmoid',\n",
       "  'type': 'type1_BERT_Embeddings_Train',\n",
       "  'start': 1603015180.1090422,\n",
       "  'phase': 'train',\n",
       "  'end': 1603015182.6632113}]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "print( \"t1_\"   +str(50)+\"_bipolar_sigmoid_train_start \\n\")\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_start\"]=time.time()\n",
    "t1_models[\"t1_\"+str(50)+\"_bipolar_sigmoid\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=50, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "predicted, eval_dict=t1_models[\"t1_\"+str(50)+\"_bipolar_sigmoid\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_end\"]=time.time()\n",
    "\n",
    "add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_start\"], \"train\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_end\"])\n",
    "\n",
    "print( \"t1_\"   +str(50)+\"_bipolar_sigmoid_test_start \\n\")\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_start\"]=time.time()\n",
    "predicted, eval_dict=t1_models[\"t1_\"+str(50)+\"_bipolar_sigmoid\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_end\"]=time.time()\n",
    "\n",
    "add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_start\"], \"test\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_end\"])\n",
    "\n",
    "metrics_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   HiddenNodes  lit_HamminLosss  lit_accuracy  lit_precision  lit_recall  \\\n",
       "0           50         0.040817      0.065172       0.147812     0.07125   \n",
       "1           50         0.042372      0.062612       0.142279     0.06872   \n",
       "\n",
       "     lit_f1  sklearn_hamLoss  sklearn_accuracy  sklearn_macro_precision  \\\n",
       "0  0.096152         0.065172          0.023079                 0.067181   \n",
       "1  0.092678         0.062612          0.024612                 0.043213   \n",
       "\n",
       "   sklearn_micro_precision-  sklearn_macro_recall  sklearn_micro_precision  \\\n",
       "0                  0.065172              0.007055                 0.590578   \n",
       "1                  0.062612              0.006811                 0.552381   \n",
       "\n",
       "   sklearn_macro_f1  sklearn_micro_f1  term_wise_accuracy        activation  \\\n",
       "0          0.009659          0.110324            0.959183  _bipolar_sigmoid   \n",
       "1          0.009193          0.104746            0.957628  _bipolar_sigmoid   \n",
       "\n",
       "                          type         start  phase           end  \n",
       "0  type1_BERT_Embeddings_Train  1.603015e+09  train  1.603015e+09  \n",
       "1  type1_BERT_Embeddings_Train  1.603015e+09  train  1.603015e+09  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>HiddenNodes</th>\n      <th>lit_HamminLosss</th>\n      <th>lit_accuracy</th>\n      <th>lit_precision</th>\n      <th>lit_recall</th>\n      <th>lit_f1</th>\n      <th>sklearn_hamLoss</th>\n      <th>sklearn_accuracy</th>\n      <th>sklearn_macro_precision</th>\n      <th>sklearn_micro_precision-</th>\n      <th>sklearn_macro_recall</th>\n      <th>sklearn_micro_precision</th>\n      <th>sklearn_macro_f1</th>\n      <th>sklearn_micro_f1</th>\n      <th>term_wise_accuracy</th>\n      <th>activation</th>\n      <th>type</th>\n      <th>start</th>\n      <th>phase</th>\n      <th>end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50</td>\n      <td>0.040817</td>\n      <td>0.065172</td>\n      <td>0.147812</td>\n      <td>0.07125</td>\n      <td>0.096152</td>\n      <td>0.065172</td>\n      <td>0.023079</td>\n      <td>0.067181</td>\n      <td>0.065172</td>\n      <td>0.007055</td>\n      <td>0.590578</td>\n      <td>0.009659</td>\n      <td>0.110324</td>\n      <td>0.959183</td>\n      <td>_bipolar_sigmoid</td>\n      <td>type1_BERT_Embeddings_Train</td>\n      <td>1.603015e+09</td>\n      <td>train</td>\n      <td>1.603015e+09</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>0.042372</td>\n      <td>0.062612</td>\n      <td>0.142279</td>\n      <td>0.06872</td>\n      <td>0.092678</td>\n      <td>0.062612</td>\n      <td>0.024612</td>\n      <td>0.043213</td>\n      <td>0.062612</td>\n      <td>0.006811</td>\n      <td>0.552381</td>\n      <td>0.009193</td>\n      <td>0.104746</td>\n      <td>0.957628</td>\n      <td>_bipolar_sigmoid</td>\n      <td>type1_BERT_Embeddings_Train</td>\n      <td>1.603015e+09</td>\n      <td>train</td>\n      <td>1.603015e+09</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "pd.DataFrame([{'HiddenNodes': 50,\n",
    "  'lit_HamminLosss': 0.040816720324052015,\n",
    "  'lit_accuracy': 0.06517243102961386,\n",
    "  'lit_precision': 0.14781150103628765,\n",
    "  'lit_recall': 0.07125021350638472,\n",
    "  'lit_f1': 0.09615190887664843,\n",
    "  'sklearn_hamLoss': 0.06517243102961386,\n",
    "  'sklearn_accuracy': 0.023079355042681,\n",
    "  'sklearn_macro_precision': 0.06718106591129645,\n",
    "  'sklearn_micro_precision-': 0.06517243102961386,\n",
    "  'sklearn_macro_recall': 0.007055354298003807,\n",
    "  'sklearn_micro_precision': 0.5905784551437478,\n",
    "  'sklearn_macro_f1': 0.009658794470472982,\n",
    "  'sklearn_micro_f1': 0.11032385389368793,\n",
    "  'term_wise_accuracy': 0.959183279675948,\n",
    "  'activation': '_bipolar_sigmoid',\n",
    "  'type': 'type1_BERT_Embeddings_Train',\n",
    "  'start': 1603015170.3740754,\n",
    "  'phase': 'train',\n",
    "  'end': 1603015180.1090422},\n",
    " {'HiddenNodes': 50,\n",
    "  'lit_HamminLosss': 0.04237223747067706,\n",
    "  'lit_accuracy': 0.06261162443434075,\n",
    "  'lit_precision': 0.14227916385704653,\n",
    "  'lit_recall': 0.06872040757447923,\n",
    "  'lit_f1': 0.0926777439714883,\n",
    "  'sklearn_hamLoss': 0.06261162443434075,\n",
    "  'sklearn_accuracy': 0.024612272420768713,\n",
    "  'sklearn_macro_precision': 0.04321333803939541,\n",
    "  'sklearn_micro_precision-': 0.06261162443434075,\n",
    "  'sklearn_macro_recall': 0.006810913283398186,\n",
    "  'sklearn_micro_precision': 0.5523809523809524,\n",
    "  'sklearn_macro_f1': 0.009192850502792355,\n",
    "  'sklearn_micro_f1': 0.1047456606802448,\n",
    "  'term_wise_accuracy': 0.9576277625293229,\n",
    "  'activation': '_bipolar_sigmoid',\n",
    "  'type': 'type1_BERT_Embeddings_Train',\n",
    "  'start': 1603015180.1090422,\n",
    "  'phase': 'train',\n",
    "  'end': 1603015182.6632113}])"
   ]
  },
  {
   "source": [
    "TRAINING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_of_models_hidden_nodes:\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_bipolar_sigmoid_train_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_start\"]=time.time()\n",
    "    t1_models[\"t1_\"+str(i)+\"_bipolar_sigmoid\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_bipolar_sigmoid\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_start\"], \"train\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_end\"])\n",
    "\n",
    "    \n",
    "    print( \"t1_\"   +str(i)+\"_relu_leaky_train_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_train_start\"]=time.time()\n",
    "    t1_models[\"t1_\"+str(i)+\"_relu_leaky\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_relu_leaky)\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_relu_leaky\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(i)+\"_relu_leaky_train_start\"], \"train\", time_log[\"t1_\" +str(i)+\"_relu_leaky_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_biploar_step_train_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_train_start\"]=time.time()\n",
    "    t1_models[\"t1_\"+str(i)+\"_biploar_step\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_biploar_step)\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_biploar_step\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(i)+\"_biploar_step_train_start\"], \"train\", time_log[\"t1_\" +str(i)+\"_biploar_step_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_bipolar_sigmoid_train_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_start\"]=time.time()\n",
    "    t2_models[\"t2_\"+str(i)+\"_bipolar_sigmoid\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_bipolar_sigmoid\"].fit(type2_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type2_BERT_Embeddings_Train\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_start\"], \"train\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_relu_leaky_train_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_train_start\"]=time.time()\n",
    "    t2_models[\"t2_\"+str(i)+\"_relu_leaky\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_relu_leaky)\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_relu_leaky\"].fit(type2_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type2_BERT_Embeddings_Train\", time_log[\"t2_\" +str(i)+\"_relu_leaky_train_start\"], \"train\", time_log[\"t2_\" +str(i)+\"_relu_leaky_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_biploar_step_train_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_train_start\"]=time.time()\n",
    "    t2_models[\"t2_\"+str(i)+\"_biploar_step\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_biploar_step)\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_biploar_step\"].fit(type2_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type2_BERT_Embeddings_Train\", time_log[\"t2_\" +str(i)+\"_biploar_step_train_start\"], \"train\", time_log[\"t2_\" +str(i)+\"_biploar_step_train_end\"])\n",
    "\n"
   ]
  },
  {
   "source": [
    "TESTING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_of_models_hidden_nodes:\n",
    "    print( \"t1_\"   +str(i)+\"_bipolar_sigmoid_test_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_bipolar_sigmoid\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_start\"], \"test\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_relu_leaky_test_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_relu_leaky\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(i)+\"_relu_leaky_test_start\"], \"test\", time_log[\"t1_\" +str(i)+\"_relu_leaky_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_biploar_step_test_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_biploar_step\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(i)+\"_biploar_step_test_start\"], \"test\", time_log[\"t1_\" +str(i)+\"_biploar_step_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_bipolar_sigmoid_test_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_bipolar_sigmoid\"].predict(type2_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type2_BERT_Embeddings_Test\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_start\"], \"test\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_relu_leaky_test_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_relu_leaky\"].predict(type2_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type2_BERT_Embeddings_Test\", time_log[\"t2_\" +str(i)+\"_relu_leaky_test_start\"], \"test\", time_log[\"t2_\" +str(i)+\"_relu_leaky_test_end\"])\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_biploar_step_test_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_biploar_step\"].predict(type2_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type2_BERT_Embeddings_Test\", time_log[\"t2_\" +str(i)+\"_biploar_step_test_start\"], \"test\", time_log[\"t2_\" +str(i)+\"_biploar_step_test_end\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_500= ELM_MultiLabel(input_nodes=768, hidden_nodes=500, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 500 hidden nodes\")\n",
    "t1_emlMLTC_500.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_1000= ELM_MultiLabel(input_nodes=768, hidden_nodes=1000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 1000 hidden nodes\")\n",
    "t1_emlMLTC_1000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_2000= ELM_MultiLabel(input_nodes=768, hidden_nodes=2000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 2000 hidden nodes\")\n",
    "t1_emlMLTC_2000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_3000= ELM_MultiLabel(input_nodes=768, hidden_nodes=3000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 3000 hidden nodes\")\n",
    "t1_emlMLTC_3000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_4000= ELM_MultiLabel(input_nodes=768, hidden_nodes=4000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 4000 hidden nodes\")\n",
    "t1_emlMLTC_4000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_5000= ELM_MultiLabel(input_nodes=768, hidden_nodes=5000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 5000 hidden nodes\")\n",
    "t1_emlMLTC_5000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_10000= ELM_MultiLabel(input_nodes=768, hidden_nodes=10000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 10000 hidden nodes\")\n",
    "t1_emlMLTC_10000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_15000= ELM_MultiLabel(input_nodes=768, hidden_nodes=15000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 15000 hidden nodes\")\n",
    "t1_emlMLTC_15000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_emlMLTC_20000=ELM_MultiLabel(input_nodes=768, hidden_nodes=20000, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 20000 hidden nodes\")\n",
    "t1_emlMLTC_20000.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_list.append(time.time())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "source": [
    "Doing Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_model_list={\"t1_emlMLTC_500\":t1_emlMLTC_500,\n",
    "\"t1_emlMLTC_1000\":t1_emlMLTC_1000,\n",
    "\"t1_emlMLTC_2000\":t1_emlMLTC_2000,\n",
    "\"t1_emlMLTC_3000\":t1_emlMLTC_3000,\n",
    "\"t1_emlMLTC_4000\":t1_emlMLTC_4000,\n",
    "\"t1_emlMLTC_5000\":t1_emlMLTC_5000,\n",
    "\"t1_emlMLTC_10000\":t1_emlMLTC_10000,\n",
    "\"t1_emlMLTC_15000\":t1_emlMLTC_15000,\n",
    "\"t1_emlMLTC_20000\":t1_emlMLTC_20000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_train=time.time()\n",
    "time_list_test=[start_time_train]\n",
    "\n",
    "for x in t1_model_list.keys():\n",
    "    print(\"For \"+ x)\n",
    "    t1_model_list[x].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_list_test.append(time.time())\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "display_name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "870dc015ab544cf4fd4c91e2f8042f40253e24aac5eeb5e28336055566efd434"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}