{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "main_start=time.time()\n",
    "time_log={\"main_start\":main_start}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         imdb_id                                          title  \\\n",
       "0      tt0057603                        I tre volti della paura   \n",
       "1      tt1733125  Dungeons & Dragons: The Book of Vile Darkness   \n",
       "2      tt0033045                     The Shop Around the Corner   \n",
       "3      tt0113862                             Mr. Holland's Opus   \n",
       "4      tt0086250                                       Scarface   \n",
       "...          ...                                            ...   \n",
       "14823  tt0219952                                  Lucky Numbers   \n",
       "14824  tt1371159                                     Iron Man 2   \n",
       "14825  tt0063443                                     Play Dirty   \n",
       "14826  tt0039464                                      High Wall   \n",
       "14827  tt0235166                               Against All Hope   \n",
       "\n",
       "                                           plot_synopsis  \\\n",
       "0      Note: this synopsis is for the orginal Italian...   \n",
       "1      Two thousand years ago, Nhagruul the Foul, a s...   \n",
       "2      Matuschek's, a gift store in Budapest, is the ...   \n",
       "3      Glenn Holland, not a morning person by anyone'...   \n",
       "4      In May 1980, a Cuban man named Tony Montana (A...   \n",
       "...                                                  ...   \n",
       "14823  In 1988 Russ Richards (John Travolta), the wea...   \n",
       "14824  In Russia, the media covers Tony Stark's discl...   \n",
       "14825  During the North African Campaign in World War...   \n",
       "14826  Steven Kenet catches his unfaithful wife in th...   \n",
       "14827  Sometime in the 1950s in Chicago a man, Cecil ...   \n",
       "\n",
       "                                                    tags  split  \\\n",
       "0              cult, horror, gothic, murder, atmospheric  train   \n",
       "1                                               violence  train   \n",
       "2                                               romantic   test   \n",
       "3                 inspiring, romantic, stupid, feel-good  train   \n",
       "4      cruelty, murder, dramatic, cult, violence, atm...    val   \n",
       "...                                                  ...    ...   \n",
       "14823                                     comedy, murder   test   \n",
       "14824                         good versus evil, violence  train   \n",
       "14825                                           anti war  train   \n",
       "14826                                             murder   test   \n",
       "14827                                     christian film   test   \n",
       "\n",
       "      synopsis_source  \n",
       "0                imdb  \n",
       "1                imdb  \n",
       "2                imdb  \n",
       "3                imdb  \n",
       "4                imdb  \n",
       "...               ...  \n",
       "14823       wikipedia  \n",
       "14824       wikipedia  \n",
       "14825       wikipedia  \n",
       "14826       wikipedia  \n",
       "14827       wikipedia  \n",
       "\n",
       "[14828 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imdb_id</th>\n      <th>title</th>\n      <th>plot_synopsis</th>\n      <th>tags</th>\n      <th>split</th>\n      <th>synopsis_source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>tt0057603</td>\n      <td>I tre volti della paura</td>\n      <td>Note: this synopsis is for the orginal Italian...</td>\n      <td>cult, horror, gothic, murder, atmospheric</td>\n      <td>train</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tt1733125</td>\n      <td>Dungeons &amp; Dragons: The Book of Vile Darkness</td>\n      <td>Two thousand years ago, Nhagruul the Foul, a s...</td>\n      <td>violence</td>\n      <td>train</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tt0033045</td>\n      <td>The Shop Around the Corner</td>\n      <td>Matuschek's, a gift store in Budapest, is the ...</td>\n      <td>romantic</td>\n      <td>test</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tt0113862</td>\n      <td>Mr. Holland's Opus</td>\n      <td>Glenn Holland, not a morning person by anyone'...</td>\n      <td>inspiring, romantic, stupid, feel-good</td>\n      <td>train</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>tt0086250</td>\n      <td>Scarface</td>\n      <td>In May 1980, a Cuban man named Tony Montana (A...</td>\n      <td>cruelty, murder, dramatic, cult, violence, atm...</td>\n      <td>val</td>\n      <td>imdb</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14823</th>\n      <td>tt0219952</td>\n      <td>Lucky Numbers</td>\n      <td>In 1988 Russ Richards (John Travolta), the wea...</td>\n      <td>comedy, murder</td>\n      <td>test</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14824</th>\n      <td>tt1371159</td>\n      <td>Iron Man 2</td>\n      <td>In Russia, the media covers Tony Stark's discl...</td>\n      <td>good versus evil, violence</td>\n      <td>train</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14825</th>\n      <td>tt0063443</td>\n      <td>Play Dirty</td>\n      <td>During the North African Campaign in World War...</td>\n      <td>anti war</td>\n      <td>train</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14826</th>\n      <td>tt0039464</td>\n      <td>High Wall</td>\n      <td>Steven Kenet catches his unfaithful wife in th...</td>\n      <td>murder</td>\n      <td>test</td>\n      <td>wikipedia</td>\n    </tr>\n    <tr>\n      <th>14827</th>\n      <td>tt0235166</td>\n      <td>Against All Hope</td>\n      <td>Sometime in the 1950s in Chicago a man, Cecil ...</td>\n      <td>christian film</td>\n      <td>test</td>\n      <td>wikipedia</td>\n    </tr>\n  </tbody>\n</table>\n<p>14828 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "mpstDF= pd.read_csv(\"mpst.csv\")\n",
    "mpstDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape Train (9489, 6)\nShape Validation (2373, 6)\nShape Test (2966, 6)\n"
     ]
    }
   ],
   "source": [
    "final_mpstDF_train=mpstDF[mpstDF[\"split\"]==\"train\"]\n",
    "final_mpstDF_validation=mpstDF[mpstDF[\"split\"]==\"val\"]\n",
    "final_mpstDF_test=mpstDF[mpstDF[\"split\"]==\"test\"]\n",
    "print(\"Shape Train\",final_mpstDF_train.shape)\n",
    "print(\"Shape Validation\",final_mpstDF_validation.shape)\n",
    "print(\"Shape Test\",final_mpstDF_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Int64Index([    0,     1,     3,     6,     7,     8,     9,    10,    11,\n",
       "               12,\n",
       "            ...\n",
       "            14810, 14811, 14813, 14815, 14817, 14818, 14820, 14822, 14824,\n",
       "            14825],\n",
       "           dtype='int64', length=9489)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "final_mpstDF_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Int64Index([    4,     5,    18,    20,    26,    32,    40,    48,    53,\n",
       "               65,\n",
       "            ...\n",
       "            14767, 14769, 14772, 14777, 14779, 14789, 14795, 14796, 14812,\n",
       "            14821],\n",
       "           dtype='int64', length=2373)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "final_mpstDF_validation.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Int64Index([    2,    15,    19,    24,    27,    31,    35,    37,    41,\n",
       "               42,\n",
       "            ...\n",
       "            14776, 14801, 14806, 14808, 14814, 14816, 14819, 14823, 14826,\n",
       "            14827],\n",
       "           dtype='int64', length=2966)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "final_mpstDF_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING THE BERT EMBEDDINGS AND Y MATRIX\n",
    "xl_embedding=np.load(r\"D:\\CodeRepo\\Thesis\\Thesis\\XLNet\\xl_embeddings.npz\")\n",
    "label_values=np.load(r\"D:\\CodeRepo\\Thesis\\Thesis\\EDA\\Y.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "type1_XL_Embeddings=xl_embedding[\"t1\"]\n",
    "type2_XL_Embeddings=xl_embedding[\"t2\"]\n",
    "label_values=label_values[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(14828, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "type1_XL_Embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(14828, 71)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "label_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_Embeddings(x_t1,x_t2,y,df,partition_nm):\n",
    "    _df=df[df[\"split\"]==partition_nm]\n",
    "    index_list=list(_df.index)\n",
    "    temp_array_x_t1=[]\n",
    "    temp_array_x_t2=[]\n",
    "    temp_array_y=[]\n",
    "    for index in index_list:\n",
    "        temp_array_x_t1.append(x_t1[index,:])\n",
    "        temp_array_x_t2.append(x_t2[index,:])\n",
    "        temp_array_y.append(y[index,:])\n",
    "    temp_array_x_t1=np.array(temp_array_x_t1)\n",
    "    temp_array_x_t2=np.array(temp_array_x_t2)\n",
    "    temp_array_y=np.array(temp_array_y)\n",
    "    return temp_array_x_t1,temp_array_x_t2, temp_array_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TRAIN\n",
    "type1_XL_Embeddings_Train,type2_XL_Embeddings_Train,label_values_Train=get_partition_Embeddings(type1_XL_Embeddings,type2_XL_Embeddings,label_values,mpstDF,\"train\")\n",
    "# FOR VALIDATION\n",
    "type1_XL_Embeddings_Val,type2_XL_Embeddings_Val,label_values_Val=get_partition_Embeddings(type1_XL_Embeddings,type2_XL_Embeddings,label_values,mpstDF,\"val\")\n",
    "# FOR TEST\n",
    "type1_XL_Embeddings_Test,type2_XL_Embeddings_Test,label_values_Test=get_partition_Embeddings(type1_XL_Embeddings,type2_XL_Embeddings,label_values,mpstDF,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9489, 768)\n(9489, 768)\n(2373, 768)\n(2373, 768)\n(2966, 768)\n(2966, 768)\n(9489, 71)\n(2373, 71)\n(2966, 71)\n"
     ]
    }
   ],
   "source": [
    "# SEPARTAING THE TRAIN TEST AND VALIDATION ROWS\n",
    "print(type1_XL_Embeddings_Train.shape)\n",
    "print(type2_XL_Embeddings_Train.shape)\n",
    "print(type1_XL_Embeddings_Val.shape)\n",
    "print(type2_XL_Embeddings_Val.shape)\n",
    "print(type1_XL_Embeddings_Test.shape)\n",
    "print(type2_XL_Embeddings_Test.shape)\n",
    "print(label_values_Train.shape)\n",
    "print(label_values_Val.shape)\n",
    "print(label_values_Test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix, hamming_loss,classification_report\n",
    "\n",
    "# Activation Functions\n",
    "\n",
    "# def _identity(x):\n",
    "#     return x\n",
    "# def _binary_step(x, threshold = 0):\n",
    "#     return 1 if x=threshold else 0\n",
    "# def _biploar_step(x ,threshold = 0):\n",
    "#     return 1 if x>=threshold else -1\n",
    "# def _binary_sigmoid(x):\n",
    "#     return 1. / (1. + np.exp(-x))\n",
    "# def _bipolar_sigmoid(x):\n",
    "#     return (1. - np.exp(-x))/(1. + np.exp(-x))\n",
    "# def _relu_function(x):\n",
    "#     return np.max(0, x)\n",
    "# def _relu_leaky(x):\n",
    "#     return np.max(0.01*x, x)\n",
    "\n",
    "\n",
    "_identity =np.vectorize(lambda x: x)\n",
    "_binary_step =np.vectorize(lambda x,t=0: 1 if x>t else 0)\n",
    "_biploar_step =np.vectorize(lambda x,t=0: 1 if x>t else -1)\n",
    "_binary_sigmoid=np.vectorize(lambda x: 1. / (1. + np.exp(-x)))\n",
    "_bipolar_sigmoid=np.vectorize(lambda x: (1. - np.exp(-x))/(1. + np.exp(-x)))\n",
    "_relu_function=np.vectorize(lambda x: np.max([0, x]))\n",
    "_relu_leaky=np.vectorize(lambda x: np.max([0.01*x, x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ELM_MultiLabel:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, activation, bias=True, random_gen=\"uniform\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_nodes ([integer]): Number of Input nodes\n",
    "            hidden_nodes ([integer]): Number of hidden nodes\n",
    "            output_nodes ([integer]): Number of output nodes\n",
    "            activation ([function]): The function which will be used as the activation function in the hidden layer\n",
    "            bias ([boolean]): Flag to use bias, if True then randomly generate bias @random_gen else bias - 0.\n",
    "            random_gen (str, optional): The type way in which random weight are generated. Defaults to \"uniform\".\n",
    "        \"\"\"\n",
    "        self.__input_nodes = input_nodes\n",
    "        self.__hidden_nodes = hidden_nodes\n",
    "        self.__output_nodes = output_nodes\n",
    "        if random_gen == \"uniform\":\n",
    "            self.__beta = np.random.uniform(-1.,1.,size = (self.__hidden_nodes, self.__output_nodes))\n",
    "            self.__alpha = np.random.uniform(-1.,1.,size = (self.__input_nodes, self.__hidden_nodes))\n",
    "            self.__bias = np.random.uniform(size = (self.__hidden_nodes,))\n",
    "        else:\n",
    "            self.__beta = np.random.normal(-1.,1.,size=(self.__hidden_nodes, self.__output_nodes))\n",
    "            self.__alpha = np.random.normal(-1.,1.,size=(self.__input_nodes, self.__hidden_nodes))\n",
    "            self.__bias = np.random.normal(size=(self.__n_hidden_nodes,)) \n",
    "        self.__activation = activation # Sigmoid Function\n",
    "\n",
    "    def getInputNodes(self):\n",
    "        return  self.__input_nodes\n",
    "\n",
    "    def getHiddenNodes(self):\n",
    "        return  self.__hidden_nodes\n",
    "\n",
    "    def getOutputNodes(self):\n",
    "        return  self.__output_nodes\n",
    "    \n",
    "    def getBetaWeights(self):\n",
    "        return self.__beta\n",
    "    \n",
    "    def getAlphaWeight(self):\n",
    "        return self.__alphs\n",
    "    \n",
    "    def getBias(self):\n",
    "        return self.__bias\n",
    "\n",
    "    def __get_H_matrix(self, train_x, verbose=False):\n",
    "        # 1 Propagate data from Input to hidden Layer\n",
    "        if verbose:\n",
    "            print(\"Propagate data from Input to hidden Layer\")\n",
    "        inp = np.dot(train_x , self.__alpha)\n",
    "        if verbose:\n",
    "            print(inp)\n",
    "            print(\"Adding Biases\")\n",
    "        inp = inp  + self.__bias\n",
    "        if verbose:\n",
    "            print(inp)\n",
    "            print(\"Applyin activation function\")\n",
    "        inp_activation = np.apply_along_axis(self.__activation, 1, inp)\n",
    "        return inp_activation\n",
    "\n",
    "    def fit(self, train_x, train_y, verbose = False, show_metrics = True):\n",
    "        \"\"\"\n",
    "        This function calculates the Beta weights or the output weights\n",
    "        train_x : input matrix\n",
    "        train_y : output matrix to be predicted or learned upon unipolar\n",
    "\n",
    "        returns: if test_y is not given then\n",
    "                returns the predicted output\n",
    "                if test_y is given then returns predicted output and evaluation metrics dict \n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"train_x shape:\", train_x.shape)\n",
    "            print(\"train_y shape:\", train_y.shape)\n",
    "        inp_activation = self.__get_H_matrix(train_x, verbose)\n",
    "        # This is the H matrix getting its Moore Penrose Inverse\n",
    "        if verbose:\n",
    "            print(inp_activation)\n",
    "            print(\"Getting the Generalized Moore Penrose Inverse\")\n",
    "        generalizedInverse = np.linalg.pinv(inp_activation)\n",
    "        if verbose:\n",
    "            print(generalizedInverse)\n",
    "            print(\"Finding Beta, output weights\")\n",
    "        # Now find output weight matrix Beta \n",
    "        # convert input Y values according to the threshold using biploar step function\n",
    "        predicted_bipolar=  np.apply_along_axis(_biploar_step, 1, train_y)\n",
    "        self.__beta = np.dot(generalizedInverse, predicted_bipolar)\n",
    "        if verbose:\n",
    "            print(\"Beta Matrix Weights\")\n",
    "            print(self.__beta)\n",
    "\n",
    "        print(\"Model Metrics, for Training :\")\n",
    "        return self.predict(train_x, train_y,verbose,show_metrics)\n",
    "    \n",
    "    def predict(self, test_x, test_y = None, verbose = False, show_metrics= True):\n",
    "        \"\"\"\n",
    "        preditcts the output for the input test data\n",
    "        call this after calling the fit.\n",
    "        test_data shape should be (batch_size,768 or input_nodes)\n",
    "        output_shape will be (batch_size, 71 or output_nodes)\n",
    "\n",
    "        returns: if test_y is not given then\n",
    "                returns the predicted output\n",
    "                if test_y is given then returns predicted output and evaluation metrics dict\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"Predicting outputs\")\n",
    "        inp_activation = self.__get_H_matrix(test_x, verbose)\n",
    "        output_predicted = np.dot(inp_activation, self.__beta)\n",
    "        # convert predicted according to the threshold using biploar step function\n",
    "        predicted_bipolar =  np.apply_along_axis(_biploar_step, 1, output_predicted)\n",
    "        predicted_binary = np.apply_along_axis(_binary_step, 1, predicted_bipolar)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"predicted output\")\n",
    "            print(output_predicted)\n",
    "            print(\"predicted_bipolar\")\n",
    "            print(predicted_bipolar)\n",
    "            print(\"predicted_binary\")\n",
    "            print(predicted_binary)\n",
    "            print(\"Original Binary\")\n",
    "            print(test_y)\n",
    "\n",
    "        eval_dict={}\n",
    "        if (test_y is not None):\n",
    "            eval_dict=self.__evaluate(test_y,predicted_binary, for_test=False)\n",
    "        if(test_y is not None):\n",
    "            return predicted_binary, eval_dict\n",
    "        else:\n",
    "            return predicted_binary\n",
    "\n",
    "    def __evaluate(self, real, predicted, for_test=True):\n",
    "        \"\"\"\n",
    "        real values as 0,1\n",
    "        predicted values as 0,1\n",
    "        \"\"\"\n",
    "        # Now we find accuracy, precision, recall, Hamming Loss and F1 Measure\n",
    "        accuracy = accuracy_score(real, predicted)\n",
    "        hamLoss = hamming_loss(real, predicted)\n",
    "        # element wise correctness\n",
    "        term_wise_accuracy=np.sum(np.logical_not(np.logical_xor(real, predicted)))/real.size\n",
    "\n",
    "        macro_precision = precision_score(real, predicted, average='macro')\n",
    "        macro_recall = recall_score(real, predicted, average='macro')\n",
    "        macro_f1 = f1_score(real, predicted, average='macro')\n",
    "\n",
    "        micro_precision = precision_score(real, predicted, average='micro')\n",
    "        micro_recall = recall_score(real, predicted, average='micro')\n",
    "        micro_f1 = f1_score(real, predicted, average='micro')\n",
    "        \n",
    "        metricTable=prettytable.PrettyTable()\n",
    "        metricTable.field_names = [\"Metric\", \"Macro Value\", \"Micro Value\"]\n",
    "        metricTable.add_row([\"Hamming Loss\",\"{0:.3f}\".format(hamLoss) ,\"\"])\n",
    "        metricTable.add_row([\"Term Wise Accuracy\",\"{0:.3f}\".format(term_wise_accuracy) ,\"\"])\n",
    "\n",
    "        metricTable.add_row([\"Accuracy\",\"{0:.3f}\".format(accuracy),\"\"])\n",
    "        metricTable.add_row([\"Precision\",\"{0:.3f}\".format(macro_precision),\"{0:.3f}\".format(micro_precision)])\n",
    "        metricTable.add_row([\"Recall\",\"{0:.3f}\".format(macro_recall),\"{0:.3f}\".format(micro_recall)])\n",
    "        metricTable.add_row([\"F1-measure\",\"{0:.3f}\".format(macro_f1),\"{0:.3f}\".format(micro_f1)])\n",
    "\n",
    "        print(metricTable)\n",
    "\n",
    "        print(\"Metrics @ Literature\")\n",
    "        lit_HamminLosss, lit_accuracy, lit_precision, lit_recall, lit_f1 = self.get_eval_metrics(real,predicted)\n",
    "\n",
    "        return_dict = {\"HiddenNodes\": self.getHiddenNodes(),\n",
    "                \"lit_HamminLosss\": lit_HamminLosss,\n",
    "                \"lit_accuracy\": lit_accuracy,\n",
    "                \"lit_precision\": lit_precision,\n",
    "                \"lit_recall\": lit_recall,\n",
    "                \"lit_f1\": lit_f1,\n",
    "                \"sklearn_hamLoss\": lit_accuracy,\n",
    "                \"sklearn_accuracy\": accuracy,\n",
    "                \"sklearn_macro_precision\": macro_precision,\n",
    "                \"sklearn_micro_precision-\": lit_accuracy,\n",
    "                \"sklearn_macro_recall\": macro_recall,\n",
    "                \"sklearn_micro_precision\": micro_precision,\n",
    "                \"sklearn_macro_f1\": macro_f1,\n",
    "                \"sklearn_micro_f1\": micro_f1,\n",
    "                \"term_wise_accuracy\": term_wise_accuracy,\n",
    "                }\n",
    "\n",
    "\n",
    "        print(\"Test Classification Report\")\n",
    "        print(classification_report(real,predicted))\n",
    "\n",
    "        return return_dict\n",
    "\n",
    "    def get_eval_metrics(self, real, predicted, verbose= False):\n",
    "        err_cnt_accuracy=0\n",
    "        err_cnt_precision=0\n",
    "        err_cnt_recall=0\n",
    "        if verbose:\n",
    "            print(real)\n",
    "            print(predicted)\n",
    "        for x in range(real.shape[0]):\n",
    "            err_and= np.logical_and(real[x],predicted[x])\n",
    "            err_or = np.logical_or(real[x],predicted[x])\n",
    "            # Accuracy\n",
    "            err_cnt_accuracy +=(sum(err_and)/sum(err_or))\n",
    "\n",
    "            # Precision\n",
    "            if sum(err_and) != 0:\n",
    "                err_cnt_precision += (sum(err_and) / sum(predicted[x]))\n",
    "            # Recall\n",
    "            err_cnt_recall += (sum(err_and) / sum(real[x]))\n",
    "            if verbose:\n",
    "                print(\"Iteration :\",x)\n",
    "                print((sum(err_and)/sum(err_or)))\n",
    "                print(err_and)\n",
    "                print(err_or)\n",
    "        \n",
    "        err_count_hamming = np.zeros((real.shape))\n",
    "\n",
    "        for i in range(real.shape[0]):\n",
    "            for j in range(real.shape[1]):\n",
    "                if real[i,j] != predicted[i,j]:\n",
    "                    err_count_hamming[1,j] = err_count_hamming[1,j]+1\n",
    "\n",
    "        sum_err = np.sum(err_count_hamming);\n",
    "        HammingLoss = sum_err/real.size;\n",
    "        accuracy = err_cnt_accuracy / real.shape[0]\n",
    "        precision = err_cnt_precision / real.shape[0]\n",
    "        recall = err_cnt_recall / real.shape[0]\n",
    "        f1 = 2*((precision*recall)/(precision+recall))\n",
    "        if verbose:\n",
    "            print(\"Final: \")\n",
    "            print(\"Hamming Loss: \", HammingLoss)\n",
    "            print(\"Accuracy: \",accuracy)\n",
    "            print(\"precision: \",precision)\n",
    "            print(\"recall: \",recall)\n",
    "            print(\"f1: \",f1)\n",
    "\n",
    "        metricTable=prettytable.PrettyTable()\n",
    "        metricTable.field_names = [\"Metric\", \"Value\"]\n",
    "        metricTable.add_row([\" Literature Hamming Loss\",\"{0:.3f}\".format(HammingLoss)])\n",
    "        metricTable.add_row([\"Literature Accuracy\",\"{0:.3f}\".format(accuracy)])\n",
    "\n",
    "        metricTable.add_row([\"Literature Precision\",\"{0:.3f}\".format(precision)])\n",
    "        metricTable.add_row([\"LiteratureRecall\",\"{0:.3f}\".format(recall)])\n",
    "        metricTable.add_row([\"LiteratureF1-measure\",\"{0:.3f}\".format(f1)])\n",
    "\n",
    "        print(metricTable)\n",
    "\n",
    "        return HammingLoss,accuracy,precision,recall,f1\n"
   ]
  },
  {
   "source": [
    "Now The preprocessing and is done. We will save the time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a object to store the mertics for each model created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_log[\"End preprocessing\"]=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing a test for ELM model with 100 nodes\n",
    "#TRAIN\n",
    "print(\"TRAINING \\n\")\n",
    "t1_emlMLTC_50= ELM_MultiLabel(input_nodes=768, hidden_nodes=50, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 100 hidden nodes\")\n",
    "t1_emlMLTC_50.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "\n",
    "#TEST\n",
    "print(\"TESTING \\n\")\n",
    "t1_emlMLTC_50.predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n"
   ]
  },
  {
   "source": [
    "Now We will run the model for all the three types data sets we have viz.\n",
    "- TRAIN X\n",
    "  - type1_BERT_Embeddings_Train\n",
    "  - type2_BERT_Embeddings_Train\n",
    "- VALIDATION X\n",
    "  - type1_BERT_Embeddings_Val\n",
    "  - type2_BERT_Embeddings_Val\n",
    "- TEST X\n",
    "  - type1_BERT_Embeddings_Test\n",
    "  - type2_BERT_Embeddings_Test\n",
    "- TRAIN Y\n",
    "  - label_values_Train.shape\n",
    "- VALIDATION Y\n",
    "  - label_values_Val.shape\n",
    "- TEST Y\n",
    "  - label_values_Test.shape"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models_hidden_nodes=[100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000, 10000, 15000, 20000]\n",
    "t1_models={}\n",
    "t2_models={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_list=[]\n",
    "\n",
    "def add_data_to_metric_list(eval_dict, activation, type, start, phase, end, metrics_dict_list=metrics_dict_list):\n",
    "    eval_dict[\"activation\"]=activation\n",
    "    eval_dict[\"type\"]=type\n",
    "    eval_dict[\"start\"]=start\n",
    "    eval_dict[\"phase\"]=phase\n",
    "    eval_dict[\"end\"]=end\n",
    "\n",
    "    metrics_dict_list.append(eval_dict)\n"
   ]
  },
  {
   "source": [
    "Testing the above function with a simple 50 hidden layer node model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"t1_\"   +str(50)+\"_bipolar_sigmoid_train_start \\n\")\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_start\"]=time.time()\n",
    "t1_models[\"t1_\"+str(50)+\"_bipolar_sigmoid\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=50, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "predicted, eval_dict=t1_models[\"t1_\"+str(50)+\"_bipolar_sigmoid\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_end\"]=time.time()\n",
    "\n",
    "add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_start\"], \"train\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_train_end\"])\n",
    "\n",
    "print( \"t1_\"   +str(50)+\"_bipolar_sigmoid_test_start \\n\")\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_start\"]=time.time()\n",
    "predicted, eval_dict=t1_models[\"t1_\"+str(50)+\"_bipolar_sigmoid\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_end\"]=time.time()\n",
    "\n",
    "add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_start\"], \"test\", time_log[\"t1_\" +str(50)+\"_bipolar_sigmoid_test_end\"])\n",
    "\n",
    "metrics_dict_list"
   ]
  },
  {
   "source": [
    "TRAINING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_of_models_hidden_nodes:\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_bipolar_sigmoid_train_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_start\"]=time.time()\n",
    "    t1_models[\"t1_\"+str(i)+\"_bipolar_sigmoid\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_bipolar_sigmoid\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_start\"], \"train\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_train_end\"])\n",
    "\n",
    "    \n",
    "    print( \"t1_\"   +str(i)+\"_relu_leaky_train_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_train_start\"]=time.time()\n",
    "    t1_models[\"t1_\"+str(i)+\"_relu_leaky\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_relu_leaky)\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_relu_leaky\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(i)+\"_relu_leaky_train_start\"], \"train\", time_log[\"t1_\" +str(i)+\"_relu_leaky_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_biploar_step_train_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_train_start\"]=time.time()\n",
    "    t1_models[\"t1_\"+str(i)+\"_biploar_step\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_biploar_step)\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_biploar_step\"].fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type1_BERT_Embeddings_Train\", time_log[\"t1_\" +str(i)+\"_biploar_step_train_start\"], \"train\", time_log[\"t1_\" +str(i)+\"_biploar_step_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_bipolar_sigmoid_train_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_start\"]=time.time()\n",
    "    t2_models[\"t2_\"+str(i)+\"_bipolar_sigmoid\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_bipolar_sigmoid\"].fit(type2_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type2_BERT_Embeddings_Train\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_start\"], \"train\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_relu_leaky_train_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_train_start\"]=time.time()\n",
    "    t2_models[\"t2_\"+str(i)+\"_relu_leaky\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_relu_leaky)\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_relu_leaky\"].fit(type2_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type2_BERT_Embeddings_Train\", time_log[\"t2_\" +str(i)+\"_relu_leaky_train_start\"], \"train\", time_log[\"t2_\" +str(i)+\"_relu_leaky_train_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_biploar_step_train_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_train_start\"]=time.time()\n",
    "    t2_models[\"t2_\"+str(i)+\"_biploar_step\"]=ELM_MultiLabel(input_nodes=768, hidden_nodes=i, output_nodes=71, activation=_biploar_step)\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_biploar_step\"].fit(type2_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_train_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type2_BERT_Embeddings_Train\", time_log[\"t2_\" +str(i)+\"_biploar_step_train_start\"], \"train\", time_log[\"t2_\" +str(i)+\"_biploar_step_train_end\"])\n",
    "\n"
   ]
  },
  {
   "source": [
    "TESTING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_of_models_hidden_nodes:\n",
    "    print( \"t1_\"   +str(i)+\"_bipolar_sigmoid_test_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_bipolar_sigmoid\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_start\"], \"test\", time_log[\"t1_\" +str(i)+\"_bipolar_sigmoid_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_relu_leaky_test_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_relu_leaky\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_relu_leaky_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(i)+\"_relu_leaky_test_start\"], \"test\", time_log[\"t1_\" +str(i)+\"_relu_leaky_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t1_\"   +str(i)+\"_biploar_step_test_start \\n\")\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t1_models[\"t1_\"+str(i)+\"_biploar_step\"].predict(type1_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t1_\" +str(i)+\"_biploar_step_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type1_BERT_Embeddings_Test\", time_log[\"t1_\" +str(i)+\"_biploar_step_test_start\"], \"test\", time_log[\"t1_\" +str(i)+\"_biploar_step_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_bipolar_sigmoid_test_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_bipolar_sigmoid\"].predict(type2_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_bipolar_sigmoid\", \"type2_BERT_Embeddings_Test\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_start\"], \"test\", time_log[\"t2_\" +str(i)+\"_bipolar_sigmoid_test_end\"])\n",
    "\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_relu_leaky_test_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_relu_leaky\"].predict(type2_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_relu_leaky_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_relu_leaky\", \"type2_BERT_Embeddings_Test\", time_log[\"t2_\" +str(i)+\"_relu_leaky_test_start\"], \"test\", time_log[\"t2_\" +str(i)+\"_relu_leaky_test_end\"])\n",
    "\n",
    "    print( \"t2_\"   +str(i)+\"_biploar_step_test_start \\n\")\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_test_start\"]=time.time()\n",
    "    predicted, eval_dict=t2_models[\"t2_\"+str(i)+\"_biploar_step\"].predict(type2_BERT_Embeddings_Test,label_values_Test, show_metrics=True)\n",
    "    time_log[\"t2_\" +str(i)+\"_biploar_step_test_end\"]=time.time()\n",
    "\n",
    "    add_data_to_metric_list(eval_dict, \"_biploar_step\", \"type2_BERT_Embeddings_Test\", time_log[\"t2_\" +str(i)+\"_biploar_step_test_start\"], \"test\", time_log[\"t2_\" +str(i)+\"_biploar_step_test_end\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_metrics_df= pd.DataFrame(metrics_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_metrics_df.to_csv(\"Final_ELM_Mertics_BERT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_metrics_df"
   ]
  },
  {
   "source": [
    "Doing Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "display_name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "870dc015ab544cf4fd4c91e2f8042f40253e24aac5eeb5e28336055566efd434"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}