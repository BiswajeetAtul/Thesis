{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "display_name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "870dc015ab544cf4fd4c91e2f8042f40253e24aac5eeb5e28336055566efd434"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&#39;1.14.0&#39;"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tensorflow.python.client.session.Session at 0x231794e5c08&gt;"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[22. 28.]\n [49. 64.]]\n"
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[name: &quot;/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 16095579395700543135\n, name: &quot;/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 6620742943\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13759873787467052866\nphysical_device_desc: &quot;device: 0, name: GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5&quot;\n]\n"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf.test.is_gpu_available()\n",
    "assert tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\n"
    }
   ],
   "source": [
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type Tensor which has no callable conjugate method",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: &#39;Tensor&#39; object has no attribute &#39;conjugate&#39;",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m&lt;ipython-input-20-e8d807468e78&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----&gt; 8\u001b[1;33m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m&lt;__array_function__ internals&gt;\u001b[0m in \u001b[0;36mpinv\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36mpinv\u001b[1;34m(a, rcond, hermitian)\u001b[0m\n\u001b[0;32m   2000\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-&gt; 2002\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2003\u001b[0m     \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhermitian\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhermitian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type Tensor which has no callable conjugate method"
     ]
    }
   ],
   "source": [
    "# import tensorflow_probability as tfp\n",
    "\n",
    "a = tf.constant([[1.,  0.4,  0.5],\n",
    "                 [0.4, 0.2,  0.25],\n",
    "                 [0.5, 0.25, 0.35]])\n",
    "import numpy as np\n",
    "a= np.array(a)\n",
    "b= np.linalg.pinv(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedding=np.load(\"D:\\CodeRepo\\Thesis\\Thesis\\BERT\\embeddings.npz\")\n",
    "y_df=np.load(\"D:\\CodeRepo\\Thesis\\Thesis\\EDA\\Y.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.23652047, -0.46730292,  0.62800884, ..., -0.3478354 ,\n         0.17314139, -0.6017166 ],\n       [ 0.2007698 , -0.9118703 ,  0.74586886, ..., -0.7119921 ,\n         0.29034543, -0.35151947],\n       [-0.54077786,  0.09937867,  0.61585516, ..., -0.38611546,\n         0.10635425, -0.26011148],\n       ...,\n       [-0.23731072, -0.38172564,  0.07030658, ...,  0.16959587,\n         0.52745634, -1.0168482 ],\n       [-0.3484214 , -0.721707  ,  0.16562451, ..., -0.51939166,\n         0.4619452 , -0.33824992],\n       [-0.44174305, -0.1110604 , -0.01783825, ..., -0.34670058,\n         0.34106705, -0.15129046]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "bert_embedding[\"t1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_Embeddings(x,y,df,partition_nm):\n",
    "    _df=df[df[\"split\"]=partition_nm]\n",
    "    index_list=list(_df.index)\n",
    "    temp_array_x=[]\n",
    "    temp_array_y=[]\n",
    "    for index in index_list:\n",
    "        temp_array_x.append(x[index,:])\n",
    "        temp_array_y.append(y[index,:])\n",
    "    temp_array_x=np.array(temp_array_x)\n",
    "    temp_array_y=np.array(temp_array_y)\n",
    "    return temp_array_x, temp_array_y\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 0.2007698  -0.9118703   0.74586886 -0.8697525  -0.57226115 -0.24949355\n -0.04359428 -0.17300647  1.0280551   0.33911157]\n[-0.54077786  0.09937867  0.61585516 -0.631934   -0.55172604  0.0163905\n -0.522085   -0.47525892  0.859443   -0.25996763]\n[-0.4701435  -0.18448775 -0.00838217 -0.9471479  -0.27859327 -0.7233698\n -0.2403428  -0.6327968   0.8770632   0.26553336]\n[-5.5013138e-01 -8.5874611e-01  3.0033991e-02 -5.6631851e-01\n -5.5838817e-01 -6.2689040e-04 -3.1155422e-01 -3.2103887e-01\n  5.6258190e-01  1.1212194e+00]\n[-0.30193123 -0.585884    0.6640047  -0.27994165 -0.27713826 -0.44309044\n -0.6207534  -0.35149023  0.7461389   0.33850378]\n[-0.5745434  -0.6234135   0.1759385  -0.98341745 -0.03513606 -0.2809695\n -0.1693242  -0.28055024 -0.24569269  0.5604806 ]\n[-0.0532444  -0.46782023  0.06806639 -0.23257655 -0.47505942 -0.5427201\n -0.45661578 -0.08468816  0.9627716   0.14629103]\n[ 0.20652439 -0.17011566  0.24326749 -0.9179479  -0.15629724 -0.69250315\n -0.28887078 -0.05260727  1.0344032  -0.17581171]\n[-0.33900252 -0.24636663 -0.22881962 -0.7638544  -0.23487115 -0.5385209\n -0.21819317 -0.42786595  0.6932757   0.9533187 ]\n[[ 0.2007698  -0.9118703   0.74586886 ... -0.7119921   0.29034543\n  -0.35151947]\n [-0.54077786  0.09937867  0.61585516 ... -0.38611546  0.10635425\n  -0.26011148]\n [-0.4701435  -0.18448775 -0.00838217 ... -0.00518429  0.3251386\n  -0.48868352]\n ...\n [-0.0532444  -0.46782023  0.06806639 ...  0.78616977  0.3842592\n  -0.2311075 ]\n [ 0.20652439 -0.17011566  0.24326749 ...  0.00439614  0.35426617\n  -0.37493023]\n [-0.33900252 -0.24636663 -0.22881962 ...  0.0215063   0.2354717\n  -0.45751822]]\n(9, 768)\n"
    }
   ],
   "source": [
    "bert_embedding[\"t1\"][0,:]\n",
    "rows=[1,2,3,6,9,10,34,67,89]\n",
    "test_array=[]\n",
    "for x in rows:\n",
    "    test_array.append(bert_embedding[\"t1\"][x,:])\n",
    "    print(bert_embedding[\"t1\"][x,:10])\n",
    "test_array=np.array(test_array)\n",
    "print(test_array)\n",
    "print(test_array.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.40418392, -0.47878775,  0.7580885 , ..., -0.47598928,\n         0.10172412, -0.33533862],\n       [ 0.2230832 , -0.48024565,  0.75589806, ..., -0.75046986,\n         0.44050843, -0.9165367 ],\n       [-0.70129013,  0.2842256 ,  0.7363471 , ..., -0.50889647,\n         0.14786258, -0.09948996],\n       ...,\n       [-0.26456812, -0.44883907,  0.15373003, ..., -0.0242345 ,\n         0.7692909 , -1.2379875 ],\n       [-0.2578964 , -0.7907244 ,  0.48381078, ..., -0.5863774 ,\n         0.67240703, -0.13996066],\n       [-0.4584682 , -0.02483118, -0.0885824 , ..., -0.4353699 ,\n         0.48302737, -0.12356079]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "bert_embedding[\"t2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(14828, 71)"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "Y=y_df['arr_0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X SHAPE: (14828, 768)\nX SHAPE: (14828, 71)\n"
    }
   ],
   "source": [
    "print(\"X SHAPE:\",bert_embedding[\"t1\"].shape)\n",
    "print(\"X SHAPE:\",Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(bert_embedding[\"t1\"], Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X_train_1 SHAPE: (10379, 768)\nX_test_1 SHAPE: (4449, 768)\nX_test_1 SHAPE: (10379, 71)\ny_test_1 SHAPE: (4449, 71)\n"
    }
   ],
   "source": [
    "print(\"X_train_1 SHAPE:\",X_train_1.shape)\n",
    "print(\"X_test_1 SHAPE:\",X_test_1.shape)\n",
    "print(\"X_test_1 SHAPE:\",y_train_1.shape)\n",
    "print(\"y_test_1 SHAPE:\",y_test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING THE ELM\n",
    "import h5py\n",
    "\n",
    "def _mean_squared_error(y_true, y_pred):\n",
    "    return 0.5 * np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def _mean_absolute_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def _identity(x):\n",
    "    return x\n",
    "\n",
    "class ELM(object):\n",
    "    def __init__(\n",
    "        self, n_input_nodes, n_hidden_nodes, n_output_nodes,\n",
    "        activation='sigmoid', loss='mean_squared_error', name=None,\n",
    "        beta_init=None, alpha_init=None, bias_init=None):\n",
    "\n",
    "        self.name = name\n",
    "        self.__n_input_nodes = n_input_nodes\n",
    "        self.__n_hidden_nodes = n_hidden_nodes\n",
    "        self.__n_output_nodes = n_output_nodes\n",
    "\n",
    "        # initialize weights and a bias\n",
    "        if isinstance(beta_init, np.ndarray):\n",
    "            if beta_init.shape != (self.__n_hidden_nodes, self.__n_output_nodes):\n",
    "                raise ValueError(\n",
    "                    'the shape of beta_init is expected to be (%d,%d).' % (self.__n_hidden_nodes, self.__n_output_nodes)\n",
    "                )\n",
    "            self.__beta = beta_init\n",
    "        else:\n",
    "            self.__beta = np.random.uniform(-1.,1.,size=(self.__n_hidden_nodes, self.__n_output_nodes))\n",
    "        if isinstance(alpha_init, np.ndarray):\n",
    "            if alpha_init.shape != (self.__n_input_nodes, self.__n_hidden_nodes):\n",
    "                raise ValueError(\n",
    "                    'the shape of alpha_init is expected to be (%d,%d).' % (self.__n_hidden_nodes, self.__n_output_nodes)\n",
    "                )\n",
    "            self.__alpha = alpha_init\n",
    "        else:\n",
    "            self.__alpha = np.random.uniform(-1.,1.,size=(self.__n_input_nodes, self.__n_hidden_nodes))\n",
    "        if isinstance(bias_init, np.ndarray):\n",
    "            if bias_init.shape != (self.__n_hidden_nodes,):\n",
    "                raise ValueError(\n",
    "                    'the shape of bias_init is expected to be (%d,).' % (self.__n_hidden_nodes,)\n",
    "                )\n",
    "            self.__bias = bias_init\n",
    "        else:\n",
    "            self.__bias = np.zeros(shape=(self.__n_hidden_nodes,))\n",
    "\n",
    "        # set an activation function\n",
    "        self.__activation = self.__get_activation_function(activation)\n",
    "\n",
    "        # set a loss function\n",
    "        self.__loss = self.__get_loss_function(loss)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self.__activation(x.dot(self.__alpha) + self.__bias)\n",
    "        return h.dot(self.__beta)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return list(self(x))\n",
    "\n",
    "    def evaluate(self, x, t, metrics=['loss']):\n",
    "        y_pred = self.predict(x)\n",
    "        y_true = t\n",
    "        y_pred_argmax = np.argmax(y_pred, axis=-1)\n",
    "        y_true_argmax = np.argmax(y_true, axis=-1)\n",
    "        ret = []\n",
    "        for m in metrics:\n",
    "            if m == 'loss':\n",
    "                loss = self.__loss(y_true, y_pred)\n",
    "                ret.append(loss)\n",
    "            elif m == 'accuracy':\n",
    "                acc = np.sum(y_pred_argmax == y_true_argmax) / len(t)\n",
    "                ret.append(acc)\n",
    "            elif m == 'uar':\n",
    "                num_classes = len(t[0])\n",
    "                uar = []\n",
    "                for i in range(num_classes):\n",
    "                    tp = np.sum((y_pred_argmax == i) & (y_true_argmax == i))\n",
    "                    tp_fn = np.sum(y_true_argmax == i)\n",
    "                    uar.append(tp / tp_fn)\n",
    "                uar = np.mean(uar)\n",
    "                ret.append(uar)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'an unknown evaluation indicator \\'%s\\'.' % m\n",
    "                )\n",
    "        if len(ret) == 1:\n",
    "            ret = ret[0]\n",
    "        elif len(ret) == 0:\n",
    "            ret = None\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def fit(self, x, t):\n",
    "        H = self.__activation(x.dot(self.__alpha) + self.__bias)\n",
    "\n",
    "        # compute a pseudoinverse of H\n",
    "        H_pinv = np.linalg.pinv(H)\n",
    "\n",
    "        # update beta\n",
    "        self.__beta = H_pinv.dot(t)\n",
    "\n",
    "    def save(self, filepath):\n",
    "        with h5py.File(filepath, 'w') as f:\n",
    "            arc = f.create_dataset('architecture', data=np.array([self.__n_input_nodes, self.__n_hidden_nodes, self.__n_output_nodes]))\n",
    "            arc.attrs['activation'] = self.__get_activation_name(self.__activation).encode('utf-8')\n",
    "            arc.attrs['loss'] = self.__get_loss_name(self.__loss).encode('utf-8')\n",
    "            arc.attrs['name'] = self.name.encode('utf-8')\n",
    "            f.create_group('weights')\n",
    "            f.create_dataset('weights/alpha', data=self.__alpha)\n",
    "            f.create_dataset('weights/beta', data=self.__beta)\n",
    "            f.create_dataset('weights/bias', data=self.__bias)\n",
    "\n",
    "    def __get_activation_function(self, name):\n",
    "        if name == 'sigmoid':\n",
    "            return _sigmoid\n",
    "        elif name == 'identity':\n",
    "            return _identity\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'an unknown activation function \\'%s\\'.' % name\n",
    "            )\n",
    "\n",
    "    def __get_activation_name(self, activation):\n",
    "        if activation == _sigmoid:\n",
    "            return 'sigmoid'\n",
    "        elif activation == _identity:\n",
    "            return 'identity'\n",
    "\n",
    "    def __get_loss_function(self, name):\n",
    "        if name == 'mean_squared_error':\n",
    "            return _mean_squared_error\n",
    "        elif name == 'mean_absolute_error':\n",
    "            return _mean_absolute_error\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'an unknown loss function \\'%s\\'.' % name\n",
    "            )\n",
    "\n",
    "    def __get_loss_name(self, loss):\n",
    "        if loss == _mean_squared_error:\n",
    "            return 'mean_squared_error'\n",
    "        elif loss == _mean_absolute_error:\n",
    "            return 'mean_absolute_error'\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return {\n",
    "            'alpha': self.__alpha,\n",
    "            'beta': self.__beta,\n",
    "            'bias': self.__bias,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def input_shape(self):\n",
    "        return (self.__n_input_nodes,)\n",
    "\n",
    "    @property\n",
    "    def output_shape(self):\n",
    "        return (self.__n_output_nodes,)\n",
    "\n",
    "    @property\n",
    "    def n_input_nodes(self):\n",
    "        return self.__n_input_nodes\n",
    "\n",
    "    @property\n",
    "    def n_hidden_nodes(self):\n",
    "        return self.__n_hidden_nodes\n",
    "\n",
    "    @property\n",
    "    def n_output_nodes(self):\n",
    "        return self.__n_output_nodes\n",
    "\n",
    "    @property\n",
    "    def activation(self):\n",
    "        return self.__get_activation_name(self.__activation)\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self.__get_loss_name(self.__loss)\n",
    "\n",
    "def load_model(filepath):\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        alpha_init = f['weights/alpha'][...]\n",
    "        beta_init = f['weights/beta'][...]\n",
    "        bias_init = f['weights/bias'][...]\n",
    "        arc = f['architecture']\n",
    "        n_input_nodes = arc[0]\n",
    "        n_hidden_nodes = arc[1]\n",
    "        n_output_nodes = arc[2]\n",
    "        activation = arc.attrs['activation'].decode('utf-8')\n",
    "        loss = arc.attrs['loss'].decode('utf-8')\n",
    "        name = arc.attrs['name'].decode('utf-8')\n",
    "        model = ELM(\n",
    "            n_input_nodes=n_input_nodes,\n",
    "            n_hidden_nodes=n_hidden_nodes,\n",
    "            n_output_nodes=n_output_nodes,\n",
    "            activation=activation,\n",
    "            loss=loss,\n",
    "            alpha_init=alpha_init,\n",
    "            beta_init=beta_init,\n",
    "            bias_init=bias_init,\n",
    "            name=name,\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    c = np.max(x, axis=-1)\n",
    "    upper = np.exp(x - c)\n",
    "    lower = np.sum(upper, axis=-1)\n",
    "    return upper / lower\n",
    "\n",
    "model = ELM(\n",
    "        n_input_nodes=768,\n",
    "        n_hidden_nodes=5000,\n",
    "        n_output_nodes=71,\n",
    "        loss=\"mean_squared_error\",\n",
    "        activation=\"sigmoid\",\n",
    "        name='elm',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train_loss: 0.008211\ntrain_acc: 0.477599\ntrain_uar: 0.450380\n"
    }
   ],
   "source": [
    "model.fit(X_train_1,y_train_1)\n",
    "train_loss, train_acc, train_uar = model.evaluate(X_train_1, y_train_1, metrics=['loss', 'accuracy', 'uar'])\n",
    "print('train_loss: %f' % train_loss) # loss value\n",
    "print('train_acc: %f' % train_acc) # accuracy\n",
    "print('train_uar: %f' % train_uar) # uar (unweighted average recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "val_loss: 0.046401\nval_acc: 0.119802\nval_uar: nan\n"
    }
   ],
   "source": [
    "val_loss, val_acc, val_uar = model.evaluate(X_test_1, y_test_1, metrics=['loss', 'accuracy', 'uar'])\n",
    "print('val_loss: %f' % val_loss)\n",
    "print('val_acc: %f' % val_acc)\n",
    "print('val_uar: %f' % val_uar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4449,71) (4449,) ",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m&lt;ipython-input-48-6e444f927aa8&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[1;34m\u001b[0m\n\u001b[1;32m----&gt; 1\u001b[1;33m \u001b[0my_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m&#39;---------- prediction %d ----------&#39;\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mclass_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m&lt;ipython-input-45-8d8728d9b691&gt;\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----&gt; 3\u001b[1;33m     \u001b[0mupper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mlower\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mupper\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4449,71) (4449,) "
     ]
    }
   ],
   "source": [
    "\n",
    "    y_predicted = softmax(model.predict(X_test_1))\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        print('---------- prediction %d ----------' % (i+1))\n",
    "        class_pred = np.argmax(y_predicted[i])\n",
    "        prob_pred = y_predicted[i][class_pred]\n",
    "        class_true = np.argmax(y_test_1[i])\n",
    "        print('prediction:')\n",
    "        print('\\tclass: %d, probability: %f' % (class_pred, prob_pred))\n",
    "        print('\\tclass (true): %d' % class_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample input matrix format - true labels and predicted labels\n",
    "#y_true = np.array([[1,0,1,1],[0,1,0,0],[0,0,0,0],[0,1,1,0]])\n",
    "#y_pred = np.array([[0,0,1,1],[1,1,0,1],[0,1,0,0],[1,1,1,1]]\n",
    "\n",
    "#function to calculate some comman values for all the metrics\n",
    "def pre_cal(target, prediction, print_all = False):\n",
    "    if(target.shape != prediction.shape):\n",
    "        print(\"Wrong predictions matrics!\")\n",
    "\n",
    "    real_pos = real_neg = pred_pos = pred_neg  = true_pos = true_neg = []\n",
    "\n",
    "    for i in range(y_true.shape[0]):\n",
    "        # real values - RP and RN\n",
    "        real_pos = np.asarray(np.append(real_pos,np.logical_and(y_true[i], y_true[i]).sum()), dtype=np.int64).reshape(-1,1)\n",
    "        real_neg = np.asarray(np.append(real_neg,np.logical_and(np.logical_not(y_true[i]),np.logical_not(y_true[i])).sum()), dtype=np.int64).reshape(-1,1)\n",
    "\n",
    "        # predicted values - PP and PN\n",
    "        pred_pos = np.asarray(np.append(pred_pos,np.logical_and(y_pred[i], y_pred[i]).sum()),dtype=np.int64).reshape(-1,1)\n",
    "        pred_neg = np.asarray(np.append(pred_neg,np.logical_and(np.logical_not(y_pred[i]), np.logical_not(y_pred[i])).sum()),dtype=np.int64).reshape(-1,1)\n",
    "\n",
    "        # true labels - TP and TN\n",
    "        true_pos = np.asarray(np.append(true_pos,np.logical_and(y_true[i], y_pred[i]).sum()),dtype=np.int64).reshape(-1,1)\n",
    "        true_neg = np.asarray(np.append(true_neg,np.logical_and(np.logical_not(y_true[i]), np.logical_not(y_pred[i])).sum()),dtype=np.int64).reshape(-1,1)\n",
    "\n",
    "    if print_all:\n",
    "\t\t# if print_all = True - it prints RP, RN, PP, PN, TP and TN\n",
    "        result = np.concatenate((real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg), axis=1)\n",
    "        print(result)\n",
    "\n",
    "    return(real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg)\n",
    "\n",
    "#function to resolve divide by zero error and accept the value 0 when divided by 0\n",
    "def divideZero( value_a, value_b):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        result = np.true_divide( value_a, value_b )\n",
    "        result[ ~ np.isfinite( result )] = 0\n",
    "    return result\n",
    "\n",
    "def accuracy(target, predicted):\n",
    "    #return the accuracy - example based\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = (true_pos + true_neg)/(pred_pos + pred_neg)\n",
    "    score = np.mean(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "def precision(target, predicted):\n",
    "    #return precision - example based\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = divideZero(true_pos, pred_pos)\n",
    "    score = np.mean(score)\n",
    "    return score\n",
    "\n",
    "def recall(target, predicted):\n",
    "    #return precision - example based\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = divideZero(true_pos, real_pos)\n",
    "    score = np.mean(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "def fscore(target, predicted,beta = 1):\n",
    "\t#return f(beta)score - example based : default beta value is 1\n",
    "    prec, rec = precision(y_true, y_pred), recall(y_true, y_pred)\n",
    "    beta_val = beta*beta\n",
    "    score = ((1+beta_val)*(prec*rec))/(beta_val*(prec+rec))\n",
    "    return score\n",
    "\n",
    "\n",
    "def hamloss(target, predicted):\n",
    "\t#return hamming loss - example based\n",
    "    hamloss = list()\n",
    "    for i in range(y_true.shape[0]):\n",
    "        hamloss = np.asarray(np.append(hamloss,np.logical_xor(y_true[i], y_pred[i]).sum()), dtype=np.int64).reshape(-1,1)\n",
    "    score = (hamloss.sum())/((y_true.shape[0])*(y_true.shape[1]))\n",
    "    return score\n",
    "\n",
    "\n",
    "def subset(target, predicted):\n",
    "\t#return subset accuracy - example based\n",
    "    subset_matrix = list()\n",
    "    for i in range(y_true.shape[0]):\n",
    "        subset_matrix = np.asarray(np.append(subset_matrix, np.array_equal(y_true[i],y_pred[i])), dtype=np.int64).reshape(-1,1)\n",
    "    score = (subset_matrix.sum())/((y_true.shape[0])*(y_true.shape[1]))\n",
    "    return score\n",
    "\n",
    "def zeroloss(target, predicted):\n",
    "    #return new array with removed element having all zero in y_true\n",
    "    condition = list()\n",
    "    index = list()\n",
    "    for i in range(y_true.shape[0]):\n",
    "        new_true = new_pred = list()\n",
    "        condition = np.logical_and(y_true[i],y_true[i]).sum()\n",
    "        if (condition==0):\n",
    "            index = np.asarray(np.append(index,i), dtype = np.int64)\n",
    "\n",
    "        new_true = np.delete(y_true,index, axis = 0)\n",
    "        new_pred = np.delete(y_pred,index, axis = 0)\n",
    "    return new_true, new_pred\n",
    "\n",
    "def microprecision(target, predicted):\n",
    "    #return micro-precision\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = true_pos.sum()/pred_pos.sum()\n",
    "    return score\n",
    "\n",
    "def microrecall(target, prediction):\n",
    "    #return micro-recall\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = true_pos.sum()/real_pos.sum()\n",
    "    return score\n",
    "\n",
    "def microfscore(target, predicted,beta = 1):\n",
    "    #return micro-fscore\n",
    "    prec, rec = microprecision(y_true, y_pred), microrecall(y_true, y_pred)\n",
    "    beta_val = beta*beta\n",
    "    score = ((1+beta_val)*(prec*rec))/(beta_val*(prec+rec))\n",
    "    return score\n",
    "\n",
    "def macroprecision(target, predicted):\n",
    "    #return macro-precision\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = divideZero(true_pos, pred_pos)\n",
    "    return score\n",
    "\n",
    "def macrorecall(target, predicted):\n",
    "    #return macro-recall\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = divideZero(true_pos, real_pos)\n",
    "    return score\n",
    "\n",
    "def macrofscore(target, predicted,beta = 1):\n",
    "    #return macro-fscore\n",
    "    prec, rec = macroprecision(y_true, y_pred), macrorecall(y_true, y_pred)\n",
    "    beta_val = beta*beta\n",
    "    score = divideZero(((1+beta_val)*(prec*rec)),(beta_val*(prec+rec)))\n",
    "    score = np.mean(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.69, 0.62],\n       [0.6 , 0.57]])"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "mult_data=np.dot(np.array([[.3,.4,.5],[.2,.5,.4]]),np.transpose(np.array([[.6,.4,.7],[.4,.5,.6]])))\n",
    "mult_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigFunc=lambda x: 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.66596693, 0.65021855],\n       [0.64565631, 0.63876318]])"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "sigFunc(mult_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiDDen=np.apply_along_axis(sigFunc,1,mult_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 114.52618125, -116.58005697],\n       [-115.76207588,  119.40364117]])"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "np.linalg.pinv(hiDDen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 114.52618125, -116.58005697],\n       [-115.76207588,  119.40364117]])"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "from scipy.linalg import pinv2 as scip_pinv\n",
    "scip_pinv(hiDDen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ELM_MultiLabel:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, activation, bias=True, random_gen=\"uniform\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_nodes ([integer]): Number of Input nodes\n",
    "            hidden_nodes ([integer]): Number of hidden nodes\n",
    "            output_nodes ([integer]): Number of output nodes\n",
    "            activation ([function]): The function which will be used as the activation function in the hidden layer\n",
    "            bias ([boolean]): Flag to use bias, if True then randomly generate bias @random_gen else bias - 0.\n",
    "            random_gen (str, optional): The type way in which random weight are generated. Defaults to \"uniform\".\n",
    "        \"\"\"\n",
    "        self.__input_nodes = input_nodes\n",
    "        self.__hidden_nodes = hidden_nodes\n",
    "        self.__output_nodes = output_nodes\n",
    "        if random_gen == \"uniform\":\n",
    "            self.__beta = np.random.uniform(-1.,1.,size = (self.__hidden_nodes, self.__output_nodes))\n",
    "            self.__alpha = np.random.uniform(-1.,1.,size = (self.__input_nodes, self.__hidden_nodes))\n",
    "            self.__bias = np.random.uniform(size = (self.__hidden_nodes,))\n",
    "        else:\n",
    "            self.__beta = np.random.normal(-1.,1.,size=(self.__hidden_nodes, self.__output_nodes))\n",
    "            self.__alpha = np.random.normal(-1.,1.,size=(self.__input_nodes, self.__hidden_nodes))\n",
    "            self.__bias = np.random.normal(size=(self.__n_hidden_nodes,)) \n",
    "        self.__activation = activation # Sigmoid Function\n",
    "\n",
    "    def getInputNodes(self):\n",
    "        return  self.__input_nodes\n",
    "\n",
    "    def getHiddenNodes(self):\n",
    "        return  self.__hidden_nodes\n",
    "\n",
    "    def getOutputNodes(self):\n",
    "        return  self.__output_nodes\n",
    "    \n",
    "    def getBetaWeights(self):\n",
    "        return self.__beta\n",
    "    \n",
    "    def getAlphaWeight(self):\n",
    "        return self.__alphs\n",
    "    \n",
    "    def getBias(self):\n",
    "        return self.__bias\n",
    "\n",
    "    def __get_H_matrix(self, train_x, verbose=False):\n",
    "        # 1 Propagate data from Input to hidden Layer\n",
    "        if verbose:\n",
    "            print(\"Propagate data from Input to hidden Layer\")\n",
    "        inp = np.dot(train_x , self.__alpha)\n",
    "        if verbose:\n",
    "            print(inp)\n",
    "            print(\"Adding Biases\")\n",
    "        inp = inp  + self.__bias\n",
    "        if verbose:\n",
    "            print(inp)\n",
    "            print(\"Applyin activation function\")\n",
    "        inp_activation = np.apply_along_axis(self.__activation, 1, inp)\n",
    "        return inp_activation\n",
    "\n",
    "    def fit(self, train_x, train_y, verbose = False, show_metrics = True):\n",
    "        \"\"\"\n",
    "        This function calculates the Beta weights or the output weights\n",
    "        train_x : input matrix\n",
    "        train_y : output matrix to be predicted or learned upon unipolar\n",
    "\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"train_x shape:\", train_x.shape)\n",
    "            print(\"train_y shape:\", train_y.shape)\n",
    "        inp_activation = self.__get_H_matrix(train_x, verbose)\n",
    "        # This is the H matrix getting its Moore Penrose Inverse\n",
    "        if verbose:\n",
    "            print(inp_activation)\n",
    "            print(\"Getting the Generalized Moore Penrose Inverse\")\n",
    "        generalizedInverse = np.linalg.pinv(inp_activation)\n",
    "        if verbose:\n",
    "            print(generalizedInverse)\n",
    "            print(\"Finding Beta, output weights\")\n",
    "        # Now find output weight matrix Beta \n",
    "        # convert input Y values according to the threshold using biploar step function\n",
    "        predicted_bipolar=  np.apply_along_axis(_biploar_step, 1, train_y)\n",
    "        self.__beta = np.dot(generalizedInverse, predicted_bipolar)\n",
    "        if verbose:\n",
    "            print(\"Beta Matrix Weights\")\n",
    "            print(self.__beta)\n",
    "\n",
    "        if(show_metrics):\n",
    "            print(\"Model Metrics, for Training :\")\n",
    "            self.predict(train_x, train_y,verbose,show_metrics)\n",
    "    \n",
    "    def predict(self, test_x, test_y = None, verbose = False, show_metrics= True):\n",
    "        \"\"\"\n",
    "        preditcts the output for the input test data\n",
    "        call this after calling the fit.\n",
    "        test_data shape should be (batch_size,768 or input_nodes)\n",
    "        output_shape will be (batch_size, 71 or output_nodes)\n",
    "\n",
    "        returns: Predicted Label Matrix\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"Predicting outputs\")\n",
    "        inp_activation = self.__get_H_matrix(test_x, verbose)\n",
    "        output_predicted = np.dot(inp_activation, self.__beta)\n",
    "        # convert predicted according to the threshold using biploar step function\n",
    "        predicted_bipolar =  np.apply_along_axis(_biploar_step, 1, output_predicted)\n",
    "        predicted_binary = np.apply_along_axis(_binary_step, 1, predicted_bipolar)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"predicted output\")\n",
    "            print(output_predicted)\n",
    "            print(\"predicted_bipolar\")\n",
    "            print(predicted_bipolar)\n",
    "            print(\"predicted_binary\")\n",
    "            print(predicted_binary)\n",
    "            print(\"Original Binary\")\n",
    "            print(test_y)\n",
    "\n",
    "        if(test_y is not None):\n",
    "            self.__evaluate(test_y,predicted_binary)\n",
    "        return predicted_binary\n",
    "\n",
    "    def __evaluate(self, real, predicted):\n",
    "        \"\"\"\n",
    "        real values as 0,1\n",
    "        predicted values as 0,1\n",
    "        \"\"\"\n",
    "        # Now we find accuracy, precision, recall, Hamming Loss and F1 Measure\n",
    "        accuracy = accuracy_score(real, predicted)\n",
    "        hamLoss = hamming_loss(real, predicted)\n",
    "        # element wise correctness\n",
    "        term_wise_accuracy=np.sum(np.logical_not(np.logical_xor(real, predicted)))/real.size\n",
    "\n",
    "        macro_precision = precision_score(real, predicted, average='macro')\n",
    "        macro_recall = recall_score(real, predicted, average='macro')\n",
    "        macro_f1 = f1_score(real, predicted, average='macro')\n",
    "\n",
    "        micro_precision = precision_score(real, predicted, average='micro')\n",
    "        micro_recall = recall_score(real, predicted, average='micro')\n",
    "        micro_f1 = f1_score(real, predicted, average='micro')\n",
    "        \n",
    "        metricTable=prettytable.PrettyTable()\n",
    "        metricTable.field_names = [\"Metric\", \"Macro Value\", \"Micro Value\"]\n",
    "        metricTable.add_row([\"Hamming Loss\",\"{0:.3f}\".format(hamLoss) ,\"\"])\n",
    "        metricTable.add_row([\"Term Wise Accuracy\",\"{0:.3f}\".format(term_wise_accuracy) ,\"\"])\n",
    "\n",
    "        metricTable.add_row([\"Accuracy\",\"{0:.3f}\".format(accuracy),\"\"])\n",
    "        metricTable.add_row([\"Precision\",\"{0:.3f}\".format(macro_precision),\"{0:.3f}\".format(micro_precision)])\n",
    "        metricTable.add_row([\"Recall\",\"{0:.3f}\".format(macro_recall),\"{0:.3f}\".format(micro_recall)])\n",
    "        metricTable.add_row([\"F1-measure\",\"{0:.3f}\".format(macro_f1),\"{0:.3f}\".format(micro_f1)])\n",
    "\n",
    "        print(metricTable)\n",
    "\n",
    "        print(\"Metrics @ Literature\")\n",
    "        self.get_eval_metrics(real,predicted)\n",
    "\n",
    "        print(\"Classification Report\")\n",
    "        print(classification_report(real,predicted))\n",
    "\n",
    "    def get_eval_metrics(self, real, predicted, verbose= False):\n",
    "        err_cnt_accuracy=0\n",
    "        err_cnt_precision=0\n",
    "        err_cnt_recall=0\n",
    "        if verbose:\n",
    "            print(real)\n",
    "            print(predicted)\n",
    "        for x in range(real.shape[0]):\n",
    "            err_and= np.logical_and(real[x],predicted[x])\n",
    "            err_or = np.logical_or(real[x],predicted[x])\n",
    "            # Accuracy\n",
    "            err_cnt_accuracy +=(sum(err_and)/sum(err_or))\n",
    "\n",
    "            # Precision\n",
    "            if sum(err_and) != 0:\n",
    "                err_cnt_precision += (sum(err_and) / sum(predicted[x]))\n",
    "            # Recall\n",
    "            err_cnt_recall += (sum(err_and) / sum(real[x]))\n",
    "            if verbose:\n",
    "                print(\"Iteration :\",x)\n",
    "                print((sum(err_and)/sum(err_or)))\n",
    "                print(err_and)\n",
    "                print(err_or)\n",
    "                print(err_cnt)\n",
    "        \n",
    "        err_count_hamming = np.zeros((a.shape))\n",
    "\n",
    "        for i in range(real.shape[0]):\n",
    "            for j in range(real.shape[1]):\n",
    "                if real[i,j] != predicted[i,j]:\n",
    "                    err_count_hamming[1,j] = err_count_hamming[1,j]+1\n",
    "\n",
    "        sum_err = np.sum(err_count_hamming);\n",
    "        HammingLoss = sum_err/real.size;\n",
    "        accuracy = err_cnt_accuracy / a.shape[1]\n",
    "        precision = err_cnt_precision / a.shape[1]\n",
    "        recall = err_cnt_recall / a.shape[1]\n",
    "        f1 = 2*((precision*recall)/(precision+recall))\n",
    "        print(\"Final: \")\n",
    "        print(\"Hamming Loss: \", HammingLoss)\n",
    "        print(\"Accuracy: \",accuracy)\n",
    "        print(\"precision: \",precision)\n",
    "        print(\"recall: \",recall)\n",
    "        print(\"f1: \",f1)\n",
    "\n",
    "        metricTable=prettytable.PrettyTable()\n",
    "        metricTable.field_names = [\"Metric\", \"Value\"]\n",
    "        metricTable.add_row([\" Literature Hamming Loss\",\"{0:.3f}\".format(HammingLoss)])\n",
    "        metricTable.add_row([\"Literature Accuracy\",\"{0:.3f}\".format(accuracy)])\n",
    "\n",
    "        metricTable.add_row([\"Literature Precision\",\"{0:.3f}\".format(precision)])\n",
    "        metricTable.add_row([\"LiteratureRecall\",\"{0:.3f}\".format(recall)])\n",
    "        metricTable.add_row([\"LiteratureF1-measure\",\"{0:.3f}\".format(f1)])\n",
    "\n",
    "        print(metricTable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix, hamming_loss,classification_report\n",
    "\n",
    "_bipolar_sigmoid=np.vectorize(lambda x: (1. - np.exp(-x))/(1. + np.exp(-x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Model with 500 hidden nodes\n"
     ]
    }
   ],
   "source": [
    "t1_emlMLTC_500= ELM_MultiLabel(input_nodes=768, hidden_nodes=500, output_nodes=71, activation=_bipolar_sigmoid)\n",
    "print(\"Training Model with 500 hidden nodes\")\n",
    "#t1_emlMLTC_500.fit(type1_BERT_Embeddings_Train,label_values_Train, verbose=False, show_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dill'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9b6c081702ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdill\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dill'"
     ]
    }
   ],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x0000018BAFFA8EE8>: attribute lookup <lambda> on __main__ failed",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f77f15364948>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ELM_500t1_bipolarstep.sav'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1_emlMLTC_500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x0000018BAFFA8EE8>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "filename = 'ELM_500t1_bipolarstep.sav'\n",
    "pickle.dump(t1_emlMLTC_500, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Object of type ELM_MultiLabel is not JSON serializable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-10b3e639d839>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1_emlMLTC_500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Thesis\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type ELM_MultiLabel is not JSON serializable"
     ]
    }
   ],
   "source": [
    "json.dumps(t1_emlMLTC_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}