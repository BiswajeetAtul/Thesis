{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import prettytable\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "main_start=time.time()\n",
    "time_log={\"main_start\":main_start}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING THE BERT EMBEDDINGS AND Y MATRIX\n",
    "bert_embedding=np.load(\"embeddings.npz\")\n",
    "label_values=np.load(\"Y.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "type1_BERT_Embeddings=bert_embedding[\"t1\"]\n",
    "type2_BERT_Embeddings=bert_embedding[\"t2\"]\n",
    "label_values=label_values[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_train_x, t1_test_x, t1_train_y, t1_test_y = train_test_split(type1_BERT_Embeddings, label_values, test_size=0.33, random_state=234)\n",
    "t2_train_x, t2_test_x, t2_train_y, t2_test_y = train_test_split(type2_BERT_Embeddings, label_values, test_size=0.33, random_state=230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape t1_train_x:  [[-0.36826527 -0.43821245 -0.15154007 ... -0.21317424  0.6740728\n  -0.89827394]\n [-0.25526345 -1.0361675   0.31187654 ... -0.41380176  0.23752367\n  -1.4694043 ]\n [-0.30190113 -0.6179864  -0.08102065 ...  0.07218534  0.40399817\n  -0.5914986 ]\n ...\n [-0.00596368 -0.52669936  0.2841632  ... -0.18374442  0.55380374\n  -0.50675297]\n [-0.48843777 -0.7013829  -0.20389749 ...  0.05295624  0.48787463\n  -0.24249634]\n [-0.4864386  -0.4691094  -0.20912297 ... -0.0936537   0.39957207\n  -0.4435219 ]]\nShape t1_test_x:  [[-0.26130378 -0.69637364  0.16091792 ... -0.04362152  0.17829853\n  -0.9757811 ]\n [ 0.06757689 -0.1472328   0.07639776 ...  0.09480745  0.31186146\n  -1.1533178 ]\n [ 0.19800258 -0.887366    0.35757995 ... -0.4777546   0.3292356\n  -0.32011172]\n ...\n [-0.1094763  -0.43274575  0.5285199  ... -0.2149579   0.23089571\n  -0.5284223 ]\n [-0.14528538 -0.18008953  0.5997345  ... -0.39525276 -0.03062117\n  -0.47663215]\n [-0.46414566 -0.2350843  -0.38872486 ... -0.07529384  0.20886713\n  -0.28005672]]\nShape t1_train_y:  [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 1 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nShape t1_test_y:  [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 1 0 0]\n ...\n [0 0 0 ... 1 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nShape t2_train_x:  [[-0.12113115 -0.4721356   0.32802156 ... -0.01129055  0.22955656\n  -0.43235385]\n [-0.37490067 -0.10924493  0.20562199 ... -0.15478972  0.8272832\n   0.03282541]\n [-0.6756584  -0.37155265  0.37979376 ... -0.21698126  0.25919294\n  -0.07541876]\n ...\n [-0.65533835 -0.02729864  0.9285103  ... -0.532351    0.2994408\n  -0.3371567 ]\n [-0.14980441 -0.4930426   0.31083623 ... -0.00909521  0.16175531\n  -1.0065575 ]\n [ 0.20077105 -0.5476451   0.1454247  ... -0.27448085  0.25988725\n  -0.9119779 ]]\nShape t2_test_x:  [[-0.71968806 -0.36738312  0.3997091  ... -0.64548594  0.51239526\n   0.0188057 ]\n [-0.5111096  -0.44507238  0.67590475 ... -0.35881698  0.42847353\n  -0.6184632 ]\n [-0.32719004 -0.38634512  0.51854366 ... -0.347214    0.64176226\n  -0.4653134 ]\n ...\n [ 0.08885844 -0.3623479   0.3193009  ... -0.07060965  0.15481278\n  -0.6156052 ]\n [-0.51937264 -0.4332677   0.4960333  ... -0.7791502   0.08809341\n  -0.24398422]\n [-0.32892975 -0.46162462  0.51651114 ... -0.3790055   0.54254913\n  -0.43450084]]\nShape t2_train_y:  [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nShape t2_test_y:  [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape t1_train_x: \", t1_train_x)\n",
    "print(\"Shape t1_test_x: \", t1_test_x)\n",
    "print(\"Shape t1_train_y: \", t1_train_y)\n",
    "print(\"Shape t1_test_y: \", t1_test_y)\n",
    "print(\"Shape t2_train_x: \", t2_train_x)\n",
    "print(\"Shape t2_test_x: \", t2_test_x)\n",
    "print(\"Shape t2_train_y: \", t2_train_y)\n",
    "print(\"Shape t2_test_y: \", t2_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Activation Functions\n",
    "\n",
    "# def _identity(x):\n",
    "#     return x\n",
    "# def _binary_step(x, threshold = 0):\n",
    "#     return 1 if x=threshold else 0\n",
    "# def _biploar_step(x ,threshold = 0):\n",
    "#     return 1 if x>=threshold else -1\n",
    "# def _binary_sigmoid(x):\n",
    "#     return 1. / (1. + np.exp(-x))\n",
    "# def _bipolar_sigmoid(x):\n",
    "#     return (1. - np.exp(-x))/(1. + np.exp(-x))\n",
    "# def _relu_function(x):\n",
    "#     return np.max(0, x)\n",
    "# def _relu_leaky(x):\n",
    "#     return np.max(0.01*x, x)\n",
    "\n",
    "\n",
    "_identity =np.vectorize(lambda x: x)\n",
    "_binary_step =np.vectorize(lambda x,t=0: 1 if x>t else 0)\n",
    "_biploar_step =np.vectorize(lambda x,t=0: 1 if x>t else -1)\n",
    "_binary_sigmoid=np.vectorize(lambda x: 1. / (1. + np.exp(-x)))\n",
    "_bipolar_sigmoid=np.vectorize(lambda x: (1. - np.exp(-x))/(1. + np.exp(-x)))\n",
    "_relu_function=np.vectorize(lambda x: np.max([0, x]))\n",
    "_relu_leaky=np.vectorize(lambda x: np.max([0.01*x, x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ELM_MultiLabel:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, activation=\"_identity\", bias=True, random_gen=\"uniform\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_nodes ([integer]): Number of Input nodes\n",
    "            hidden_nodes ([integer]): Number of hidden nodes\n",
    "            output_nodes ([integer]): Number of output nodes\n",
    "            activation ([function]): The function name which will be used as the activation function in the hidden layer. Defaults to \"_identity\".\n",
    "                possible values: _binary_step, _biploar_step, _binary_sigmoid, _bipolar_sigmoid, _relu_function, _relu_leaky, _identity\n",
    "            bias ([boolean]): Flag to use bias, if True then randomly generate bias @random_gen else bias - 0.\n",
    "            random_gen (str, optional): The type way in which random weight are generated. Defaults to \"uniform\".\n",
    "        \"\"\"\n",
    "        self.__input_nodes = input_nodes\n",
    "        self.__hidden_nodes = hidden_nodes\n",
    "        self.__output_nodes = output_nodes\n",
    "\n",
    "        if random_gen == \"uniform\":\n",
    "            self.__beta = np.random.uniform(-1.,1.,size = (self.__hidden_nodes, self.__output_nodes))\n",
    "            self.__alpha = np.random.uniform(-1.,1.,size = (self.__input_nodes, self.__hidden_nodes))\n",
    "            self.__bias = np.random.uniform(size = (self.__hidden_nodes,))\n",
    "        else:\n",
    "            self.__beta = np.random.normal(-1.,1.,size=(self.__hidden_nodes, self.__output_nodes))\n",
    "            self.__alpha = np.random.normal(-1.,1.,size=(self.__input_nodes, self.__hidden_nodes))\n",
    "            self.__bias = np.random.normal(size=(self.__n_hidden_nodes,))\n",
    "        \n",
    "\n",
    "        if activation == \"_biploar_step\":\n",
    "            self.__activation = _biploar_step\n",
    "        elif activation == \"_bipolar_sigmoid\":\n",
    "            self.__activation = _bipolar_sigmoid\n",
    "        elif activation == \"_relu_leaky\":\n",
    "            self.__activation =_relu_leaky\n",
    "        elif activation == \"_binary_step\":\n",
    "            self.__activation =_binary_step\n",
    "        elif activation == \"_binary_sigmoid\":\n",
    "            self.__activation =_binary_sigmoid\n",
    "        elif activation == \"_relu_function\":\n",
    "            self.__activation =_relu_function\n",
    "        else:\n",
    "            self.__activation =_identity\n",
    "    \n",
    "\n",
    "    def getInputNodes(self):\n",
    "        return  self.__input_nodes\n",
    "\n",
    "    def getHiddenNodes(self):\n",
    "        return  self.__hidden_nodes\n",
    "\n",
    "    def getOutputNodes(self):\n",
    "        return  self.__output_nodes\n",
    "    \n",
    "    def getBetaWeights(self):\n",
    "        return self.__beta\n",
    "    \n",
    "    def getAlphaWeight(self):\n",
    "        return self.__alphs\n",
    "    \n",
    "    def getBias(self):\n",
    "        return self.__bias\n",
    "\n",
    "    def __get_H_matrix(self, train_x, verbose=False):\n",
    "        # 1 Propagate data from Input to hidden Layer\n",
    "        if verbose:\n",
    "            print(\"Propagate data from Input to hidden Layer\")\n",
    "        inp = np.dot(train_x , self.__alpha)\n",
    "        if verbose:\n",
    "            print(inp)\n",
    "            print(\"Adding Biases\")\n",
    "        inp = inp  + self.__bias\n",
    "        if verbose:\n",
    "            print(inp)\n",
    "            print(\"Applyin activation function\")\n",
    "        inp_activation = np.apply_along_axis(self.__activation, 1, inp)\n",
    "        return inp_activation\n",
    "\n",
    "    def fit(self, train_x, train_y, verbose = False, show_metrics = True):\n",
    "        \"\"\"\n",
    "        This function calculates the Beta weights or the output weights\n",
    "        train_x : input matrix\n",
    "        train_y : output matrix to be predicted or learned upon unipolar\n",
    "\n",
    "        returns: if test_y is not given then\n",
    "                returns the predicted output\n",
    "                if test_y is given then returns predicted output and evaluation metrics dict \n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"train_x shape:\", train_x.shape)\n",
    "            print(\"train_y shape:\", train_y.shape)\n",
    "        inp_activation = self.__get_H_matrix(train_x, verbose)\n",
    "        # This is the H matrix getting its Moore Penrose Inverse\n",
    "        if verbose:\n",
    "            print(inp_activation)\n",
    "            print(\"Getting the Generalized Moore Penrose Inverse\")\n",
    "        generalizedInverse = np.linalg.pinv(inp_activation)\n",
    "        if verbose:\n",
    "            print(generalizedInverse)\n",
    "            print(\"Finding Beta, output weights\")\n",
    "        # Now find output weight matrix Beta \n",
    "        # convert input Y values according to the threshold using biploar step function\n",
    "        _bipolar_y=  np.apply_along_axis(_biploar_step, 1, train_y)\n",
    "        self.__beta = np.dot(generalizedInverse, _bipolar_y)\n",
    "        if verbose:\n",
    "            print(\"Beta Matrix Weights\")\n",
    "            print(self.__beta)\n",
    "\n",
    "        # print(\"Model Metrics, for Training :\")\n",
    "        return self.predict(train_x, train_y,verbose,show_metrics)\n",
    "    \n",
    "    def predict(self, test_x, test_y = None, verbose = False, show_metrics= True):\n",
    "        \"\"\"\n",
    "        preditcts the output for the input test data\n",
    "        call this after calling the fit.\n",
    "        test_data shape should be (batch_size,768 or input_nodes)\n",
    "        output_shape will be (batch_size, 71 or output_nodes)\n",
    "\n",
    "        returns: if test_y is not given then\n",
    "                returns the predicted output\n",
    "                if test_y is given then returns predicted output and evaluation metrics dict\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"Predicting outputs\")\n",
    "        inp_activation = self.__get_H_matrix(test_x, verbose)\n",
    "        output_predicted = np.dot(inp_activation, self.__beta)\n",
    "        # convert predicted according to the threshold using biploar step function\n",
    "        predicted_bipolar =  np.apply_along_axis(_biploar_step, 1, output_predicted)\n",
    "        predicted_binary = np.apply_along_axis(_binary_step, 1, predicted_bipolar)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"predicted output\")\n",
    "            print(output_predicted)\n",
    "            print(\"predicted_bipolar\")\n",
    "            print(predicted_bipolar)\n",
    "            print(\"predicted_binary\")\n",
    "            print(predicted_binary)\n",
    "            print(\"Original Binary\")\n",
    "            print(test_y)\n",
    "\n",
    "        eval_dict={}\n",
    "        if (test_y is not None):\n",
    "            eval_dict=self.__evaluate(test_y,predicted_binary, for_test=False)\n",
    "        if(test_y is not None):\n",
    "            return predicted_binary, eval_dict\n",
    "        else:\n",
    "            return predicted_binary\n",
    "\n",
    "    def __evaluate(self, real, predicted, for_test=True):\n",
    "        \"\"\"\n",
    "        real values as 0,1\n",
    "        predicted values as 0,1\n",
    "        \"\"\"\n",
    "        # Now we find accuracy, precision, recall, Hamming Loss and F1 Measure\n",
    "        accuracy = accuracy_score(real, predicted)\n",
    "        hamLoss = hamming_loss(real, predicted)\n",
    "        # element wise correctness\n",
    "        term_wise_accuracy=np.sum(np.logical_not(np.logical_xor(real, predicted)))/real.size\n",
    "\n",
    "        macro_precision = precision_score(real, predicted, average='macro')\n",
    "        macro_recall = recall_score(real, predicted, average='macro')\n",
    "        macro_f1 = f1_score(real, predicted, average='macro')\n",
    "\n",
    "        micro_precision = precision_score(real, predicted, average='micro')\n",
    "        micro_recall = recall_score(real, predicted, average='micro')\n",
    "        micro_f1 = f1_score(real, predicted, average='micro')\n",
    "        \n",
    "        metricTable=prettytable.PrettyTable()\n",
    "        metricTable.field_names = [\"Metric\", \"Macro Value\", \"Micro Value\"]\n",
    "        metricTable.add_row([\"Hamming Loss\",\"{0:.3f}\".format(hamLoss) ,\"\"])\n",
    "        metricTable.add_row([\"Term Wise Accuracy\",\"{0:.3f}\".format(term_wise_accuracy) ,\"\"])\n",
    "\n",
    "        metricTable.add_row([\"Accuracy\",\"{0:.3f}\".format(accuracy),\"\"])\n",
    "        metricTable.add_row([\"Precision\",\"{0:.3f}\".format(macro_precision),\"{0:.3f}\".format(micro_precision)])\n",
    "        metricTable.add_row([\"Recall\",\"{0:.3f}\".format(macro_recall),\"{0:.3f}\".format(micro_recall)])\n",
    "        metricTable.add_row([\"F1-measure\",\"{0:.3f}\".format(macro_f1),\"{0:.3f}\".format(micro_f1)])\n",
    "\n",
    "        print(metricTable)\n",
    "\n",
    "        #\n",
    "        # print(\"Metrics @ Literature\")\n",
    "        lit_accuracy, lit_precision, lit_recall, lit_f1 = self.get_eval_metrics(real,predicted)\n",
    "\n",
    "        return_dict = {\"HiddenNodes\": self.getHiddenNodes(),\n",
    "                \"lit_accuracy\": lit_accuracy,\n",
    "                \"lit_precision\": lit_precision,\n",
    "                \"lit_recall\": lit_recall,\n",
    "                \"lit_f1\": lit_f1,\n",
    "                \"sklearn_hamLoss\": hamLoss,\n",
    "                \"sklearn_accuracy\": accuracy,\n",
    "                \"term_wise_accuracy\": term_wise_accuracy,\n",
    "                \"sklearn_macro_precision\": macro_precision,\n",
    "                \"sklearn_micro_precision\": micro_precision,\n",
    "                \"sklearn_macro_recall\": macro_recall,\n",
    "                \"sklearn_micro_recall\": micro_recall,\n",
    "                \"sklearn_macro_f1\": macro_f1,\n",
    "                \"sklearn_micro_f1\": micro_f1,\n",
    "                }\n",
    "\n",
    "        return return_dict\n",
    "\n",
    "    def get_eval_metrics(self, real, predicted, verbose= False):\n",
    "        err_cnt_accuracy=0\n",
    "        err_cnt_precision=0\n",
    "        err_cnt_recall=0\n",
    "        if verbose:\n",
    "            print(real)\n",
    "            print(predicted)\n",
    "        for x in range(real.shape[0]):\n",
    "            err_and= np.logical_and(real[x],predicted[x])\n",
    "            err_or = np.logical_or(real[x],predicted[x])\n",
    "            # Accuracy\n",
    "            err_cnt_accuracy +=(sum(err_and)/sum(err_or))\n",
    "\n",
    "            # Precision\n",
    "            if sum(err_and) != 0:\n",
    "                err_cnt_precision += (sum(err_and) / sum(predicted[x]))\n",
    "            # Recall\n",
    "            err_cnt_recall += (sum(err_and) / sum(real[x]))\n",
    "            if verbose:\n",
    "                print(\"Iteration :\",x)\n",
    "                print((sum(err_and)/sum(err_or)))\n",
    "                print(err_and)\n",
    "                print(err_or)\n",
    "        \n",
    "        # err_count_hamming = np.zeros((real.shape))\n",
    "\n",
    "        # for i in range(real.shape[0]):\n",
    "        #     for j in range(real.shape[1]):\n",
    "        #         if real[i,j] != predicted[i,j]:\n",
    "        #             err_count_hamming[1,j] = err_count_hamming[1,j]+1\n",
    "\n",
    "        # sum_err = np.sum(err_count_hamming);\n",
    "        # HammingLoss = sum_err/real.size;\n",
    "        accuracy = err_cnt_accuracy / real.shape[0]\n",
    "        precision = err_cnt_precision / real.shape[0]\n",
    "        recall = err_cnt_recall / real.shape[0]\n",
    "        f1 = 2*((precision*recall)/(precision+recall))\n",
    "        if verbose:\n",
    "            print(\"Final: \")\n",
    "            # print(\"Hamming Loss: \", HammingLoss)\n",
    "            print(\"Accuracy: \",accuracy)\n",
    "            print(\"precision: \",precision)\n",
    "            print(\"recall: \",recall)\n",
    "            print(\"f1: \",f1)\n",
    "\n",
    "        # metricTable=prettytable.PrettyTable()\n",
    "        # metricTable.field_names = [\"Metric\", \"Value\"]\n",
    "        # metricTable.add_row([\" Literature Hamming Loss\",\"{0:.3f}\".format(HammingLoss)])\n",
    "        # metricTable.add_row([\"Literature Accuracy\",\"{0:.3f}\".format(accuracy)])\n",
    "\n",
    "        # metricTable.add_row([\"Literature Precision\",\"{0:.3f}\".format(precision)])\n",
    "        # metricTable.add_row([\"LiteratureRecall\",\"{0:.3f}\".format(recall)])\n",
    "        # metricTable.add_row([\"LiteratureF1-measure\",\"{0:.3f}\".format(f1)])\n",
    "\n",
    "        # # print(metricTable)\n",
    "\n",
    "        return accuracy,precision,recall,f1\n"
   ]
  },
  {
   "source": [
    "Now The preprocessing and is done."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now We will run the model for all the three types data sets we have viz.\n",
    "- TRAIN X\n",
    "  - t1_train_x\n",
    "  - t2_train_x\n",
    "- TEST X\n",
    "  - t1_test_x\n",
    "  - t2_test_x\n",
    "- TRAIN Y\n",
    "  - t1_train_y\n",
    "  - t2_train_y\n",
    "- TEST Y\n",
    "  - t1_test_y\n",
    "  - t2_test_y\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models_hidden_nodes=[100,200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000, 10000]#, 15000, 20000]\n",
    "\n",
    "INPUT_NODES= 768\n",
    "OUTPUT_NODES= 71\n",
    "activations= [\"_identity\",\"_biploar_step\",\"_bipolar_sigmoid\",\"_relu_leaky\",\"_binary_sigmoid\"]\n",
    "# activations= [\"_identity\"]\n",
    "\n",
    "randomizations =\"normal\"\n",
    "datasets ={\"t1_bert\":(t1_train_x,t1_train_y,t1_test_x,t1_test_y),\"t2_bert\":(t2_train_x,t2_train_y,t2_test_x,t2_test_y)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_list=[]\n",
    "\n",
    "def add_data_to_metric_list(eval_dict, activation, type, start, phase, end, metrics_dict_list=metrics_dict_list):\n",
    "    eval_dict[\"activation\"]=activation\n",
    "    eval_dict[\"type\"]=type\n",
    "    eval_dict[\"phase\"]=phase\n",
    "    eval_dict[\"total_time\"]=end-start\n",
    "\n",
    "    metrics_dict_list.append(eval_dict)\n"
   ]
  },
  {
   "source": [
    "Testing the above function with a simple 50 hidden layer node model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the models\n",
    "model_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t1_bert_5000__identity_normal\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.038    |             |\n",
      "| Term Wise Accuracy |    0.962    |             |\n",
      "|      Accuracy      |    0.081    |             |\n",
      "|     Precision      |    0.235    |    0.693    |\n",
      "|       Recall       |    0.024    |    0.152    |\n",
      "|     F1-measure     |    0.034    |    0.249    |\n",
      "+--------------------+-------------+-------------+\n",
      "t1_bert_200__identity_normal\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.052    |             |\n",
      "|     Precision      |    0.078    |    0.641    |\n",
      "|       Recall       |    0.014    |    0.105    |\n",
      "|     F1-measure     |    0.018    |    0.180    |\n",
      "+--------------------+-------------+-------------+\n",
      "t2_bert_5000__identity_normal\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.038    |             |\n",
      "| Term Wise Accuracy |    0.962    |             |\n",
      "|      Accuracy      |    0.086    |             |\n",
      "|     Precision      |    0.288    |    0.707    |\n",
      "|       Recall       |    0.026    |    0.159    |\n",
      "|     F1-measure     |    0.037    |    0.259    |\n",
      "+--------------------+-------------+-------------+\n",
      "t2_bert_200__identity_normal\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.060    |             |\n",
      "|     Precision      |    0.079    |    0.654    |\n",
      "|       Recall       |    0.015    |    0.111    |\n",
      "|     F1-measure     |    0.020    |    0.190    |\n",
      "+--------------------+-------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets.keys():\n",
    "    for HIDDEN_NODES in list_of_models_hidden_nodes:\n",
    "        for activation in activations:\n",
    "                start = time.time()\n",
    "                print(dataset+\"_\"+str(HIDDEN_NODES)+\"_\"+activation+\"_\"+randomizations)\n",
    "                model_dict[dataset+\"_\"+str(HIDDEN_NODES)+\"_\"+activation+\"_\"+randomizations]= ELM_MultiLabel(input_nodes=INPUT_NODES,hidden_nodes=HIDDEN_NODES,output_nodes=OUTPUT_NODES, activation=activations)\n",
    "                predicted, eval_dict=model_dict[dataset+\"_\"+str(HIDDEN_NODES)+\"_\"+activation+\"_\"+randomizations].fit(datasets[dataset][0],datasets[dataset][1], verbose=False, show_metrics=True)\n",
    "\n",
    "                end =time.time()\n",
    "                add_data_to_metric_list(eval_dict, activation, dataset, start, \"train\", end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t1_bert_5000__identity_normal\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.041    |             |\n",
      "| Term Wise Accuracy |    0.959    |             |\n",
      "|      Accuracy      |    0.055    |             |\n",
      "|     Precision      |    0.114    |    0.563    |\n",
      "|       Recall       |    0.018    |    0.118    |\n",
      "|     F1-measure     |    0.024    |    0.195    |\n",
      "+--------------------+-------------+-------------+\n",
      "t1_bert_200__identity_normal\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.041    |             |\n",
      "| Term Wise Accuracy |    0.959    |             |\n",
      "|      Accuracy      |    0.044    |             |\n",
      "|     Precision      |    0.067    |    0.592    |\n",
      "|       Recall       |    0.012    |    0.091    |\n",
      "|     F1-measure     |    0.016    |    0.157    |\n",
      "+--------------------+-------------+-------------+\n",
      "t2_bert_5000__identity_normal\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.063    |             |\n",
      "|     Precision      |    0.103    |    0.566    |\n",
      "|       Recall       |    0.020    |    0.131    |\n",
      "|     F1-measure     |    0.026    |    0.213    |\n",
      "+--------------------+-------------+-------------+\n",
      "t2_bert_200__identity_normal\n",
      "+--------------------+-------------+-------------+\n",
      "|       Metric       | Macro Value | Micro Value |\n",
      "+--------------------+-------------+-------------+\n",
      "|    Hamming Loss    |    0.040    |             |\n",
      "| Term Wise Accuracy |    0.960    |             |\n",
      "|      Accuracy      |    0.055    |             |\n",
      "|     Precision      |    0.061    |    0.615    |\n",
      "|       Recall       |    0.014    |    0.105    |\n",
      "|     F1-measure     |    0.018    |    0.180    |\n",
      "+--------------------+-------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets.keys():\n",
    "    for HIDDEN_NODES in list_of_models_hidden_nodes:\n",
    "        for activation in activations:\n",
    "                start = time.time()\n",
    "                print(dataset+\"_\"+str(HIDDEN_NODES)+\"_\"+activation+\"_\"+randomizations)\n",
    "                model_dict[dataset+\"_\"+str(HIDDEN_NODES)+\"_\"+activation+\"_\"+randomizations]\n",
    "                predicted, eval_dict=model_dict[dataset+\"_\"+str(HIDDEN_NODES)+\"_\"+activation+\"_\"+randomizations].predict(datasets[dataset][2],datasets[dataset][3], show_metrics=True)\n",
    "                end =time.time()\n",
    "                add_data_to_metric_list(eval_dict, activation, dataset, start, \"test\", end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   HiddenNodes  lit_accuracy  lit_precision  lit_recall    lit_f1  \\\n",
       "0         5000      0.174942       0.337462    0.188125  0.241578   \n",
       "1          200      0.119984       0.241846    0.130124  0.169207   \n",
       "2         5000      0.186166       0.359511    0.200837  0.257708   \n",
       "3          200      0.132336       0.265502    0.143000  0.185883   \n",
       "4         5000      0.132934       0.267538    0.148197  0.190739   \n",
       "5          200      0.105219       0.215025    0.116366  0.151010   \n",
       "6         5000      0.147357       0.286020    0.163032  0.207684   \n",
       "7          200      0.123676       0.245879    0.134199  0.173632   \n",
       "\n",
       "   sklearn_hamLoss  sklearn_accuracy  term_wise_accuracy  \\\n",
       "0         0.038347          0.080632            0.961653   \n",
       "1         0.039955          0.052345            0.960045   \n",
       "2         0.038360          0.085967            0.961640   \n",
       "3         0.040069          0.060499            0.959931   \n",
       "4         0.041097          0.054965            0.958903   \n",
       "5         0.041019          0.043727            0.958981   \n",
       "6         0.040130          0.062934            0.959870   \n",
       "7         0.039767          0.055170            0.960233   \n",
       "\n",
       "   sklearn_macro_precision  sklearn_micro_precision  sklearn_macro_recall  \\\n",
       "0                 0.234586                 0.692629              0.024335   \n",
       "1                 0.077644                 0.640829              0.013636   \n",
       "2                 0.288305                 0.706770              0.026435   \n",
       "3                 0.079276                 0.653680              0.014841   \n",
       "4                 0.114471                 0.562907              0.017674   \n",
       "5                 0.067346                 0.592064              0.011748   \n",
       "6                 0.103019                 0.565778              0.019792   \n",
       "7                 0.060872                 0.614512              0.013969   \n",
       "\n",
       "   sklearn_micro_recall  sklearn_macro_f1  sklearn_micro_f1 activation  \\\n",
       "0              0.151726          0.034298          0.248924  _identity   \n",
       "1              0.104672          0.017916          0.179951  _identity   \n",
       "2              0.158569          0.037344          0.259024  _identity   \n",
       "3              0.111391          0.019532          0.190345  _identity   \n",
       "4              0.117755          0.024131          0.194767  _identity   \n",
       "5              0.090550          0.015575          0.157076  _identity   \n",
       "6              0.131266          0.026278          0.213093  _identity   \n",
       "7              0.105402          0.018263          0.179941  _identity   \n",
       "\n",
       "      type  phase  total_time  \n",
       "0  t1_bert  train  108.484474  \n",
       "1  t1_bert  train    7.745290  \n",
       "2  t2_bert  train  108.230605  \n",
       "3  t2_bert  train    7.996619  \n",
       "4  t1_bert   test    6.342042  \n",
       "5  t1_bert   test    3.861674  \n",
       "6  t2_bert   test    6.763947  \n",
       "7  t2_bert   test    4.015265  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>HiddenNodes</th>\n      <th>lit_accuracy</th>\n      <th>lit_precision</th>\n      <th>lit_recall</th>\n      <th>lit_f1</th>\n      <th>sklearn_hamLoss</th>\n      <th>sklearn_accuracy</th>\n      <th>term_wise_accuracy</th>\n      <th>sklearn_macro_precision</th>\n      <th>sklearn_micro_precision</th>\n      <th>sklearn_macro_recall</th>\n      <th>sklearn_micro_recall</th>\n      <th>sklearn_macro_f1</th>\n      <th>sklearn_micro_f1</th>\n      <th>activation</th>\n      <th>type</th>\n      <th>phase</th>\n      <th>total_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5000</td>\n      <td>0.174942</td>\n      <td>0.337462</td>\n      <td>0.188125</td>\n      <td>0.241578</td>\n      <td>0.038347</td>\n      <td>0.080632</td>\n      <td>0.961653</td>\n      <td>0.234586</td>\n      <td>0.692629</td>\n      <td>0.024335</td>\n      <td>0.151726</td>\n      <td>0.034298</td>\n      <td>0.248924</td>\n      <td>_identity</td>\n      <td>t1_bert</td>\n      <td>train</td>\n      <td>108.484474</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>0.119984</td>\n      <td>0.241846</td>\n      <td>0.130124</td>\n      <td>0.169207</td>\n      <td>0.039955</td>\n      <td>0.052345</td>\n      <td>0.960045</td>\n      <td>0.077644</td>\n      <td>0.640829</td>\n      <td>0.013636</td>\n      <td>0.104672</td>\n      <td>0.017916</td>\n      <td>0.179951</td>\n      <td>_identity</td>\n      <td>t1_bert</td>\n      <td>train</td>\n      <td>7.745290</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5000</td>\n      <td>0.186166</td>\n      <td>0.359511</td>\n      <td>0.200837</td>\n      <td>0.257708</td>\n      <td>0.038360</td>\n      <td>0.085967</td>\n      <td>0.961640</td>\n      <td>0.288305</td>\n      <td>0.706770</td>\n      <td>0.026435</td>\n      <td>0.158569</td>\n      <td>0.037344</td>\n      <td>0.259024</td>\n      <td>_identity</td>\n      <td>t2_bert</td>\n      <td>train</td>\n      <td>108.230605</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>200</td>\n      <td>0.132336</td>\n      <td>0.265502</td>\n      <td>0.143000</td>\n      <td>0.185883</td>\n      <td>0.040069</td>\n      <td>0.060499</td>\n      <td>0.959931</td>\n      <td>0.079276</td>\n      <td>0.653680</td>\n      <td>0.014841</td>\n      <td>0.111391</td>\n      <td>0.019532</td>\n      <td>0.190345</td>\n      <td>_identity</td>\n      <td>t2_bert</td>\n      <td>train</td>\n      <td>7.996619</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5000</td>\n      <td>0.132934</td>\n      <td>0.267538</td>\n      <td>0.148197</td>\n      <td>0.190739</td>\n      <td>0.041097</td>\n      <td>0.054965</td>\n      <td>0.958903</td>\n      <td>0.114471</td>\n      <td>0.562907</td>\n      <td>0.017674</td>\n      <td>0.117755</td>\n      <td>0.024131</td>\n      <td>0.194767</td>\n      <td>_identity</td>\n      <td>t1_bert</td>\n      <td>test</td>\n      <td>6.342042</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>200</td>\n      <td>0.105219</td>\n      <td>0.215025</td>\n      <td>0.116366</td>\n      <td>0.151010</td>\n      <td>0.041019</td>\n      <td>0.043727</td>\n      <td>0.958981</td>\n      <td>0.067346</td>\n      <td>0.592064</td>\n      <td>0.011748</td>\n      <td>0.090550</td>\n      <td>0.015575</td>\n      <td>0.157076</td>\n      <td>_identity</td>\n      <td>t1_bert</td>\n      <td>test</td>\n      <td>3.861674</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5000</td>\n      <td>0.147357</td>\n      <td>0.286020</td>\n      <td>0.163032</td>\n      <td>0.207684</td>\n      <td>0.040130</td>\n      <td>0.062934</td>\n      <td>0.959870</td>\n      <td>0.103019</td>\n      <td>0.565778</td>\n      <td>0.019792</td>\n      <td>0.131266</td>\n      <td>0.026278</td>\n      <td>0.213093</td>\n      <td>_identity</td>\n      <td>t2_bert</td>\n      <td>test</td>\n      <td>6.763947</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>200</td>\n      <td>0.123676</td>\n      <td>0.245879</td>\n      <td>0.134199</td>\n      <td>0.173632</td>\n      <td>0.039767</td>\n      <td>0.055170</td>\n      <td>0.960233</td>\n      <td>0.060872</td>\n      <td>0.614512</td>\n      <td>0.013969</td>\n      <td>0.105402</td>\n      <td>0.018263</td>\n      <td>0.179941</td>\n      <td>_identity</td>\n      <td>t2_bert</td>\n      <td>test</td>\n      <td>4.015265</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "_metrics_df= pd.DataFrame(metrics_dict_list)\n",
    "_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**WRITING METRICS DATA TO FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_metrics_df.to_csv(\"Final_ELM_Mertics_BERT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**END**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "display_name": "Python 3.7.9 64-bit ('Thesis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "870dc015ab544cf4fd4c91e2f8042f40253e24aac5eeb5e28336055566efd434"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}